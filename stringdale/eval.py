# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/017_eval.ipynb.

# %% auto 0
__all__ = ['logger', 'EVAL_COMPARISONS', 'EVAL_DEFAULT_COMPARISON', 'parse_trace_log', 'cosine_dist', 'eq', 'any', 'safe_eval',
           'DataPoint', 'evaluate_datapoint', 'summarize_datapoint', 'trace_log_len', 'serialize_test_case',
           'TestSetRun', 'eval_dataset', 'Comparison', 'sort_conditions', 'limit_to_datapoint', 'get_datapoint',
           'describe_changes', 'compare_datasets', 'eval']

# %% ../nbs/017_eval.ipynb 3
import os
import json
import yaml
from stringdale import (
    Define,
    Scope,
    V,
    E,
    Condition,
    draw_nx
)
from stringdale.stream_warping import (
    TestCase,
    parse_test_case,
    TraceLog,
    event_stream_warp,
    word_overlap,
    regex,
)


from pathlib import Path
from frozendict import frozendict
from .core import  checkLogs
import pytest
import asyncio
from pydantic import BaseModel, ConfigDict

from typing import List, Union
import jsonlines
import logging


# %% ../nbs/017_eval.ipynb 4
logger = logging.getLogger(__name__)

# %% ../nbs/017_eval.ipynb 7
def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:
    """
    Parse a trace file into a list of Trace objects.
    """
    with jsonlines.open(trace_path) as reader:
        traces = [trace for trace in reader]
        return TraceLog(steps=traces)

# %% ../nbs/017_eval.ipynb 14
import numpy as np
import asyncio
from .db import openai_embed
from .chat import Chat

# %% ../nbs/017_eval.ipynb 15
async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:
    """Compute cosine distance between two strings using OpenAI embeddings.
    
    Args:
        out: First string to compare
        expected: Second string to compare
        model: OpenAI embedding model to use (default: 'text-embedding-3-small')
        
    Returns:
        float: Cosine similarity between the two strings (between -1 and 1)
    """
    # Get embeddings for both strings
    if not isinstance(out,str):
        return np.inf
    if not isinstance(expected,str):
        raise ValueError(f"cosine_dist: expected is not a string: {expected}")
    out_embedding = await openai_embed(out, model=model)
    expected_embedding = await openai_embed(expected, model=model)
    
    # Compute cosine similarity
    dot_product = np.dot(out_embedding, expected_embedding)
    norm_out = np.linalg.norm(out_embedding)
    norm_expected = np.linalg.norm(expected_embedding)
    
    # Return cosine similarity
    return 1-dot_product / (norm_out * norm_expected)



# %% ../nbs/017_eval.ipynb 18
from .core import jinja_undeclared_vars
from typing import Any

# %% ../nbs/017_eval.ipynb 22
def eq(a,b):
    if a == b:
        return 0
    else:
        return np.inf

def any(a,b):
    return 0

# %% ../nbs/017_eval.ipynb 25
from .tools import run_python_code


# %% ../nbs/017_eval.ipynb 26
def safe_eval(out,expression):
    try:
        formatted_expressions = expression.format(out)
    except Exception as e:
        logger.warning(f"Error formatting expression: {expression} with value {out}, error: {e}")
        return np.inf
    value = run_python_code(formatted_expressions)
    if isinstance(value,str) and value.startswith("Error"):
        logger.warning(
            f"Error evaluating expression: {formatted_expressions} = {value}\n"
            f"out: {out}\n"
            f"expression: {expression}\n"
            f"error: {e}"
        )
        return np.inf
    logger.debug(f"safe_eval: {formatted_expressions} = {value}")
    if isinstance(value,bool):
        return 0 if value else np.inf
    elif isinstance(value,float):
        return value
    else:
        logger.debug(
            f"When evaluating {expression} with value {out}\n"
            f"Expected float or bool, got {type(value)} with value {repr(value)}"
            )
        return np.inf

# %% ../nbs/017_eval.ipynb 30
from typing import List,Dict,Callable

# %% ../nbs/017_eval.ipynb 31
class DataPoint(BaseModel):
    traces:TraceLog
    expected:TestCase
    

# %% ../nbs/017_eval.ipynb 32
async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):
    d=Agent()
    with jsonlines.open(trace_log_path,'w') as writer:
        for input in test_case.inputs:
            async for trace in d.arun(input):
                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))
            if d.finished:
                break

async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):
    if trace_log_path is None:
        trace_log_path = test_case_path.parent/test_case_path.name.replace(".yaml", ".jsonl").replace("expected", "actual")

    if not trace_log_path.parent.exists():
        os.makedirs(trace_log_path.parent,exist_ok=True)
    try:
        test_case = parse_test_case(test_case_path)
    except Exception as e:
        raise ValueError(f"Error parsing test case {test_case_path}: {e}") from e
        

    if force_run or not trace_log_path.exists():
        if not trace_log_path.exists():
            logger.info(f"Trace file {trace_log_path.name} does not exist, running agent")
        else:
            logger.info(f"Force running {trace_log_path.name}")
        await _run_agent(Agent,test_case,trace_log_path)
    else:
        logger.info(f"Trace file {trace_log_path.name} already exists, skipping agent run")

    parsed_trace = parse_trace_log(trace_log_path)
    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)
    
    return aligned_trace,score,debug_info,trace_log_path


# %% ../nbs/017_eval.ipynb 35
with checkLogs():
    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)

assert alignment is None
alignment,score,trace_out


# %% ../nbs/017_eval.ipynb 37
import pandas as pd

# %% ../nbs/017_eval.ipynb 38
def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):
    """
    Reorder the columns of a pandas dataframe to put the first_columns first.
    """
    return df[first_columns + [c for c in df.columns if c not in first_columns]]



# %% ../nbs/017_eval.ipynb 41
from copy import deepcopy
from itertools import count

# %% ../nbs/017_eval.ipynb 42
def summarize_datapoint(name,alignment,debug_info):
    """
    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage
    by using the alignment and the debug info
    """
    deep_dive_fit = []

    comp_counter = count()
    for expected_node_id,trace_idx in alignment.items():
        match_data = debug_info[expected_node_id][trace_idx]
        for comp in match_data['comparisons']:
            summary = deepcopy(match_data) | deepcopy(comp) 
            summary['comp_id'] = next(comp_counter)
            summary.pop('comparisons')
            deep_dive_fit.append(summary)

    df = pd.DataFrame(deep_dive_fit)
    df['datapoint'] = str(name)
    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])
    return df

# %% ../nbs/017_eval.ipynb 46
from . import DiagramSchema
from pprint import pprint, pformat

# %% ../nbs/017_eval.ipynb 47
def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):
    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(".jsonl")

def _find_yamls(test_dir:Path):
    expected_yamls = list(test_dir.glob("**/*.yaml")) + list(test_dir.glob("**/*.yml"))
    return expected_yamls


def trace_log_len(trace_log_path:Path):
    """Gets number of traces in a trace log file
    This is equal to the number of nodes executed in the workflow
    """
    return len(trace_log_path.read_text().splitlines())

def serialize_test_case(test_case:Path):
    yml_version = yaml.safe_load(test_case.read_text())
    json_version = json.dumps(yml_version,indent=2)
    return json_version

# %% ../nbs/017_eval.ipynb 48
class TestSetRun(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    test_dir: Path
    dir: Path
    summary: pd.DataFrame
    details: pd.DataFrame
    debug: dict

    def __repr__(self):     
        return (
            f"TestSetRun(\n"
            f"  test_dir={self.test_dir}, \n"
            f"  dir={self.log_dir}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f"  debug=dict)")

    def __str__(self):
        return self.__repr__()

    def save(self,dir:Path):
        self.summary.to_csv(dir/"summary.csv")
        self.details.to_csv(dir/"details.csv")
        (dir/'test_cases_loc.txt').write_text(str(self.test_dir.relative_to(self.dir)))
        with open(dir/"debug.json","w") as f:
            json.dump(self.debug,f)

    @classmethod
    def load(cls, dir: Path):
        # Initialize empty DataFrames and dict for missing files
        summary = pd.DataFrame(columns=['datapoint','distance','avg_distance','coverage','alignment','serialized_test_case'])
        details = pd.DataFrame()
        debug = {}
        test_cases_loc = dir  # Default to the input directory
        
        # Try to load files if they exist
        try:
            if (dir/"summary.csv").exists():
                summary = pd.read_csv(dir/"summary.csv")
            if (dir/"details.csv").exists():
                details = pd.read_csv(dir/"details.csv")
            if (dir/"debug.json").exists():
                with open(dir/"debug.json") as f:
                    debug = json.load(f)
            if (dir/'test_cases_loc.txt').exists():
                test_cases_loc = (dir/'test_cases_loc.txt').read_text().strip()
                test_cases_loc = dir/test_cases_loc
        except Exception as e:
            # Log the error but continue with empty/default values
            print(f"Warning: Error loading some files: {str(e)}")
        
        return cls(
            test_dir=test_cases_loc,
            dir=dir,
            summary=summary,
            details=details,
            debug=debug
        )


    def trace_log_path(self,datapoint:str):
        return self.dir/'logs'/datapoint/'.jsonl'
# TODO from here, factor all TestSetRun file logic to this class
    def datapoint_len(self,datapoint:str):
        datapoint_yaml = (self.test_dir/datapoint).with_suffix(".yaml")
        return trace_log_len(self.dir/datapoint_yaml)

    def serialize_test_case(self,datapoint_path):
        pass

    def is_datapoint_stale(self,datapoint_path):
        pass
        # if the datapoint is not in the summary, it is stale
        # load_serialized_test_case
        # if it is in summary but the serialized_test_case is different, it is stale
        # else, not stale


# %% ../nbs/017_eval.ipynb 49
async def eval_dataset(Agent:DiagramSchema,test_dir,out_dir,comparisons,default_comparison,force_run=False):

    # TODO from here
    # check if log_dir has a summary (only if not force_run)
    # if so, load it, and compare the test_case_json to the existing_yaml
    # only if it is different, and the datapoint to the datapoints to be run
    # since we computed ourselves which datapoints to run, we call evaluate_datapoint with force_run=True

    run = TestSetRun.load(out_dir)

    # TODO
    # get_all_yamls
    # filter to stale ones
    # only run the stale ones


    test_cases = _find_yamls(test_dir)
    relative_test_cases = [test_cases.relative_to(test_dir) for test_cases in test_cases]

    trace_files  = [_trace_out_path(test_case,test_dir,out_dir/'logs') for test_case in test_cases]

    logger.info(f"Evaluating {len(test_cases)} datapoints, logging to {out_dir/'logs'}")
    datapoint_tasks = [evaluate_datapoint(
            Agent=Agent,
            comparisons=comparisons,
            default_comparison=default_comparison,
            test_case_path=test_case,
            trace_log_path=trace_file,
            force_run=force_run,
        ) for test_case,trace_file in zip(test_cases,trace_files) if trace_file in trace_files]
    
    datapoint_results = await asyncio.gather(*datapoint_tasks,return_exceptions=True)

    for result,relative_test_case in zip(datapoint_results,relative_test_cases):
      if isinstance(result,Exception):
        result.args = (f"When evaluating datapoint {relative_test_case}:\n{result.args[0]}", )+ result.args[1:]
        raise result

    summary_data = list()
    deep_dives = list()
    debug_infos = dict()

    for (test_case,alignment,score,debug_info,trace_out),test_case_path in zip(datapoint_results,test_cases):
        datapoint_name = trace_out.relative_to(log_dir).with_suffix("")
        serialized_test_case = serialize_test_case(test_case_path)
        deep_dive = summarize_datapoint(datapoint_name,alignment,debug_info)
        deep_dives.append(deep_dive)
        avg_distance = deep_dive.distance.mean()
        coverage = len(alignment) / trace_log_len(trace_out)
        summary = {'datapoint':str(datapoint_name),'distance':score,'avg_distance':avg_distance,'coverage':coverage,'alignment':alignment,'serialized_test_case':serialized_test_case}
        summary_data.append(summary)
        debug_infos[datapoint_name] = debug_info
    

    # update the existing summary and details dataframes
    summary_df = pd.DataFrame(summary_data)
    if len(deep_dives) > 0:
        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)
    else:
        deep_dives_df = pd.DataFrame()

    return TestSetRun(
        test_dir=test_dir,
        log_dir=log_dir,
        summary=summary_df,
        details=deep_dives_df,
        debug=debug_infos
    )

# %% ../nbs/017_eval.ipynb 62
import math
from typing import Optional
import textwrap

# %% ../nbs/017_eval.ipynb 63
class Comparison(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    from_run: TestSetRun
    to_run: TestSetRun
    log_dir: Optional[Path] = None
    summary: pd.DataFrame
    details: pd.DataFrame

    def __repr__(self):     
        return (
            f"Comparison(\n"
            f"  from_run={textwrap.indent(self.from_run.__repr__(), '  ').strip()}, \n"
            f"  to_run={textwrap.indent(self.to_run.__repr__(), '  ').strip()}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f")")

    def __str__(self):
        return self.__repr__()

# %% ../nbs/017_eval.ipynb 64
def sort_conditions(df):
    return df.sort_values(by=['node_idx','comparison','expected'])

def limit_to_datapoint(df,datapoint):
    return df.loc[df['datapoint'] == datapoint]

def get_datapoint(ds,datapoint):
    return sort_conditions(limit_to_datapoint(ds.details,datapoint))


def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):
    """
    Describe the changes between two datapoints
    """
    
    # get the detailed version of the datasets and limit to only rows of the given datapoint
    datapoint_df1 = get_datapoint(ds1,datapoint)
    datapoint_df2 = get_datapoint(ds2,datapoint)

    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)
    # lets assert this in the code
    assert datapoint_df1.shape == datapoint_df2.shape, f"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}"

    changes = []

    # get the first comparison whose node aligned to a different trace
    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if row1.trace_idx != row2.trace_idx:
            changes.append({
                'datapoint':datapoint,
                'change_type':'alignment_change',
                'before':row1.trace_idx,
                'after':row2.trace_idx,
                'comparison_id':row1.comp_id,
            })
            break
            

    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):
            continue
        if row2.distance + epsilon > row1.distance:
            change_types = 'regression'
        elif row2.distance - epsilon < row1.distance:
            change_types = 'improvement'
        else:
            continue
        
        changes.append({
            'datapoint':datapoint,
            'change_type':change_types,
            'value':row1.distance - row2.distance,
            'comparison_id':row1.comp_id,
            'node_label':row1.node_label,
            'expected':row1.expected,
            'before':row1.actual,
            'after':row2.actual,
        })
    
    return pd.DataFrame(changes)


            

# %% ../nbs/017_eval.ipynb 66
def compare_datasets(ds1,ds2,epsilon=1e-3,log_dir=None):
    """
    Compare two datasets
    """
    
    summary_1 = ds1.summary.sort_values(by='datapoint')
    summary_2 = ds2.summary.sort_values(by='datapoint')

    changed_datapoints = []
    change_summary = []
    detailed_changes = []

    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):
        datapoint = row1.datapoint
        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)
        coverage_change = row1.coverage != row2.coverage

        if distance_change or coverage_change:
            changed_datapoints.append(datapoint)

            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)
            detailed_changes.append(detailed_change)

            change_types = set(detailed_change['change_type'])
            if 'alignment_change' in change_types:
                alignment_change = True
            else:
                alignment_change = False
            
            if 'improvement' in change_types and not 'regression' in change_types:
                score_change = 'improved'
            elif 'regression' in change_types and not 'improvement' in change_types:
                score_change = 'regressed'
            else:
                score_change = 'changed'
            
            total_score_change = row1.distance-row2.distance

            change_summary.append({
                'datapoint':datapoint,
                'alignment_change':alignment_change,
                'score_change_type':score_change,
                'total_score_change':total_score_change,
            })

    changes_summary = pd.DataFrame(change_summary)
    if len(detailed_changes) > 0:
        detailed_changes = pd.concat(detailed_changes)
    else:
        detailed_changes = pd.DataFrame()

    return Comparison(
        from_run=ds1,
        to_run=ds2,
        summary=changes_summary,
        details=detailed_changes,
        log_dir=log_dir,
    )

# %% ../nbs/017_eval.ipynb 77
from typing import Callable,Dict,List,Optional,Tuple

# %% ../nbs/017_eval.ipynb 78
EVAL_COMPARISONS = {
    'eq':eq,
    'eval':safe_eval,
    'chat_eval':chat_eval,
    'cosine_dist':cosine_dist,
}

EVAL_DEFAULT_COMPARISON = cosine_dist

# %% ../nbs/017_eval.ipynb 80
async def eval(
  test_dir:Path,
  out_dir:Path,
  agents:List[Tuple[str,DiagramSchema]],
  k:Optional[int]=5,
  force_run:bool=False,
  silent:bool=False,
  comparisons: Optional[Dict[str,Callable]]=None,
  default_comparison:Optional[Callable]=None,
  ):
  """
  The main eval function.
  Evaluates a set of agents on a set of tests.
  Compares the results of all agents to the first agent.
  pprints a summary of the results to the console.
  and saves all files to the out_dir.

  Args:
    tests_dir: Path to the directory containing the tests.
    out_dir: Path to the directory to write the results to.
    agents: A list of tuples of agent names and their DiagramSchema.
    k: The number of datapoints to print to the summary at most. Defaults to 5.
    force_run: If True, deletes out dir content and reruns the agents. 
      If False, we skip the agents that have already been run.
      Defaults to False.
    silent: If True, dont pprint comparisons. Defaults to False.
    comparisons: A dictionary of comparison names and their functions, to add to the allowed comparisons.
      Defaults to None.
    default_comparison: The default comparison function to use if no comparison is specified.
      This is used to compare the first agent to the rest of the agents.
      Defaults to stringdale.eval.cosine_dist
  """

  global EVAL_COMPARISONS
  global EVAL_DEFAULT_COMPARISON
  if comparisons is not None:
    comparisons = EVAL_COMPARISONS | comparisons
  else: 
    comparisons = EVAL_COMPARISONS
  if default_comparison is None:
    default_comparison = EVAL_DEFAULT_COMPARISON

  eval_dataset_tasks = []
  for agent_name,agent_schema in agents:
    log_dir = f'{out_dir}/logs/{agent_name}'
    eval_dataset_tasks.append(eval_dataset(
      Agent=agent_schema,
      test_dir=test_dir,
      log_dir=log_dir,
      comparisons=comparisons,
      default_comparison=default_comparison))

  datasets = await asyncio.gather(*eval_dataset_tasks,return_exceptions=True)

  for result,(agent_name,_) in zip(datasets,agents):
    if isinstance(result,Exception):
      result.args = (f"When evaluating agent {agent_name}:\n{result.args[0]}", )+ result.args[1:]
      raise result

  first_dataset = datasets[0]
  comparisons = []
  for dataset in datasets[1:]:
    comparisons.append(compare_datasets(first_dataset,dataset))
  
  return datasets,comparisons

  # then, we pprint the summary of each comparison seperately

  # then we group the per datapoint comps across all dataset comparisons by the datapoint id
  # and for each datapoint we pprint a combined datapoint comparison.
  # combined datapoints for each datapoint, the total metrics of each version
  # and then for each comparison that is different from baseline, say how it is different for every version.

  # we return an EvalResult object that tracks the input of the EvalData, but also has the DataSet and DataSetComp objects for each dataset and comparison
  
  pass

