# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/017_eval.ipynb.

# %% auto 0
__all__ = ['logger', 'EVAL_COMPARISONS', 'EVAL_DEFAULT_COMPARISON', 'parse_trace_log', 'cosine_dist', 'eq', 'any', 'safe_eval',
           'DataPoint', 'evaluate_datapoint', 'summarize_datapoint', 'filter_and_concat', 'TestSetRun', 'eval_dataset',
           'Comparison', 'sort_conditions', 'limit_to_datapoint', 'get_datapoint', 'describe_changes',
           'compare_datasets', 'EvalResult', 'eval', 'rprint']

# %% ../nbs/017_eval.ipynb 3
import os
import json
import yaml
from stringdale import (
    Define,
    Scope,
    V,
    E,
    Condition,
    draw_nx
)
from stringdale.stream_warping import (
    TestCase,
    parse_test_case,
    TraceLog,
    event_stream_warp,
    word_overlap,
    regex,
)


from pathlib import Path
from frozendict import frozendict
from .core import  checkLogs,await_all
import pytest
import asyncio
from pydantic import BaseModel, ConfigDict

from typing import List, Union
import jsonlines
import logging


# %% ../nbs/017_eval.ipynb 4
logger = logging.getLogger(__name__)

# %% ../nbs/017_eval.ipynb 7
def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:
    """
    Parse a trace file into a list of Trace objects.
    """
    with jsonlines.open(trace_path) as reader:
        traces = [trace for trace in reader]
        return TraceLog(steps=traces)

# %% ../nbs/017_eval.ipynb 14
import numpy as np
import asyncio
from .db import openai_embed
from .chat import Chat

# %% ../nbs/017_eval.ipynb 15
async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:
    """Compute cosine distance between two strings using OpenAI embeddings.
    
    Args:
        out: First string to compare
        expected: Second string to compare
        model: OpenAI embedding model to use (default: 'text-embedding-3-small')
        
    Returns:
        float: Cosine similarity between the two strings (between -1 and 1)
    """
    # Get embeddings for both strings
    if not isinstance(out,str):
        return np.inf
    if not isinstance(expected,str):
        raise ValueError(f"cosine_dist: expected is not a string: {expected}")
    out_embedding = await openai_embed(out, model=model)
    expected_embedding = await openai_embed(expected, model=model)
    
    # Compute cosine similarity
    dot_product = np.dot(out_embedding, expected_embedding)
    norm_out = np.linalg.norm(out_embedding)
    norm_expected = np.linalg.norm(expected_embedding)
    
    # Return cosine similarity
    return 1-dot_product / (norm_out * norm_expected)



# %% ../nbs/017_eval.ipynb 18
from .core import jinja_undeclared_vars
from typing import Any

# %% ../nbs/017_eval.ipynb 22
def eq(a,b):
    if a == b:
        return 0
    else:
        return np.inf

def any(a,b):
    return 0

# %% ../nbs/017_eval.ipynb 25
from .tools import run_python_code


# %% ../nbs/017_eval.ipynb 26
def safe_eval(out,expression):
    try:
        formatted_expressions = expression.format(out)
    except Exception as e:
        logger.warning(f"Error formatting expression: {expression} with value {out}, error: {e}")
        return np.inf
    value = run_python_code(formatted_expressions)
    if isinstance(value,str) and value.startswith("Error"):
        logger.warning(
            f"Error evaluating expression: {formatted_expressions} = {value}\n"
            f"out: {out}\n"
            f"expression: {expression}\n"
            f"error: {e}"
        )
        return np.inf
    logger.debug(f"safe_eval: {formatted_expressions} = {value}")
    if isinstance(value,bool):
        return 0 if value else np.inf
    elif isinstance(value,float):
        return value
    else:
        logger.debug(
            f"When evaluating {expression} with value {out}\n"
            f"Expected float or bool, got {type(value)} with value {repr(value)}"
            )
        return np.inf

# %% ../nbs/017_eval.ipynb 30
from typing import List,Dict,Callable

# %% ../nbs/017_eval.ipynb 31
class DataPoint(BaseModel):
    traces:TraceLog
    expected:TestCase
    

# %% ../nbs/017_eval.ipynb 32
async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):
    d=Agent()
    with jsonlines.open(trace_log_path,'w') as writer:
        for input in test_case.inputs:
            async for trace in d.arun(input):
                if trace.node_func is None:
                    continue
                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))
            if d.finished:
                break

async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):
    if trace_log_path is None:
        trace_log_path = test_case_path.parent/test_case_path.name.replace(".yaml", ".jsonl").replace("expected", "actual")

    if not trace_log_path.parent.exists():
        os.makedirs(trace_log_path.parent,exist_ok=True)
    try:
        test_case = parse_test_case(test_case_path)
    except Exception as e:
        raise ValueError(f"Error parsing test case {test_case_path}: {e}") from e
        

    if force_run or not trace_log_path.exists():
        if not trace_log_path.exists():
            logger.info(f"Trace file {trace_log_path.name} does not exist, running agent")
        else:
            logger.info(f"Force running {trace_log_path.name}")
        await _run_agent(Agent,test_case,trace_log_path)
    else:
        logger.info(f"Trace file {trace_log_path.name} already exists, skipping agent run")

    parsed_trace = parse_trace_log(trace_log_path)
    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)
    
    return aligned_trace,score,debug_info,trace_log_path


# %% ../nbs/017_eval.ipynb 35
with checkLogs():
    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)

assert alignment is None
alignment,score,trace_out


# %% ../nbs/017_eval.ipynb 37
import pandas as pd

# %% ../nbs/017_eval.ipynb 38
def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):
    """
    Reorder the columns of a pandas dataframe to put the first_columns first.
    """
    return df[first_columns + [c for c in df.columns if c not in first_columns]]



# %% ../nbs/017_eval.ipynb 41
from copy import deepcopy
from itertools import count


# %% ../nbs/017_eval.ipynb 42
def summarize_datapoint(name,alignment,debug_info):
    """
    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage
    by using the alignment and the debug info
    """
    deep_dive_fit = []

    comp_counter = count()
    for expected_node_id,trace_idx in alignment.items():
        match_data = debug_info[expected_node_id][trace_idx]
        for comp in match_data['comparisons']:
            summary = deepcopy(match_data) | deepcopy(comp) 
            summary['comp_id'] = next(comp_counter)
            summary.pop('comparisons')
            summary['aggregation'] = comp['aggregation']
            deep_dive_fit.append(summary)


    df = pd.DataFrame(deep_dive_fit)
    df['datapoint'] = str(name)
    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])
    return df

# %% ../nbs/017_eval.ipynb 46
def filter_and_concat(df1: pd.DataFrame, df2: pd.DataFrame, keys: list) -> pd.DataFrame:
    """
    Filter df1 by removing rows with matching key values in df2, then concatenate with df2.
    
    Args:
        df1 (pd.DataFrame): First DataFrame to filter
        df2 (pd.DataFrame): Second DataFrame to concatenate
        keys (list): List of column names to use as keys for matching
        
    Returns:
        pd.DataFrame: Concatenated DataFrame with filtered df1 and df2
    """
    if df1.empty:
        return df2
    if df2.empty:
        return df1
    # Create tuples of key values for comparison
    mask = df1[keys].apply(tuple, axis=1).isin(df2[keys].apply(tuple, axis=1))
    
    # Filter df1 to keep only rows that don't exist in df2 (using inverse mask)
    df1_filtered = df1[~mask]
    
    # Concatenate the filtered df1 with df2
    result = pd.concat([df1_filtered, df2], ignore_index=True)
    
    return result

# %% ../nbs/017_eval.ipynb 48
from . import DiagramSchema
from pprint import pprint, pformat
from fastcore.basics import patch
from typing import Optional
from pydantic import BaseModel, ConfigDict, PrivateAttr


# %% ../nbs/017_eval.ipynb 49
class TestSetRun(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    # Private attributes
    _summary_dict: dict = PrivateAttr(default_factory=dict)
    _details_dict: dict = PrivateAttr(default_factory=dict)
    
    # Regular fields
    test_dir: Path
    dir: Path
    summary: pd.DataFrame
    details: pd.DataFrame
    debug: dict

    def find_cases(self):
        yaml_paths =  list(self.test_dir.glob("**/*.yaml"))
        return [str(p.relative_to(self.test_dir).with_suffix("")) for p in yaml_paths]

    def trace_log_path(self,datapoint:str):
        return self.dir/'logs'/f'{datapoint}.jsonl'

    def trace_log_len(self,datapoint:str):
        log_path = self.trace_log_path(datapoint)
        return len(log_path.read_text().splitlines())

    def testcase_path(self,datapoint:str):
        return self.test_dir/f'{datapoint}.yaml'

    def serialize_test_case(self,datapoint:str):
        yml_version = yaml.safe_load(self.testcase_path(datapoint).read_text())
        json_version = json.dumps(yml_version,indent=2)
        return json_version

    def datapoint_len(self,datapoint:str):
        datapoint_yaml = (self.test_dir/datapoint).with_suffix(".yaml")
        return trace_log_len(self.dir/datapoint_yaml)
        
    def __repr__(self):     
        return (
            f"TestSetRun(\n"
            f"  test_dir={self.test_dir}, \n"
            f"  dir={self.dir}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f"  debug=dict)")

    def __str__(self):
        return self.__repr__()

    def save(self,dir:Path):
        self.summary.to_csv(dir/"summary.csv",index=False)
        self.details.to_csv(dir/"details.csv",index=False)
        test_dir_rel = Path(os.path.relpath(self.test_dir,self.dir))
        (dir/'test_cases_loc.txt').write_text(str(test_dir_rel))
        with open(dir/"debug.json","w") as f:
            json.dump(self.debug,f)

    @classmethod
    def load(cls, dir: Path,test_dir:Optional[Path]=None):
        # Initialize empty DataFrames and dict for missing files
        summary = pd.DataFrame()
        details = pd.DataFrame()
        debug = {}
        if test_dir is None:
            test_dir = dir
        
        # Try to load files if they exist
        try:
            if (dir/"summary.csv").exists():
                summary = pd.read_csv(dir/"summary.csv",index_col=False)
            if (dir/"details.csv").exists():
                details = pd.read_csv(dir/"details.csv",index_col=False)
            if (dir/"debug.json").exists():
                with open(dir/"debug.json") as f:
                    debug = json.load(f)
            if (dir/'test_cases_loc.txt').exists():
                test_cases_loc = (dir/'test_cases_loc.txt').read_text().strip()
                test_cases_loc = dir/test_cases_loc
        except Exception as e:
            # Log the error but continue with empty/default values
            print(f"Warning: Error loading some files: {str(e)}")
        
        return cls(
            test_dir=test_dir,
            dir=dir,
            summary=summary,
            details=details,
            debug=debug
        )

    def is_datapoint_stale(self,datapoint_path):
        if self.summary.empty:
            return True
        datapoints  = self.summary['datapoint'].unique().tolist()
        # if the datapoint is not in the summary, it is stale
        if datapoint_path not in datapoints:
            return True
        
        summarized_test_case = self.summary.loc[self.summary['datapoint'] == datapoint_path]['serialized_test_case'].iloc[0]
        current_test_case = self.serialize_test_case(datapoint_path)
        return summarized_test_case != current_test_case
        


# %% ../nbs/017_eval.ipynb 55
async def eval_dataset(Agent:DiagramSchema,test_dir,out_dir,comparisons,default_comparison,force_run=False):

    run = TestSetRun.load(out_dir)
    run.test_dir = test_dir    
    datapoints = run.find_cases()

    if not force_run:
        stale_datapoints = [p for p in datapoints if run.is_datapoint_stale(p)]
    else:
        stale_datapoints = datapoints

    if len(stale_datapoints) > 0:
        logger.info(f"{run.dir.name}: Evaluating {len(stale_datapoints)}/{len(datapoints)} datapoints")
    else:
        logger.info(f"{run.dir.name}: No stale datapoints, skipping evaluation")
        return run
    
    datapoint_results = await await_all(
        [
            evaluate_datapoint(
                Agent=Agent,
                comparisons=comparisons,
                default_comparison=default_comparison,
                test_case_path=run.testcase_path(datapoint),
                trace_log_path=run.trace_log_path(datapoint),
                force_run=True, # since we computed which datapoints to run, we can force run them
            ) for datapoint in stale_datapoints
        ],
        error_prefix=[
            f"When evaluating datapoint {datapoint}"
            for datapoint in stale_datapoints
        ]
    )
    
    summary_data = list()
    deep_dives = list()
    debug_infos = dict()

    for (alignment,score,debug_info,trace_out),datapoint in zip(datapoint_results,stale_datapoints):
        debug_infos[datapoint] = debug_info
        deep_dive = summarize_datapoint(datapoint,alignment,debug_info)
        deep_dive['datapoint'] = datapoint
        deep_dives.append(deep_dive)
        summary_data.append({
            'datapoint':str(datapoint),
            'distance':score,
            'avg_distance':deep_dive.distance.mean(),
            'coverage':len(alignment) / run.trace_log_len(datapoint),
            'alignment':alignment,
            'serialized_test_case':run.serialize_test_case(datapoint)
            })

    
    new_summary = pd.DataFrame.from_records(summary_data).reset_index(drop=True)
    run.summary = filter_and_concat(run.summary,new_summary,['datapoint'])

    details_data = pd.concat(deep_dives,ignore_index=True)
    run.details = filter_and_concat(run.details,details_data,['datapoint'])
    run.debug = {**run.debug,**debug_infos}
    run.save(out_dir)
    
    return run

# %% ../nbs/017_eval.ipynb 66
import math
from typing import Optional
import textwrap

# %% ../nbs/017_eval.ipynb 67
class Comparison(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    from_run: TestSetRun
    to_run: TestSetRun
    dir: Optional[Path]
    summary: pd.DataFrame
    details: pd.DataFrame

    def __repr__(self):     
        return (
            f"Comparison(\n"
            f"  from_run={textwrap.indent(self.from_run.__repr__(), '  ').strip()}, \n"
            f"  to_run={textwrap.indent(self.to_run.__repr__(), '  ').strip()}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f")")

    def __str__(self):
        return self.__repr__()

    def save(self):
        if self.dir is None:
            return
        self.dir.mkdir(parents=True,exist_ok=True)
        self.summary.to_csv(self.dir/"summary.csv",index=False)
        self.details.to_csv(self.dir/"details.csv",index=False)
    


# %% ../nbs/017_eval.ipynb 68
def sort_conditions(df):
    return df.sort_values(by=['node_idx','comparison','expected'])

def limit_to_datapoint(df,datapoint):
    return df.loc[df['datapoint'] == datapoint]

def get_datapoint(ds,datapoint):
    return sort_conditions(limit_to_datapoint(ds.details,datapoint))


def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):
    """
    Describe the changes between two datapoints
    """
    
    # get the detailed version of the datasets and limit to only rows of the given datapoint
    datapoint_df1 = get_datapoint(ds1,datapoint)
    datapoint_df2 = get_datapoint(ds2,datapoint)

    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)
    # lets assert this in the code
    assert datapoint_df1.shape == datapoint_df2.shape, f"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}"

    changes = []

    # get the first comparison whose node aligned to a different trace
    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if row1.trace_idx != row2.trace_idx:
            changes.append({
                'datapoint':datapoint,
                'change_type':'alignment_change',
                'before':row1.trace_idx,
                'after':row2.trace_idx,
                'comparison_id':row1.comp_id,
            })
            break
            

    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):
            continue
        if row2.distance + epsilon > row1.distance:
            change_types = 'regressed'
        elif row2.distance - epsilon < row1.distance:
            change_types = 'improved'
        else:
            continue
        
        changes.append({
            'datapoint':datapoint,
            'change_type':change_types,
            'value':row1.distance - row2.distance,
            'comp_id':row1.comp_id,
            'node_label':row1.node_label,
            'expected':row1.expected,
            'before':row1.actual,
            'after':row2.actual,
        })
    
    return pd.DataFrame(changes)


            

# %% ../nbs/017_eval.ipynb 70
def compare_datasets(ds1,ds2,epsilon=1e-3,out_dir=None):
    """
    Compare two datasets
    """
    
    summary_1 = ds1.summary.sort_values(by='datapoint')
    summary_2 = ds2.summary.sort_values(by='datapoint')

    changed_datapoints = []
    change_summary = []
    detailed_changes = []

    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):
        datapoint = row1.datapoint
        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)
        coverage_change = row1.coverage != row2.coverage

        if distance_change or coverage_change:
            changed_datapoints.append(datapoint)

            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)
            detailed_changes.append(detailed_change)

            change_types = set(detailed_change['change_type'])
            if 'alignment_change' in change_types:
                alignment_change = True
            else:
                alignment_change = False
            
            if 'improved' in change_types and not 'regressed' in change_types:
                score_change = 'improved'
            elif 'regressed' in change_types and not 'improved' in change_types:
                score_change = 'regressed'
            else:
                score_change = 'changed'
            
            total_score_change = row1.distance-row2.distance

            change_summary.append({
                'datapoint':datapoint,
                'alignment_change':alignment_change,
                'score_change_type':score_change,
                'total_score_change':total_score_change,
            })

    changes_summary = pd.DataFrame(change_summary)
    if len(detailed_changes) > 0:
        detailed_changes = pd.concat(detailed_changes)
    else:
        detailed_changes = pd.DataFrame()

    comp =  Comparison(
        from_run=ds1,
        to_run=ds2,
        summary=changes_summary,
        details=detailed_changes,
        dir=out_dir,
    )
    comp.save()
    return comp

# %% ../nbs/017_eval.ipynb 80
from typing import Callable,Dict,List,Optional,Tuple

# %% ../nbs/017_eval.ipynb 81
EVAL_COMPARISONS = {
    'eq':eq,
    'eval':safe_eval,
    'chat_eval':chat_eval,
    'cosine_dist':cosine_dist,
}

EVAL_DEFAULT_COMPARISON = 'cosine_dist'

# %% ../nbs/017_eval.ipynb 82
class EvalResult(BaseModel):
    """
    A class to track evaluation results, including individual runs and comparisons between runs.
    
    Attributes:
        runs (Dict[str, TestSetRun]): Dictionary mapping agent names to their test run results
        comparisons (Dict[Tuple[str, str], Comparison]): Dictionary mapping pairs of agent names 
            (base_run, other_run) to their comparison results
    """
    model_config = ConfigDict(arbitrary_types_allowed=True)
    
    runs: Dict[str, TestSetRun]
    comparisons: Dict[Tuple[str, str], Comparison]
    
    def __repr__(self) -> str:
        runs_str = f"runs: {list(self.runs.keys())}"
        comparisons_str = f"comparisons: {list(self.comparisons.keys())}"
        return f"EvalResult(\n  {runs_str},\n  {comparisons_str}\n)"
    
    def __str__(self) -> str:
        return self.__repr__()

# %% ../nbs/017_eval.ipynb 83
async def eval(
  test_dir:Path,
  out_dir:Path,
  agents:List[Tuple[str,DiagramSchema]],
  k:Optional[int]=5,
  force_run:bool=False,
  silent:bool=False,
  comparisons: Optional[Dict[str,Callable]]=None,
  default_comparison:Optional[Callable]=None,
  ):
  """
  The main eval function.
  Evaluates a set of agents on a set of tests.
  Compares the results of all agents to the first agent.
  pprints a summary of the results to the console.
  and saves all files to the out_dir.

  Args:
    tests_dir: Path to the directory containing the tests.
    out_dir: Path to the directory to write the results to.
    agents: A list of tuples of agent names and their DiagramSchema.
    k: The number of datapoints to print to the summary at most. Defaults to 5.
    force_run: If True, deletes out dir content and reruns the agents. 
      If False, we skip the agents that have already been run.
      Defaults to False.
    silent: If True, dont pprint comparisons. Defaults to False.
    comparisons: A dictionary of comparison names and their functions, to add to the allowed comparisons.
      Defaults to None.
    default_comparison: The default comparison function to use if no comparison is specified.
      This is used to compare the first agent to the rest of the agents.
      Defaults to stringdale.eval.cosine_dist
  """

  global EVAL_COMPARISONS
  global EVAL_DEFAULT_COMPARISON
  if comparisons is not None:
    comparisons = EVAL_COMPARISONS | comparisons
  else: 
    comparisons = EVAL_COMPARISONS
  if default_comparison is None:
    default_comparison = EVAL_DEFAULT_COMPARISON

  eval_dataset_tasks = []
  for agent_name,agent_schema in agents:
    log_dir = out_dir / 'logs'/ agent_name
    eval_dataset_tasks.append(eval_dataset(
      Agent=agent_schema,
      test_dir=test_dir,
      out_dir=log_dir,
      comparisons=comparisons,
      default_comparison=default_comparison))

  datasets = await asyncio.gather(*eval_dataset_tasks,return_exceptions=True)

  for result,(agent_name,_) in zip(datasets,agents):
    if isinstance(result,Exception):
      result.args = (f"When evaluating agent {agent_name}:\n{result.args[0]}", )+ result.args[1:]
      raise result

  first_dataset = datasets[0]
  comparisons = dict()
  for dataset in datasets[1:]:
    comp_dir = out_dir / 'comparisons' / f'{first_dataset.dir.name}_{dataset.dir.name}'
    comp = compare_datasets(first_dataset,dataset,out_dir=comp_dir)
    comparisons[(first_dataset.dir.name,dataset.dir.name)] = comp
  
  res = EvalResult(
    runs={agent_name:dataset for (agent_name,_),dataset in zip(agents,datasets)},
    comparisons=comparisons)


  return res



# %% ../nbs/017_eval.ipynb 93
import rich
from rich.padding import Padding

# %% ../nbs/017_eval.ipynb 94
def rprint(obj,indent:int=0,sep_by:int=2):
    rich.print(Padding(obj,pad=(0,0,0,indent*sep_by)))
