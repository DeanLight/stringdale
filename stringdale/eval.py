# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/017_eval.ipynb.

# %% auto 0
__all__ = ['logger', 'datapoint_template', 'summary_template', 'comparison_summary_template', 'EVAL_COMPARISONS',
           'EVAL_DEFAULT_COMPARISON', 'parse_trace_log', 'cosine_dist', 'eq', 'any', 'safe_eval', 'DataPoint',
           'evaluate_datapoint', 'summarize_datapoint', 'filter_and_concat', 'parse_json', 'TestSetRun', 'eval_dataset',
           'Comparison', 'sort_conditions', 'limit_to_datapoint', 'get_datapoint', 'describe_changes',
           'compare_datasets', 'EvalResult', 'rprint', 'pprint_run_summary', 'pprint_comparison_summary', 'eval']

# %% ../nbs/017_eval.ipynb 3
import os
import json
import yaml
from stringdale import (
    Define,
    Scope,
    V,
    E,
    Condition,
    draw_nx
)
from stringdale.stream_warping import (
    TestCase,
    parse_test_case,
    TraceLog,
    event_stream_warp,
    word_overlap,
    regex,
)


from pathlib import Path
from frozendict import frozendict
from .core import  checkLogs,await_all
import pytest
import asyncio
from pydantic import BaseModel, ConfigDict

from typing import List, Union
import jsonlines
import logging


# %% ../nbs/017_eval.ipynb 4
logger = logging.getLogger(__name__)

# %% ../nbs/017_eval.ipynb 7
def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:
    """
    Parse a trace file into a list of Trace objects.
    """
    with jsonlines.open(trace_path) as reader:
        traces = [trace for trace in reader]
        return TraceLog(steps=traces)

# %% ../nbs/017_eval.ipynb 14
import numpy as np
import asyncio
from .db import openai_embed
from .chat import Chat

# %% ../nbs/017_eval.ipynb 15
async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:
    """Compute cosine distance between two strings using OpenAI embeddings.
    
    Args:
        out: First string to compare
        expected: Second string to compare
        model: OpenAI embedding model to use (default: 'text-embedding-3-small')
        
    Returns:
        float: Cosine similarity between the two strings (between -1 and 1)
    """
    # Get embeddings for both strings
    if not isinstance(out,str):
        return np.inf
    if not isinstance(expected,str):
        raise ValueError(f"cosine_dist: expected is not a string: {expected}")
    out_embedding = await openai_embed(out, model=model)
    expected_embedding = await openai_embed(expected, model=model)
    
    # Compute cosine similarity
    dot_product = np.dot(out_embedding, expected_embedding)
    norm_out = np.linalg.norm(out_embedding)
    norm_expected = np.linalg.norm(expected_embedding)
    
    # Return cosine similarity
    return 1-dot_product / (norm_out * norm_expected)



# %% ../nbs/017_eval.ipynb 18
from .core import jinja_undeclared_vars
from typing import Any

# %% ../nbs/017_eval.ipynb 22
def eq(a,b):
    if a == b:
        return 0
    else:
        return np.inf

def any(a,b):
    return 0

# %% ../nbs/017_eval.ipynb 25
from .tools import run_python_code


# %% ../nbs/017_eval.ipynb 26
def safe_eval(out,expression):
    try:
        formatted_expressions = expression.format(out)
    except Exception as e:
        logger.warning(f"Error formatting expression: {expression} with value {out}, error: {e}")
        return np.inf
    value = run_python_code(formatted_expressions)
    if isinstance(value,str) and value.startswith("Error"):
        logger.warning(
            f"Error evaluating expression: {formatted_expressions} = {value}\n"
            f"out: {out}\n"
            f"expression: {expression}\n"
            f"error: {e}"
        )
        return np.inf
    logger.debug(f"safe_eval: {formatted_expressions} = {value}")
    if isinstance(value,bool):
        return 0 if value else np.inf
    elif isinstance(value,float):
        return value
    else:
        logger.debug(
            f"When evaluating {expression} with value {out}\n"
            f"Expected float or bool, got {type(value)} with value {repr(value)}"
            )
        return np.inf

# %% ../nbs/017_eval.ipynb 30
from typing import List,Dict,Callable

# %% ../nbs/017_eval.ipynb 31
class DataPoint(BaseModel):
    traces:TraceLog
    expected:TestCase
    

# %% ../nbs/017_eval.ipynb 32
async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):
    d=Agent()
    with jsonlines.open(trace_log_path,'w') as writer:
        for input in test_case.inputs:
            async for trace in d.arun(input):
                if trace.node_func is None:
                    continue
                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))
            if d.finished:
                break

async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):
    if trace_log_path is None:
        trace_log_path = test_case_path.parent/test_case_path.name.replace(".yaml", ".jsonl").replace("expected", "actual")

    if not trace_log_path.parent.exists():
        os.makedirs(trace_log_path.parent,exist_ok=True)
    try:
        test_case = parse_test_case(test_case_path)
    except Exception as e:
        raise ValueError(f"Error parsing test case {test_case_path}: {e}") from e
        

    if force_run or not trace_log_path.exists():
        if not trace_log_path.exists():
            logger.info(f"Trace file {trace_log_path.name} does not exist, running agent")
        else:
            logger.info(f"Force running {trace_log_path.name}")
        await _run_agent(Agent,test_case,trace_log_path)
    else:
        logger.info(f"Trace file {trace_log_path.name} already exists, skipping agent run")

    parsed_trace = parse_trace_log(trace_log_path)
    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)
    
    return aligned_trace,score,debug_info,trace_log_path


# %% ../nbs/017_eval.ipynb 35
with checkLogs():
    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)

assert alignment is None
alignment,score,trace_out


# %% ../nbs/017_eval.ipynb 37
import pandas as pd

# %% ../nbs/017_eval.ipynb 38
def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):
    """
    Reorder the columns of a pandas dataframe to put the first_columns first.
    """
    return df[first_columns + [c for c in df.columns if c not in first_columns]]



# %% ../nbs/017_eval.ipynb 41
from copy import deepcopy
from itertools import count


# %% ../nbs/017_eval.ipynb 42
def summarize_datapoint(name,alignment,debug_info):
    """
    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage
    by using the alignment and the debug info
    """
    deep_dive_fit = []

    comp_counter = count()
    for expected_node_id,trace_idx in alignment.items():
        match_data = debug_info[expected_node_id][trace_idx]
        for comp in match_data['comparisons']:
            summary = deepcopy(match_data) | deepcopy(comp) 
            summary['comp_id'] = next(comp_counter)
            summary.pop('comparisons')
            summary['aggregation'] = comp['aggregation']
            deep_dive_fit.append(summary)


    df = pd.DataFrame(deep_dive_fit)
    df['datapoint'] = str(name)
    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])
    return df

# %% ../nbs/017_eval.ipynb 46
def filter_and_concat(df1: pd.DataFrame, df2: pd.DataFrame, keys: list) -> pd.DataFrame:
    """
    Filter df1 by removing rows with matching key values in df2, then concatenate with df2.
    
    Args:
        df1 (pd.DataFrame): First DataFrame to filter
        df2 (pd.DataFrame): Second DataFrame to concatenate
        keys (list): List of column names to use as keys for matching
        
    Returns:
        pd.DataFrame: Concatenated DataFrame with filtered df1 and df2
    """
    if df1.empty:
        return df2
    if df2.empty:
        return df1
    # Create tuples of key values for comparison
    mask = df1[keys].apply(tuple, axis=1).isin(df2[keys].apply(tuple, axis=1))
    
    # Filter df1 to keep only rows that don't exist in df2 (using inverse mask)
    df1_filtered = df1[~mask]
    
    # Concatenate the filtered df1 with df2
    result = pd.concat([df1_filtered, df2], ignore_index=True)
    
    return result

# %% ../nbs/017_eval.ipynb 48
from . import DiagramSchema
from pprint import pprint, pformat
from fastcore.basics import patch
from typing import Optional
from pydantic import BaseModel, ConfigDict, PrivateAttr
import pandas as pd
import json


# %% ../nbs/017_eval.ipynb 49
# Define a JSON parser function that handles potential errors
def parse_json(data):
    try:
        # Handle cases where JSON might use single quotes instead of double quotes
        if isinstance(data, str):
            data = data.replace("'", '"')
        return json.loads(data)
    except (json.JSONDecodeError, TypeError):
        return None

# %% ../nbs/017_eval.ipynb 50
class TestSetRun(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    # Private attributes
    _summary_dict: dict = PrivateAttr(default_factory=dict)
    _details_dict: dict = PrivateAttr(default_factory=dict)
    
    # Regular fields
    test_dir: Path
    dir: Path
    summary: pd.DataFrame
    details: pd.DataFrame
    debug: dict

    def find_cases(self):
        yaml_paths =  list(self.test_dir.glob("**/*.yaml"))
        return [str(p.relative_to(self.test_dir).with_suffix("")) for p in yaml_paths]

    def trace_log_path(self,datapoint:str):
        return self.dir/'logs'/f'{datapoint}.jsonl'

    def trace_log_len(self,datapoint:str):
        log_path = self.trace_log_path(datapoint)
        return len(log_path.read_text().splitlines())

    def testcase_path(self,datapoint:str):
        return self.test_dir/f'{datapoint}.yaml'

    def serialize_test_case(self,datapoint:str):
        yml_version = yaml.safe_load(self.testcase_path(datapoint).read_text())
        json_version = json.dumps(yml_version,indent=2)
        return json_version

    def datapoint_len(self,datapoint:str):
        datapoint_yaml = (self.test_dir/datapoint).with_suffix(".yaml")
        return trace_log_len(self.dir/datapoint_yaml)
        
    def __repr__(self):     
        return (
            f"TestSetRun(\n"
            f"  test_dir={self.test_dir}, \n"
            f"  dir={self.dir}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f"  debug=dict)")

    def __str__(self):
        return self.__repr__()

    def save(self,dir:Path):
        self.summary.to_csv(dir/"summary.csv",index=False)
        self.details.to_csv(dir/"details.csv",index=False)
        test_dir_rel = Path(os.path.relpath(self.test_dir,self.dir))
        (dir/'test_cases_loc.txt').write_text(str(test_dir_rel))
        with open(dir/"debug.json","w") as f:
            json.dump(self.debug,f)

    @classmethod
    def load(cls, dir: Path,test_dir:Optional[Path]=None):
        # Initialize empty DataFrames and dict for missing files
        summary = pd.DataFrame()
        details = pd.DataFrame()
        debug = {}
        if test_dir is None:
            test_dir = dir
        
        # Try to load files if they exist
        try:
            if (dir/"summary.csv").exists():
                summary = pd.read_csv(dir/"summary.csv",index_col=False)
            if (dir/"details.csv").exists():
                details = pd.read_csv(dir/"details.csv",index_col=False,converters={'kwargs':parse_json})
            if (dir/"debug.json").exists():
                with open(dir/"debug.json") as f:
                    debug = json.load(f)
            if (dir/'test_cases_loc.txt').exists():
                test_cases_loc = (dir/'test_cases_loc.txt').read_text().strip()
                test_cases_loc = dir/test_cases_loc
        except Exception as e:
            # Log the error but continue with empty/default values
            print(f"Warning: Error loading some files: {str(e)}")
        
        return cls(
            test_dir=test_dir,
            dir=dir,
            summary=summary,
            details=details,
            debug=debug
        )

    def is_datapoint_stale(self,datapoint_path):
        if self.summary.empty:
            return True
        datapoints  = self.summary['datapoint'].unique().tolist()
        # if the datapoint is not in the summary, it is stale
        if datapoint_path not in datapoints:
            return True
        
        summarized_test_case = self.summary.loc[self.summary['datapoint'] == datapoint_path]['serialized_test_case'].iloc[0]
        current_test_case = self.serialize_test_case(datapoint_path)
        return summarized_test_case != current_test_case
        


# %% ../nbs/017_eval.ipynb 56
async def eval_dataset(Agent:DiagramSchema,test_dir,out_dir,comparisons,default_comparison,force_run=False):

    run = TestSetRun.load(out_dir)
    run.test_dir = test_dir    
    datapoints = run.find_cases()

    if not force_run:
        stale_datapoints = [p for p in datapoints if run.is_datapoint_stale(p)]
    else:
        stale_datapoints = datapoints

    if len(stale_datapoints) > 0:
        logger.info(f"{run.dir.name}: Evaluating {len(stale_datapoints)}/{len(datapoints)} datapoints")
    else:
        logger.info(f"{run.dir.name}: No stale datapoints, skipping evaluation")
        return run
    
    datapoint_results = await await_all(
        [
            evaluate_datapoint(
                Agent=Agent,
                comparisons=comparisons,
                default_comparison=default_comparison,
                test_case_path=run.testcase_path(datapoint),
                trace_log_path=run.trace_log_path(datapoint),
                force_run=True, # since we computed which datapoints to run, we can force run them
            ) for datapoint in stale_datapoints
        ],
        error_prefix=[
            f"When evaluating datapoint {datapoint}"
            for datapoint in stale_datapoints
        ]
    )
    
    summary_data = list()
    deep_dives = list()
    debug_infos = dict()

    for (alignment,score,debug_info,trace_out),datapoint in zip(datapoint_results,stale_datapoints):
        debug_infos[datapoint] = debug_info
        deep_dive = summarize_datapoint(datapoint,alignment,debug_info)
        deep_dive['datapoint'] = datapoint
        deep_dives.append(deep_dive)
        summary_data.append({
            'datapoint':str(datapoint),
            'distance':score,
            'avg_distance':deep_dive.distance.mean(),
            'coverage':len(alignment) / run.trace_log_len(datapoint),
            'alignment':alignment,
            'serialized_test_case':run.serialize_test_case(datapoint)
            })

    
    new_summary = pd.DataFrame.from_records(summary_data).reset_index(drop=True)
    run.summary = filter_and_concat(run.summary,new_summary,['datapoint'])

    details_data = pd.concat(deep_dives,ignore_index=True)
    run.details = filter_and_concat(run.details,details_data,['datapoint'])
    run.debug = {**run.debug,**debug_infos}
    run.save(out_dir)
    
    return run

# %% ../nbs/017_eval.ipynb 67
import math
from typing import Optional
import textwrap

# %% ../nbs/017_eval.ipynb 68
class Comparison(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    from_run: TestSetRun
    to_run: TestSetRun
    dir: Optional[Path]
    summary: pd.DataFrame
    details: pd.DataFrame

    def __repr__(self):     
        return (
            f"Comparison(\n"
            f"  from_run={textwrap.indent(self.from_run.__repr__(), '  ').strip()}, \n"
            f"  to_run={textwrap.indent(self.to_run.__repr__(), '  ').strip()}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f")")

    def __str__(self):
        return self.__repr__()

    def save(self):
        if self.dir is None:
            return
        self.dir.mkdir(parents=True,exist_ok=True)
        self.summary.to_csv(self.dir/"summary.csv",index=False)
        self.details.to_csv(self.dir/"details.csv",index=False)
    


# %% ../nbs/017_eval.ipynb 69
def sort_conditions(df):
    return df.sort_values(by=['node_idx','comparison','expected'])

def limit_to_datapoint(df,datapoint):
    return df.loc[df['datapoint'] == datapoint]

def get_datapoint(ds,datapoint):
    return sort_conditions(limit_to_datapoint(ds.details,datapoint))


def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):
    """
    Describe the changes between two datapoints
    """
    
    # get the detailed version of the datasets and limit to only rows of the given datapoint
    datapoint_df1 = get_datapoint(ds1,datapoint)
    datapoint_df2 = get_datapoint(ds2,datapoint)

    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)
    # lets assert this in the code
    assert datapoint_df1.shape == datapoint_df2.shape, f"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}"

    changes = []

    # get the first comparison whose node aligned to a different trace
    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if row1.trace_idx != row2.trace_idx:
            changes.append({
                'datapoint':datapoint,
                'change_type':'alignment_change',
                'before':row1.trace_idx,
                'after':row2.trace_idx,
                'comparison_id':row1.comp_id,
            })
            break
            

    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):
            continue
        if row2.distance + epsilon > row1.distance:
            change_types = 'regressed'
        elif row2.distance - epsilon < row1.distance:
            change_types = 'improved'
        else:
            continue
        
        changes.append({
            'datapoint':datapoint,
            'change_type':change_types,
            'value':row1.distance - row2.distance,
            'comp_id':row1.comp_id,
            'node_label':row1.node_label,
            'expected':row1.expected,
            'before':row1.actual,
            'after':row2.actual,
        })
    
    return pd.DataFrame(changes)


            

# %% ../nbs/017_eval.ipynb 71
def compare_datasets(ds1,ds2,epsilon=1e-3,out_dir=None):
    """
    Compare two datasets
    """
    
    summary_1 = ds1.summary.sort_values(by='datapoint')
    summary_2 = ds2.summary.sort_values(by='datapoint')

    changed_datapoints = []
    change_summary = []
    detailed_changes = []

    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):
        datapoint = row1.datapoint
        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)
        coverage_change = row1.coverage != row2.coverage

        if distance_change or coverage_change:
            changed_datapoints.append(datapoint)

            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)
            detailed_changes.append(detailed_change)

            change_types = set(detailed_change['change_type'])
            if 'alignment_change' in change_types:
                alignment_change = True
            else:
                alignment_change = False
            
            if 'improved' in change_types and not 'regressed' in change_types:
                score_change = 'improved'
            elif 'regressed' in change_types and not 'improved' in change_types:
                score_change = 'regressed'
            else:
                score_change = 'changed'
            
            total_score_change = row1.distance-row2.distance

            change_summary.append({
                'datapoint':datapoint,
                'alignment_change':alignment_change,
                'score_change_type':score_change,
                'total_score_change':total_score_change,
            })

    changes_summary = pd.DataFrame(change_summary)
    if len(detailed_changes) > 0:
        detailed_changes = pd.concat(detailed_changes)
    else:
        detailed_changes = pd.DataFrame()

    comp =  Comparison(
        from_run=ds1,
        to_run=ds2,
        summary=changes_summary,
        details=detailed_changes,
        dir=out_dir,
    )
    comp.save()
    return comp

# %% ../nbs/017_eval.ipynb 81
from typing import Callable,Dict,List,Optional,Tuple

# %% ../nbs/017_eval.ipynb 82
class EvalResult():
    """
    A class to track evaluation results, including individual runs and comparisons between runs.
    
    Attributes:
        runs (Dict[str, TestSetRun]): Dictionary mapping agent names to their test run results
        comparisons (Dict[Tuple[str, str], Comparison]): Dictionary mapping pairs of agent names 
            (base_run, other_run) to their comparison results
    """

    def __init__(self,runs:Dict[str,TestSetRun],comparisons:Dict[Tuple[str,str],Comparison],eval_funcs:Dict[str,Callable],default_func:Callable):
        self.runs = runs
        self.comparisons = comparisons
        self.eval_funcs = eval_funcs
        self.default_func = default_func

        run_summaries = []
        run_details = []
        comp_summaries = []
        comp_details = []
        for run_name,run in runs.items():
            run_s = run.summary.copy()
            run_s['agent'] = run_name
            run_summaries.append(run_s)
            run_d = run.details.copy()
            run_d['agent'] = run_name
            run_details.append(run_d)
        for (base_run,other_run),comparison in comparisons.items():
            comp_s = comparison.summary.copy()
            comp_s['from_agent'] = base_run
            comp_s['to_agent'] = other_run
            comp_summaries.append(comp_s)
            comp_d = comparison.details.copy()
            comp_d['from_agent'] = base_run
            comp_d['to_agent'] = other_run
            comp_details.append(comp_d)

        self.run_summaries = _pd_order_columns_first( pd.concat(run_summaries),['agent'])
        self.run_details = _pd_order_columns_first( pd.concat(run_details),['agent'])
        self.comp_summaries = _pd_order_columns_first( pd.concat(comp_summaries),['from_agent','to_agent'])
        self.comp_details = _pd_order_columns_first( pd.concat(comp_details),['from_agent','to_agent'])

    
    def save(self,out_dir:Path):
        self.run_summaries.to_csv(out_dir/'run_summaries.csv',index=False)
        self.run_details.to_csv(out_dir/'run_details.csv',index=False)
        self.comp_summaries.to_csv(out_dir/'comp_summaries.csv',index=False)
        self.comp_details.to_csv(out_dir/'comp_details.csv',index=False)

    def __repr__(self) -> str:
        runs_str = f"runs: {list(self.runs.keys())}"
        comparisons_str = f"comparisons: {list(self.comparisons.keys())}"
        return f"EvalResult(\n  {runs_str},\n  {comparisons_str}\n)"
    
    def __str__(self) -> str:
        return self.__repr__()

# %% ../nbs/017_eval.ipynb 89
import rich
from rich.padding import Padding
from .core import jinja_render

# %% ../nbs/017_eval.ipynb 90
def rprint(obj,indent:int=0,sep_by:int=2):
    rich.print(Padding(obj,pad=(0,0,0,indent*sep_by)))

# %% ../nbs/017_eval.ipynb 91
def _pprint_datapoint_data_prep(res:EvalResult,datapoint:str,default_comparison="cosine_dist"):
    
    base_name = list(res.comparisons.keys())[0][0]

    run_sum = res.run_summaries[res.run_summaries['datapoint'] == datapoint]
    comp_sum = res.comp_summaries[res.comp_summaries['datapoint'] == datapoint]
    run_det = res.run_details[res.run_details['datapoint'] == datapoint]
    comp_det = res.comp_details[res.comp_details['datapoint'] == datapoint]

    run_det_with_comp = run_det.merge(
        comp_det[['to_agent','datapoint','comp_id','change_type','value']],
        left_on=['agent','datapoint','comp_id'],
        right_on=['to_agent','datapoint','comp_id'],
        how='left').fillna(value='None')

    per_comp = {
        int(comp_id):
        (
        run_det[(run_det['agent'] == base_name) & (run_det['comp_id'] == comp_id)].iloc[0],
        run_det_with_comp[run_det_with_comp['comp_id'] == comp_id]
        )
        for comp_id in comp_det['comp_id'].unique()
    }

    jinja_params = {
        'base_name':base_name,
        'datapoint':datapoint,
        'run_sum':run_sum,
        'comp_sum':comp_sum,
        'per_comp':per_comp,

        'version_style':'purple',
        'param_style':'cyan bold',
        'comp_config_style':'green bold',
        'output_style':'#CE9178',
    }
    return jinja_params
    

# %% ../nbs/017_eval.ipynb 93
datapoint_template="""{{datapoint}}
  summary:
  {%- for _,row in run_sum.iterrows() %}
    [{{version_style}}]{{row.agent}}[/{{version_style}}] - Dist: {{"%.2f"|format(row.distance)}} AvgDist: {{"%.2f"|format(row.avg_distance)}} Coverage: {{ "%.2f"|format(row.coverage)}}
  {%- endfor %}
  {%- for _,row in comp_sum.iterrows() %}
    [{{version_style}}]{{row.from_agent}}[/{{version_style}}] vs [{{version_style}}]{{row.to_agent}}[/{{version_style}}]: \
Alignment change: [{{param_style}}]{{row.alignment_change}}[/{{param_style}}] \
Score change: [{{param_style}}]{{row.score_change_type}}[/{{param_style}}] \
Score by: {{ "%.2f"|format(row.total_score_change)}}
    {%- endfor%}
  details:
  {%- for comp_id,(base_details,comp_details) in per_comp.items() %}
    Comparison #[{{comp_config_style}}]{{comp_id}}[/{{comp_config_style}}], \
node_pattern: [{{comp_config_style}}]{{base_details.node_name}}[/{{comp_config_style}}], \
key: [{{comp_config_style}}]{{base_details.key}}[/{{comp_config_style}}], \
func: [{{comp_config_style}}]{{base_details.comparison}}[/{{comp_config_style}}]
    {% if base_details.kwargs | length > 0 -%}
      kwargs: [{{output_style}}]{{base_details.kwargs}}[/{{output_style}}]
    {%- endif -%}
      expected: 
[{{output_style}}]{{base_details.expected | wordwrap(width=100) | indent(8,true) }}[/{{output_style}}]
    {% for _,row in comp_details.iterrows() %}
    {{row.agent}} - matched [green]{{row.trace_name}}[/green](#{{row.trace_idx}})
    {%- if row.change_type != 'None' -%}
        , {{row.change_type}}: {{ "%.2f"|format(row.value) }}
    {%- endif -%}:
[{{output_style}}]{{row.actual | wordwrap(width=100) | indent(8,true)}}[/{{output_style}}]
    {% endfor -%}
  {% endfor %}
"""

# %% ../nbs/017_eval.ipynb 99
summary_template = """[{{version_style}}]{{run_name}}[/{{version_style}}]
Dist: {{"%.2f"|format(summary['distance'].mean())}} AvgDist: {{"%.2f"|format(summary['avg_distance'].mean())}} Coverage: {{ "%.2f"|format(summary['coverage'].mean())}}
"""

def pprint_run_summary(res:EvalResult,run_name:str,indent:int=0):
    global summary_template
    summary = res.runs[run_name].summary
    jinja_params = {
        'version_style':'purple',
        'summary':summary,
        'run_name':run_name,
    }
    with rich.get_console():
        rprint(jinja_render(summary_template,jinja_params),indent=indent)


# %% ../nbs/017_eval.ipynb 101
from collections import defaultdict

# %% ../nbs/017_eval.ipynb 103
comparison_summary_template = """[{{version_style}}]{{base_run}}[/{{version_style}}] vs [{{version_style}}]{{other_run}}[/{{version_style}}]:
{%- if alignment_change_datapoints %}
  Alignment change ([{{param_style}}]{{alignment_change_datapoints|length}}[/{{param_style}}]): [{{param_style}}]{{alignment_change_datapoints|join(',')}}[/{{param_style}}]
{% endif -%}
{% for type,datapoints in datapoints_by_change_type.items() %}
  {{type}} (#[{{param_style}}]{{datapoints|length}}[/{{param_style}}]): {% for datapoint_name,score in datapoints -%}
    [{{param_style}}]{{datapoint_name}}[/{{param_style}}]({{"%.2f"|format(score)}}), {% endfor -%}
{% endfor -%}
"""
def _pprint_comparison_prep(res:EvalResult,comparison_name:Tuple[str,str],indent:int=0):
    global comparison_summary_template
    comparison = res.comparisons[comparison_name].summary
    alignment_change_datapoints = comparison[comparison['alignment_change'] == True].datapoint.to_list()
    datapoints_by_change_type = defaultdict(list)
    for _,row in comparison.iterrows():
        datapoints_by_change_type[row['score_change_type']].append((row['datapoint'],row['total_score_change']))
    
    for change_type,datapoints in datapoints_by_change_type.items():
        datapoints_by_change_type[change_type] = sorted(datapoints,key=lambda x:x[1],reverse=True)

    jinja_params = {
        'version_style':'purple',
        'param_style':'cyan bold',
        'base_run':comparison_name[0],
        'other_run':comparison_name[1],
        'alignment_change_datapoints':alignment_change_datapoints,
        'datapoints_by_change_type':datapoints_by_change_type,
    }

    return jinja_params
    # number of datapoints whose alignment changed (write names of topk)
    # number of datapoints whose alignment improved (write names of topk)


def pprint_comparison_summary(res:EvalResult,comparison_name:Tuple[str,str],indent:int=0):
    global comparison_summary_template
    jinja_params = _pprint_comparison_prep(res,comparison_name,indent)
    with rich.get_console():
        rprint(jinja_render(comparison_summary_template,jinja_params),indent=indent)
    return jinja_params


# %% ../nbs/017_eval.ipynb 108
@patch
def pprint(self:EvalResult,datapoint:Optional[str]=None,verbose:bool=True):
    if datapoint is None:
        return pprint_eval(self,default_func=self.default_func,verbose=verbose)
    else:
        return pprint_datapoint(self,datapoint,default_func=self.default_func,indent=0)


# %% ../nbs/017_eval.ipynb 112
EVAL_COMPARISONS = {
    'eq':eq,
    'eval':safe_eval,
    'chat_eval':chat_eval,
    'cosine_dist':cosine_dist,
}

EVAL_DEFAULT_COMPARISON = 'cosine_dist'

# %% ../nbs/017_eval.ipynb 113
async def eval(
  test_dir:Path,
  out_dir:Path,
  agents:List[Tuple[str,DiagramSchema]],
  k:Optional[int]=5,
  force_run:bool=False,
  silent:bool=False,
  verbose:bool=True,
  eval_funcs: Optional[Dict[str,Callable]]=None,
  default_func:Optional[Callable]=None,
  ):
  """
  The main eval function.
  Evaluates a set of agents on a set of tests.
  Compares the results of all agents to the first agent.
  pprints a summary of the results to the console.
  and saves all files to the out_dir.

  Args:
    tests_dir: Path to the directory containing the tests.
    out_dir: Path to the directory to write the results to.
    agents: A list of tuples of agent names and their DiagramSchema.
    k: The number of datapoints to print to the summary at most. Defaults to 5.
    force_run: If True, deletes out dir content and reruns the agents. 
      If False, we skip the agents that have already been run.
      Defaults to False.
    silent: If True, dont pprint comparisons. Defaults to False.
    eval_funcs: A dictionary of eval_func names and their functions, to add to the allowed eval_funcs.
      Defaults to None.
    default_func: The default eval_func to use if no eval_func is specified.
      This is used to compare the first agent to the rest of the agents.
      Defaults to stringdale.eval.cosine_dist
  """

  global EVAL_COMPARISONS
  global EVAL_DEFAULT_COMPARISON
  if eval_funcs is not None:
    eval_funcs = EVAL_COMPARISONS | eval_funcs
  else: 
    eval_funcs = EVAL_COMPARISONS
  if default_func is None:
    default_func = EVAL_DEFAULT_COMPARISON

  eval_dataset_tasks = []
  for agent_name,agent_schema in agents:
    log_dir = out_dir / 'runs'/ agent_name
    eval_dataset_tasks.append(eval_dataset(
      Agent=agent_schema,
      test_dir=test_dir,
      out_dir=log_dir,
      eval_funcs=eval_funcs,
      default_func=default_func))

  datasets = await asyncio.gather(*eval_dataset_tasks,return_exceptions=True)

  for result,(agent_name,_) in zip(datasets,agents):
    if isinstance(result,Exception):
      result.args = (f"When evaluating agent {agent_name}:\n{result.args[0]}", )+ result.args[1:]
      raise result

  first_dataset = datasets[0]
  comparisons = dict()
  for dataset in datasets[1:]:
    comp_dir = out_dir / 'comparisons' / f'{first_dataset.dir.name}_{dataset.dir.name}'
    comp = compare_datasets(first_dataset,dataset,out_dir=comp_dir)
    comparisons[(first_dataset.dir.name,dataset.dir.name)] = comp
  
  res = EvalResult(
    runs={agent_name:dataset for (agent_name,_),dataset in zip(agents,datasets)},
    comparisons=comparisons,
    eval_funcs=eval_funcs,
    default_func=default_func,
  )

  if not silent:
    res.pprint(verbose=verbose)

  res.save(out_dir)
  
  return res


