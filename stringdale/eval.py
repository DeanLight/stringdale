# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/017_eval.ipynb.

# %% auto 0
__all__ = ['logger', 'parse_trace_log', 'cosine_dist', 'eq', 'any', 'safe_eval', 'DataPoint', 'evaluate_datapoint',
           'summarize_datapoint', 'TestSetRun', 'trace_log_len', 'eval_dataset', 'sort_conditions',
           'limit_to_datapoint', 'get_datapoint', 'describe_changes', 'compare_datasets']

# %% ../nbs/017_eval.ipynb 3
import os
import json
import yaml
from stringdale import (
    Define,
    Scope,
    V,
    E,
    Condition,
    draw_nx
)
from stringdale.stream_warping import (
    TestCase,
    parse_test_case,
    TraceLog,
    event_stream_warp,
    word_overlap,
    regex,
)


from pathlib import Path
from frozendict import frozendict
from .core import  checkLogs
import pytest
import asyncio
from pydantic import BaseModel, ConfigDict

from typing import List, Union
import jsonlines
import logging


# %% ../nbs/017_eval.ipynb 4
logger = logging.getLogger(__name__)

# %% ../nbs/017_eval.ipynb 7
def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:
    """
    Parse a trace file into a list of Trace objects.
    """
    with jsonlines.open(trace_path) as reader:
        traces = [trace for trace in reader]
        return TraceLog(steps=traces)

# %% ../nbs/017_eval.ipynb 14
import numpy as np
import asyncio
from .db import openai_embed
from .chat import Chat

# %% ../nbs/017_eval.ipynb 15
async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:
    """Compute cosine distance between two strings using OpenAI embeddings.
    
    Args:
        out: First string to compare
        expected: Second string to compare
        model: OpenAI embedding model to use (default: 'text-embedding-3-small')
        
    Returns:
        float: Cosine similarity between the two strings (between -1 and 1)
    """
    # Get embeddings for both strings
    out_embedding = await openai_embed(out, model=model)
    expected_embedding = await openai_embed(expected, model=model)
    
    # Compute cosine similarity
    dot_product = np.dot(out_embedding, expected_embedding)
    norm_out = np.linalg.norm(out_embedding)
    norm_expected = np.linalg.norm(expected_embedding)
    
    # Return cosine similarity
    return 1-dot_product / (norm_out * norm_expected)



# %% ../nbs/017_eval.ipynb 18
from .core import jinja_undeclared_vars
from typing import Any

# %% ../nbs/017_eval.ipynb 22
def eq(a,b):
    if a == b:
        return 0
    else:
        return np.inf

def any(a,b):
    return 0

# %% ../nbs/017_eval.ipynb 25
from .tools import run_python_code


# %% ../nbs/017_eval.ipynb 26
def safe_eval(out,expression):
    try:
        formatted_expressions = expression.format(out)
    except Exception as e:
        logger.warning(f"Error formatting expression: {expression} with value {out}, error: {e}")
        return np.inf
    value = run_python_code(formatted_expressions)
    if isinstance(value,str) and value.startswith("Error"):
        logger.warning(
            f"Error evaluating expression: {formatted_expressions} = {value}\n"
            f"out: {out}\n"
            f"expression: {expression}\n"
            f"error: {e}"
        )
        return np.inf
    logger.debug(f"safe_eval: {formatted_expressions} = {value}")
    if isinstance(value,bool):
        return 0 if value else np.inf
    elif isinstance(value,float):
        return value
    else:
        logger.debug(
            f"When evaluating {expression} with value {out}\n"
            f"Expected float or bool, got {type(value)} with value {repr(value)}"
            )
        return np.inf

# %% ../nbs/017_eval.ipynb 30
from typing import List,Dict,Callable

# %% ../nbs/017_eval.ipynb 31
class DataPoint(BaseModel):
    traces:TraceLog
    expected:TestCase
    

# %% ../nbs/017_eval.ipynb 32
async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):
    d=Agent()
    with jsonlines.open(trace_log_path,'w') as writer:
        for input in test_case.inputs:
            async for trace in d.arun(input):
                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))
            if d.finished:
                break

async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):
    if trace_log_path is None:
        trace_log_path = test_case_path.parent/test_case_path.name.replace(".yaml", ".jsonl").replace("expected", "actual")

    if not trace_log_path.parent.exists():
        os.makedirs(trace_log_path.parent,exist_ok=True)
    try:
        test_case = parse_test_case(test_case_path)
    except Exception as e:
        raise ValueError(f"Error parsing test case {test_case_path}: {e}") from e
        

    if force_run or not trace_log_path.exists():
        if not trace_log_path.exists():
            logger.info(f"Trace file {trace_log_path.name} does not exist, running agent")
        else:
            logger.info(f"Force running {trace_log_path.name}")
        await _run_agent(Agent,test_case,trace_log_path)
    else:
        logger.info(f"Trace file {trace_log_path.name} already exists, skipping agent run")

    parsed_trace = parse_trace_log(trace_log_path)
    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)
    
    return aligned_trace,score,debug_info,trace_log_path


# %% ../nbs/017_eval.ipynb 35
with checkLogs():
    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)

assert alignment is None
alignment,score,trace_out


# %% ../nbs/017_eval.ipynb 37
import pandas as pd

# %% ../nbs/017_eval.ipynb 38
def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):
    """
    Reorder the columns of a pandas dataframe to put the first_columns first.
    """
    return df[first_columns + [c for c in df.columns if c not in first_columns]]



# %% ../nbs/017_eval.ipynb 41
from copy import deepcopy
from itertools import count

# %% ../nbs/017_eval.ipynb 42
def summarize_datapoint(name,alignment,debug_info):
    """
    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage
    by using the alignment and the debug info
    """
    deep_dive_fit = []

    comp_counter = count()
    for expected_node_id,trace_idx in alignment.items():
        match_data = debug_info[expected_node_id][trace_idx]
        for comp in match_data['comparisons']:
            summary = deepcopy(match_data) | deepcopy(comp) 
            summary['comp_id'] = next(comp_counter)
            summary.pop('comparisons')
            deep_dive_fit.append(summary)

    df = pd.DataFrame(deep_dive_fit)
    df['datapoint'] = str(name)
    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])
    return df

# %% ../nbs/017_eval.ipynb 46
from . import DiagramSchema
from pprint import pprint, pformat

# %% ../nbs/017_eval.ipynb 47
def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):
    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(".jsonl")

# %% ../nbs/017_eval.ipynb 48
class TestSetRun(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    test_dir: Path
    log_dir: Path
    summary: pd.DataFrame
    details: pd.DataFrame
    debug: dict

    def __repr__(self):     
        return (
            f"TestSetRun(\n"
            f"  test_dir={self.test_dir}, \n"
            f"  log_dir={self.log_dir}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f"  debug=dict)")

    def __str__(self):
        return self.__repr__()

    # TODO serialize to dir

    # deserialize from dir

# %% ../nbs/017_eval.ipynb 49
def _find_yamls(test_dir:Path):
    expected_yamls = list(test_dir.glob("**/*.yaml")) + list(test_dir.glob("**/*.yml"))
    return expected_yamls


def trace_log_len(trace_log_path:Path):
    return len(trace_log_path.read_text().splitlines())

async def eval_dataset(Agent:DiagramSchema,test_dir,log_dir,force_run=False,comparisons=None,default_comparison=None):

    test_cases = _find_yamls(test_dir)
    relative_test_cases = [test_cases.relative_to(test_dir) for test_cases in test_cases]

    trace_files  = [_trace_out_path(test_case,test_dir,log_dir) for test_case in test_cases]

    logger.info(f"Evaluating {len(test_cases)} datapoints, logging to {log_dir}")
    datapoint_tasks = [evaluate_datapoint(
            Agent=Agent,
            comparisons=comparisons,
            default_comparison=default_comparison,
            test_case_path=test_case,
            trace_log_path=trace_file,
            force_run=force_run,
        ) for test_case,trace_file in zip(test_cases,trace_files) if trace_file in trace_files]
    
    datapoint_results = await asyncio.gather(*datapoint_tasks)

    summary_data = list()
    deep_dives = list()
    debug_infos = dict()

    for alignment,score,debug_info,trace_out in datapoint_results:
        datapoint_name = trace_out.relative_to(log_dir).with_suffix("")
        coverage = len(alignment) / trace_log_len(trace_out)
        summary = {'datapoint':str(datapoint_name),'distance':score,'coverage':coverage,'alignment':alignment}
        summary_data.append(summary)
        deep_dives.append(summarize_datapoint(datapoint_name,alignment,debug_info))
        debug_infos[datapoint_name] = debug_info
    

    summary_df = pd.DataFrame(summary_data)
    if len(deep_dives) > 0:
        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)
    else:
        deep_dives_df = pd.DataFrame()

    return TestSetRun(
        test_dir=test_dir,
        log_dir=log_dir,
        summary=summary_df,
        details=deep_dives_df,
        debug=debug_infos
    )

# %% ../nbs/017_eval.ipynb 64
def sort_conditions(df):
    return df.sort_values(by=['node_idx','comparison','expected'])

def limit_to_datapoint(df,datapoint):
    return df.loc[df['datapoint'] == datapoint]

def get_datapoint(ds,datapoint):
    return sort_conditions(limit_to_datapoint(ds.details,datapoint))


def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):
    """
    Describe the changes between two datapoints
    """
    
    # get the detailed version of the datasets and limit to only rows of the given datapoint
    datapoint_df1 = get_datapoint_df(ds1,datapoint)
    datapoint_df2 = get_datapoint_df(ds2,datapoint)

    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)
    # lets assert this in the code
    assert datapoint_df1.shape == datapoint_df2.shape, f"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}"

    changes = []

    # get the first comparison whose node aligned to a different trace
    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if row1.trace_idx != row2.trace_idx:
            changes.append({
                'datapoint':datapoint,
                'change_type':'alignment_change',
                'before':row1.trace_idx,
                'after':row2.trace_idx,
                'comparison_id':row1.comp_id,
            })
            break
            

    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):
        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):
            continue
        if row2.distance + epsilon > row1.distance:
            change_types = 'regression'
        elif row2.distance - epsilon < row1.distance:
            change_types = 'improvement'
        else:
            continue
        
        changes.append({
            'datapoint':datapoint,
            'change_type':change_types,
            'value':row1.distance - row2.distance,
            'comparison_id':row1.comp_id,
            'node_label':row1.node_label,
            'expected':row1.expected,
            'before':row1.actual,
            'after':row2.actual,
        })
    
    return pd.DataFrame(changes)


            

# %% ../nbs/017_eval.ipynb 66
def compare_datasets(ds1,ds2,epsilon=1e-3):
    """
    Compare two datasets
    """
    
    summary_1 = ds1.summary.sort_values(by='datapoint')
    summary_2 = ds2.summary.sort_values(by='datapoint')

    changed_datapoints = []
    change_summary = []
    detailed_changes = []

    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):
        datapoint = row1.datapoint
        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)
        coverage_change = row1.coverage != row2.coverage

        if distance_change or coverage_change:
            changed_datapoints.append(datapoint)

            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)
            detailed_changes.append(detailed_change)

            change_types = set(detailed_change['change_type'])
            if 'alignment_change' in change_types:
                alignment_change = True
            else:
                alignment_change = False
            
            if 'improvement' in change_types and not 'regression' in change_types:
                score_change = 'improved'
            elif 'regression' in change_types and not 'improvement' in change_types:
                score_change = 'regressed'
            else:
                score_change = 'changed'
            
            total_score_change = row1.distance-row2.distance

            change_summary.append({
                'datapoint':datapoint,
                'alignment_change':alignment_change,
                'score_change_type':score_change,
                'total_score_change':total_score_change,
            })

    changes_summary = pd.DataFrame(change_summary)
    if len(detailed_changes) > 0:
        detailed_changes = pd.concat(detailed_changes)
    else:
        detailed_changes = pd.DataFrame()

    return changes_summary,detailed_changes
