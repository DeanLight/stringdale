# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/017_eval.ipynb.

# %% auto 0
__all__ = ['logger', 'parse_trace', 'cosine_dist', 'eq', 'any', 'safe_eval', 'DataPoint', 'evaluate_datapoint',
           'summarize_datapoint', 'EvalDataset', 'eval_dataset']

# %% ../nbs/017_eval.ipynb 3
import os
import json
import yaml
from stringdale import (
    Define,
    Scope,
    V,
    E,
    Condition,
    draw_nx
)
from stringdale.podtw import (
    parse_expected_trace,
    align_traces,
    word_overlap,
    regex,
    ExpectedTrace,
    Trace
)


from pathlib import Path
from frozendict import frozendict
from .core import  checkLogs
import pytest
import asyncio
from pydantic import BaseModel, ConfigDict

from typing import List, Union
import jsonlines
import logging


# %% ../nbs/017_eval.ipynb 4
logger = logging.getLogger(__name__)

# %% ../nbs/017_eval.ipynb 7
def parse_trace(trace_path:Union[str,Path]) -> List[Trace]:
    """
    Parse a trace file into a list of Trace objects.
    """
    with jsonlines.open(trace_path) as reader:
        return [Trace.model_validate(trace) for trace in reader]

# %% ../nbs/017_eval.ipynb 14
import numpy as np
import asyncio
from .db import openai_embed
from .chat import Chat

# %% ../nbs/017_eval.ipynb 15
async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:
    """Compute cosine distance between two strings using OpenAI embeddings.
    
    Args:
        out: First string to compare
        expected: Second string to compare
        model: OpenAI embedding model to use (default: 'text-embedding-3-small')
        
    Returns:
        float: Cosine similarity between the two strings (between -1 and 1)
    """
    # Get embeddings for both strings
    out_embedding = await openai_embed(out, model=model)
    expected_embedding = await openai_embed(expected, model=model)
    
    # Compute cosine similarity
    dot_product = np.dot(out_embedding, expected_embedding)
    norm_out = np.linalg.norm(out_embedding)
    norm_expected = np.linalg.norm(expected_embedding)
    
    # Return cosine similarity
    return 1-dot_product / (norm_out * norm_expected)



# %% ../nbs/017_eval.ipynb 20
def eq(a,b):
    if a == b:
        return 0
    else:
        return np.inf

def any(a,b):
    return 0

# %% ../nbs/017_eval.ipynb 23
from .tools import run_python_code


# %% ../nbs/017_eval.ipynb 24
def safe_eval(out,expression):
    try:
        formatted_expressions = expression.format(out)
    except Exception as e:
        logger.warning(f"Error formatting expression: {expression} with value {out}, error: {e}")
        return np.inf
    value = run_python_code(formatted_expressions)
    if isinstance(value,str) and value.startswith("Error"):
        logger.warning(
            f"Error evaluating expression: {formatted_expressions} = {value}\n"
            f"out: {out}\n"
            f"expression: {expression}\n"
            f"error: {e}"
        )
        return np.inf
    logger.debug(f"safe_eval: {formatted_expressions} = {value}")
    if isinstance(value,bool):
        return 0 if value else np.inf
    elif isinstance(value,float):
        return value
    else:
        logger.debug(
            f"When evaluating {expression} with value {out}\n"
            f"Expected float or bool, got {type(value)} with value {repr(value)}"
            )
        return np.inf

# %% ../nbs/017_eval.ipynb 28
from typing import List,Dict,Callable

# %% ../nbs/017_eval.ipynb 29
class DataPoint(BaseModel):
    traces:List[Trace]
    expected:ExpectedTrace
    

# %% ../nbs/017_eval.ipynb 31
async def _run_agent(Agent,expected_trace,trace_out):
    d=Agent()
    with jsonlines.open(trace_out,'w') as writer:
        for input in expected_trace.input:
            async for trace in d.arun(input):
                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))
            if d.finished:
                break

async def evaluate_datapoint(Agent,comparisons,default_comparison,expected_yaml,trace_out=None,force_run=False):
    if trace_out is None:
        trace_out = expected_yaml.parent/expected_yaml.name.replace(".yaml", ".jsonl").replace("expected", "actual")

    if not trace_out.parent.exists():
        os.makedirs(trace_out.parent,exist_ok=True)
    try:
        expected_trace = parse_expected_trace(expected_yaml)
    except Exception as e:
        raise ValueError(f"Error parsing expected trace {expected_yaml}: {e}") from e
        

    if force_run or not trace_out.exists():
        if not trace_out.exists():
            logger.info(f"Trace file {trace_out.name} does not exist, running agent")
        else:
            logger.info(f"Force running {trace_out.name}")
        await _run_agent(Agent,expected_trace,trace_out)
    else:
        logger.info(f"Trace file {trace_out.name} already exists, skipping agent run")

    parsed_trace = parse_trace(trace_out)
    aligned_trace,score,debug_info = await align_traces(parsed_trace,expected_trace,comparisons,default_comparison)
    
    return aligned_trace,score,debug_info,trace_out


# %% ../nbs/017_eval.ipynb 33
with checkLogs():
    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)

assert alignment is None
alignment,score,trace_out


# %% ../nbs/017_eval.ipynb 35
import pandas as pd

# %% ../nbs/017_eval.ipynb 36
def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):
    """
    Reorder the columns of a pandas dataframe to put the first_columns first.
    """
    return df[first_columns + [c for c in df.columns if c not in first_columns]]



# %% ../nbs/017_eval.ipynb 39
from copy import deepcopy

# %% ../nbs/017_eval.ipynb 40
def summarize_datapoint(name,alignment,debug_info):
    """
    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage
    by using the alignment and the debug info
    """
    deep_dive_fit = []

    for expected_node_id,trace_idx in alignment.items():
        match_data = debug_info[expected_node_id][trace_idx]
        for comp in match_data['comparisons']:
            summary = deepcopy(comp)
            summary['node_name'] = match_data['actual_name']
            summary['expected_name'] = match_data['expected_name']
            summary['expected_node_id'] = expected_node_id
            summary['trace_idx'] = trace_idx
            # TODO put node name and node pattern
            deep_dive_fit.append(summary)

    df = pd.DataFrame(deep_dive_fit)
    df['datapoint'] = name
    df = _pd_order_columns_first(df,['datapoint','expected_node_id','trace_idx','accessor','comparison','actual','expected','distance'])
    return df

# %% ../nbs/017_eval.ipynb 44
from . import DiagramSchema
from pprint import pprint, pformat

# %% ../nbs/017_eval.ipynb 45
def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):
    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(".jsonl")



# %% ../nbs/017_eval.ipynb 46
class EvalDataset(BaseModel):
    model_config = ConfigDict(arbitrary_types_allowed=True)
    expected_dir: Path
    trace_dir: Path
    summary: pd.DataFrame
    details: pd.DataFrame
    debug: dict

    def __repr__(self):     
        return (
            f"EvalDataset(expected_dir={self.expected_dir}, \n"
            f"  trace_dir={self.trace_dir}, \n"
            f"  summary=Dataframe({self.summary.shape}), \n"
            f"  details=Dataframe({self.details.shape}), \n"
            f"  debug=dict)")
        

# %% ../nbs/017_eval.ipynb 47
def _find_yamls(expected_dir:Path):
    expected_yamls = list(expected_dir.glob("**/*.yaml")) + list(expected_dir.glob("**/*.yml"))
    return expected_yamls


async def eval_dataset(Agent:DiagramSchema,expected_dir,trace_dir,force_run=False,comparisons=None,default_comparison=None):

    expected_yamls = _find_yamls(expected_dir)
    relative_expected_yamls = [expected_yamls.relative_to(expected_dir) for expected_yamls in expected_yamls]

    trace_files  = [_trace_out_path(expected_yaml,expected_dir,trace_dir) for expected_yaml in expected_yamls]

    logger.info(f"Evaluating {len(expected_yamls)} datapoints, logging to {trace_dir}")
    datapoint_tasks = [evaluate_datapoint(
            Agent=Agent,
            comparisons=comparisons,
            default_comparison=default_comparison,
            expected_yaml=expected_yaml,
            trace_out=trace_file,
            force_run=force_run,
        ) for expected_yaml,trace_file in zip(expected_yamls,trace_files) if trace_file in trace_files]
    
    datapoint_results = await asyncio.gather(*datapoint_tasks)

    summary_data = list()
    deep_dives = list()
    debug_infos = dict()

    for alignment,score,debug_info,trace_out in datapoint_results:
        datapoint_name = trace_out.relative_to(trace_dir).with_suffix("")
        summary = {'datapoint_name':datapoint_name,'score':score,'alignment':alignment}
        summary_data.append(summary)
        deep_dives.append(summarize_datapoint(datapoint_name,alignment,debug_info))
        debug_infos[datapoint_name] = debug_info
    

    summary_df = pd.DataFrame(summary_data)
    if len(deep_dives) > 0:
        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)
    else:
        deep_dives_df = pd.DataFrame()

    return EvalDataset(
        expected_dir=expected_dir,
        trace_dir=trace_dir,
        summary=summary_df,
        details=deep_dives_df,
        debug=debug_infos
    )
