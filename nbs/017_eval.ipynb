{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "from stringdale.podtw import (\n",
    "    parse_expected_trace,\n",
    "    align_traces,\n",
    "    word_overlap,\n",
    "    regex,\n",
    "    ExpectedTrace,\n",
    "    Trace\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from typing import List, Union\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO multiple inputs\n",
    "# TODO any comparison so we can check for existance of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse_trace(trace_path:Union[str,Path]) -> List[Trace]:\n",
    "    \"\"\"\n",
    "    Parse a trace file into a list of Trace objects.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(trace_path) as reader:\n",
    "        return [Trace.model_validate(trace) for trace in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = get_git_root() / \"sample_data\" / \"eval\"\n",
    "\n",
    "example_trace_path = sample_data_dir / \"traces0.jsonl\"\n",
    "example_expected_path = sample_data_dir / \"expected0.yaml\"\n",
    "\n",
    "\n",
    "example_comparisons = {\n",
    "    \"word_overlap\":word_overlap,\n",
    "    \"regex\":regex,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_trace = parse_trace(example_trace_path)\n",
    "example_expected = parse_expected_trace(example_expected_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 4})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_match,score,dist =await align_traces(example_trace,example_expected,comparisons=example_comparisons,default_comparison=word_overlap)\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_a1': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy went\\nto the store\\n',\n",
       "     'actual': 'jimmy went\\nto the store\\nto buy some milk',\n",
       "     'distance': 0.375}],\n",
       "   'distance': 0.375,\n",
       "   'expected_idx': 0,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_a',\n",
       "   'expected_label': 'node_a1'}},\n",
       " 'node_z': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  3: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'d': 'store'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 3,\n",
       "   'actual_name': 'node_a2',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  4: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store',\n",
       "     'distance': 0.0}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 4,\n",
       "   'actual_name': 'node_x',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  5: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good but not good enough'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  7: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'stores',\n",
       "     'distance': 1.0}],\n",
       "   'distance': 1.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 7,\n",
       "   'actual_name': 'node_y',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'}},\n",
       " '3': {2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good',\n",
       "     'distance': 0.6666666666666667}],\n",
       "   'distance': 0.6666666666666667,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good but not good enough',\n",
       "     'distance': 0.8333333333333334}],\n",
       "   'distance': 0.8333333333333334,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'}},\n",
       " '1': {5: {'comparisons': [{'comparison': 'regex',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy',\n",
       "     'actual': 'jimmy',\n",
       "     'distance': 0.0},\n",
       "    {'comparison': 'word_overlap',\n",
       "     'kwargs': {'case_sensitive': False},\n",
       "     'expected': 'is a good boy',\n",
       "     'actual': 'is a good boy',\n",
       "     'distance': 0.0}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 1,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_b',\n",
       "   'expected_label': '1'}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from stringdale.db import openai_embed\n",
    "from stringdale.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:\n",
    "    \"\"\"Compute cosine distance between two strings using OpenAI embeddings.\n",
    "    \n",
    "    Args:\n",
    "        out: First string to compare\n",
    "        expected: Second string to compare\n",
    "        model: OpenAI embedding model to use (default: 'text-embedding-3-small')\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two strings (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Get embeddings for both strings\n",
    "    out_embedding = await openai_embed(out, model=model)\n",
    "    expected_embedding = await openai_embed(expected, model=model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(out_embedding, expected_embedding)\n",
    "    norm_out = np.linalg.norm(out_embedding)\n",
    "    norm_expected = np.linalg.norm(expected_embedding)\n",
    "    \n",
    "    # Return cosine similarity\n",
    "    return 1-dot_product / (norm_out * norm_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_dist = await cosine_dist(\"hello\",\"hello\")\n",
    "basic_dist\n",
    "assert basic_dist < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39451383328055045"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await cosine_dist(\"hello\",\"hello stranger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatEvalScore(BaseModel):\n",
    "    score:float\n",
    "\n",
    "class ChatEval:\n",
    "    def __init__(self,model:str=\"gpt-4o-mini\",system_prompt:str=None):\n",
    "        self.model = model\n",
    "        base_prompt = \"\"\"\n",
    "            You are a helpful assistant that evaluates the similarity of two strings.\n",
    "            You will be given two strings, and you will need to evaluate the similarity of the two strings.\n",
    "            You will need to return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "            \"\"\"    \n",
    "        self.messages = [\n",
    "            {\"role\":\"system\",\"content\":base_prompt},\n",
    "        ]\n",
    "        if system_prompt is not None:\n",
    "            self.messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "\n",
    "            \n",
    "\n",
    "    async def __call__(self,out:str,expected:str)->float:\n",
    "        self.messages.append({\"role\":\"user\",\"content\":f\"string1: {out}\\nstring2: {expected}\"})\n",
    "        chat = Chat(model=self.model,messages=self.messages,output_schema=ChatEvalScore)\n",
    "        response = await chat()\n",
    "        return response['content'].score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = ChatEval(system_prompt=\"if one of the strings is world, output 0.5\")\n",
    "result = await eval(\"hello\",\"world\")\n",
    "assert result == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eq(a,b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def any(a,b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List,Dict,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPoint(BaseModel):\n",
    "    traces:List[Trace]\n",
    "    expected:ExpectedTrace\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.react import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml,trace_out=None):\n",
    "    if trace_out is None:\n",
    "        trace_out = expected_yaml.parent/expected_yaml.name.replace(\".yaml\", \".jsonl\").replace(\"expected\", \"actual\")\n",
    "    \n",
    "    expected_trace = parse_expected_trace(expected_yaml)\n",
    "\n",
    "    d=agent()\n",
    "    with jsonlines.open(trace_out,'w') as writer:\n",
    "        for input in expected_trace.input:\n",
    "            async for trace in d.arun(input):\n",
    "                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))\n",
    "            if d.finished:\n",
    "                break\n",
    "\n",
    "    parsed_trace = parse_trace(trace_out)\n",
    "    aligned_trace,score,debug_info = await align_traces(parsed_trace,expected_trace,comparisons,default_comparison)\n",
    "    \n",
    "    return aligned_trace,score,debug_info,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent\n",
    "expected_yaml = sample_data_dir/\"react_expected.yaml\"\n",
    "comparisons = {\n",
    "    \"eq\":eq\n",
    "}\n",
    "default_comparison = cosine_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'0': 2, '1': 5}),\n",
       " 0.3245190060308185,\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "alignment,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here try moving to UV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=[[{'role': 'user', 'content': 'Question: what is obamas age to the power of 2?'}]] expected=[ExpectedTraceStep(name='use_tool', label='0', conditions=[Condition(accessor=('content', 'name'), value='wikipedia_search', comparison='eq', kwargs={}), Condition(accessor=('content', 'input', 'q'), value='Obama', comparison=None, kwargs={})], before=[], after=[]), ExpectedTraceStep(name='use_tool', label='1', conditions=[Condition(accessor=('content', 'name'), value='run_python_code', comparison='eq', kwargs={}), Condition(accessor=('content', 'output'), value=3969, comparison='eq', kwargs={})], before=[], after=['0'])]\n",
      "[Trace(name='Start', output=[{'role': 'user', 'content': 'Question: what is obamas age to the power of 2?'}]), Trace(name='thinker', output={'role': 'assistant', 'content': {'type': 'final_answer', 'text': '3969'}, 'meta': {'input_tokens': 18047, 'output_tokens': 18}}), Trace(name='End', output={'role': 'assistant', 'content': {'type': 'final_answer', 'text': '3969'}, 'meta': {'input_tokens': 18047, 'output_tokens': 18}})]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m aligned_trace,score,debug_info,trace_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n\u001b[1;32m      2\u001b[0m aligned_trace,score,trace_out\n",
      "Cell \u001b[0;32mIn[48], line 17\u001b[0m, in \u001b[0;36mevaluate_datapoint\u001b[0;34m(agent, comparisons, default_comparison, expected_yaml, trace_out)\u001b[0m\n\u001b[1;32m     15\u001b[0m parsed_trace \u001b[38;5;241m=\u001b[39m parse_trace(trace_out)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(parsed_trace)\n\u001b[0;32m---> 17\u001b[0m aligned_trace,score,debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m align_traces(parsed_trace,expected_trace,comparisons,default_comparison)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m aligned_trace,score,debug_info,trace_out\n",
      "File \u001b[0;32m~/dl/stringdale/stringdale/podtw.py:372\u001b[0m, in \u001b[0;36malign_traces\u001b[0;34m(traces_outputs, expected_trace, comparisons, default_comparison)\u001b[0m\n\u001b[1;32m    369\u001b[0m     label_to_var\u001b[38;5;241m.\u001b[39madd_label(expected_step\u001b[38;5;241m.\u001b[39mlabel,idx)\n\u001b[1;32m    371\u001b[0m dist,debug_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m compute_distances(traces_outputs,expected_trace,comparisons,default_comparison)\n\u001b[0;32m--> 372\u001b[0m possible_mappings \u001b[38;5;241m=\u001b[39m \u001b[43mget_possible_mappings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexpected_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel_to_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m best_mapping,best_score \u001b[38;5;241m=\u001b[39m get_best_mapping(dist,possible_mappings,label_to_var)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_mapping, best_score, debug_info\n",
      "File \u001b[0;32m~/dl/stringdale/stringdale/podtw.py:322\u001b[0m, in \u001b[0;36mget_possible_mappings\u001b[0;34m(dist, expected_traces, label_to_var)\u001b[0m\n\u001b[1;32m    320\u001b[0m p \u001b[38;5;241m=\u001b[39m Problem()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_idx,expected_step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(expected_traces\u001b[38;5;241m.\u001b[39mexpected):\n\u001b[0;32m--> 322\u001b[0m     viable_trace_row_nums \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mdist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpected_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    323\u001b[0m     var_name \u001b[38;5;241m=\u001b[39m label_to_var\u001b[38;5;241m.\u001b[39mget_col(expected_step\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[1;32m    324\u001b[0m     p\u001b[38;5;241m.\u001b[39maddVariable(var_name,viable_trace_row_nums)\n",
      "\u001b[0;31mKeyError\u001b[0m: '0'"
     ]
    }
   ],
   "source": [
    "# aligned_trace,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "# aligned_trace,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert aligned_trace == frozendict({'0':2,'1':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comparisons': [{'comparison': 'eq',\n",
       "   'kwargs': {},\n",
       "   'expected': 'wikipedia_search',\n",
       "   'actual': 'wikipedia_search',\n",
       "   'distance': 0},\n",
       "  {'comparison': 'cosine_dist',\n",
       "   'kwargs': {},\n",
       "   'expected': 'Obama',\n",
       "   'actual': 'Barack Obama',\n",
       "   'distance': 0.3245190060308185}],\n",
       " 'distance': 0.3245190060308185,\n",
       " 'expected_idx': 0,\n",
       " 'actual_idx': 2,\n",
       " 'actual_name': 'use_tool',\n",
       " 'expected_name': 'use_tool',\n",
       " 'expected_label': '0'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info['0'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here:\n",
    "# get it by evalu\n",
    "# get a dataSeries the distance per step and total metrics such as sum of distances and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lift this to a least of datapoints\n",
    "# make the dataset class\n",
    "# get it from a directory, and then eval it.\n",
    "\n",
    "# make the time series into a dataframe of all datapoints (in case of directory take the filename relative to the directory)\n",
    "# maybe give ability to have nested directories with regexs to filter the files?\n",
    " \n",
    "# then we output a datasetRun class that has the results dataframe but also metadata about the run\n",
    "# for now lets start with an agent name and run id\n",
    "\n",
    "# when returning the datasetrun dataframe, also return the debug info which will be a dict of debug info for each datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes 2 datasetRuns and returns a comparison per datapoint on the difference between the two runs\n",
    "# then have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End regression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the following example\n",
    "\n",
    "# always have a factory (or a nested factory expression)\n",
    "# take the same factory and change the prompt\n",
    "# take the same agent but give it a different db\n",
    "# or maybe the same db but a different filter\n",
    "# same factory but different fewshot examples yaml files\n",
    "# and show regression testing\n",
    "\n",
    "# our function should take 2 agents, and a EvalDataSet \n",
    "# return the comparison and print the summary as the k datapoints that regressed the most\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO in future, specify 2 datasets (by dirs and regex)\n",
    "# one is the train one is the test\n",
    "# we take an agent (or 2 for comparison)\n",
    "# and we do the same logic for evaluating and comparing, however, we print the statistics only for the test set\n",
    "# but we show the regression for the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData model\\n\\nWe have a dataset\\n* containing tests\\n* each test has the input to the agent\\n* and the expected output\\n* test is any object that can be serialized to json\\n* expected output is a partial trace spec\\n\\n* partial trace spec is a list of steps\\n* each step has a name is a dict with accessors and value are how to check them\\n* names are the node name we expect to see in the trace\\n* the dict defines what we expect the value to look like\\n\\n\\nWhen we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\\nsince the partial trace spec does not \\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data model\n",
    "\n",
    "We have a dataset\n",
    "* containing tests\n",
    "* each test has the input to the agent\n",
    "* and the expected output\n",
    "* test is any object that can be serialized to json\n",
    "* expected output is a partial trace spec\n",
    "\n",
    "* partial trace spec is a list of steps\n",
    "* each step has a name is a dict with accessors and value are how to check them\n",
    "* names are the node name we expect to see in the trace\n",
    "* the dict defines what we expect the value to look like\n",
    "\n",
    "\n",
    "When we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\n",
    "since the partial trace spec does not \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedTrace:\n",
    "    pass\n",
    "\n",
    "class DataPointRun:\n",
    "    # basically a list of traces, agent input and agent output\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traces_from_file(file_path):\n",
    "    pass\n",
    "\n",
    "def collect_traces_from_logg_aggregator(logger):\n",
    "    pass\n",
    "\n",
    "def run_dataset(agent,dataset,output_dir):\n",
    "    # for each data point in the dataset\n",
    "    # run the agent\n",
    "    # collect the traces into a file\n",
    "    # return the file path\n",
    "    pass\n",
    "\n",
    "def write_comparison_to_file(dataset_run,expected_traces,output_dir):\n",
    "    # run the comparison and write the results to a file\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_summary(runs,dir):\n",
    "    # get the run files and the comparison files\n",
    "    # get the total metrics per expected node and total\n",
    "    # make them into a dataframe\n",
    "    pass\n",
    "\n",
    "def plot_runs(runs,dir):\n",
    "    # call runs_summary\n",
    "    # plot the results\n",
    "    pass\n",
    "\n",
    "def check_regressions(runs,dir):\n",
    "    # get two runs\n",
    "    # for each input, if the second run is worse than the first, then flag it\n",
    "    # make a dataframe of the regressions on a whole run basis\n",
    "    \n",
    "    # also make a dataframe of the regressions on a per node basis for the runs that regressed.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the DPTW to match each trace to an expected trace\n",
    "# than we have multiple scores\n",
    "    # total distance, \n",
    "    # total distance per expected trace, \n",
    "    # coverage (percent of nodes expected), \n",
    "    # time coverage (percent of time of nodes expected), used to ignore nodes with no logic\n",
    "\n",
    "\n",
    "# this experiment object can be dumped into a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Regression detection\n",
    "# here we just compare the runs to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
