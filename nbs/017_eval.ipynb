{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "from stringdale.stream_warping import (\n",
    "    TestCase,\n",
    "    parse_test_case,\n",
    "    TraceLog,\n",
    "    event_stream_warp,\n",
    "    word_overlap,\n",
    "    regex,\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from typing import List, Union\n",
    "import jsonlines\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO multiple inputs\n",
    "# TODO ANY comparison so we can check for existance of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:\n",
    "    \"\"\"\n",
    "    Parse a trace file into a list of Trace objects.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(trace_path) as reader:\n",
    "        traces = [trace for trace in reader]\n",
    "        return TraceLog(steps=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = get_git_root() / \"sample_data\" / \"eval\"\n",
    "\n",
    "example_trace_log_path = sample_data_dir / \"traces0.jsonl\"\n",
    "example_case_path = sample_data_dir / \"expected0.yaml\"\n",
    "\n",
    "\n",
    "example_comparisons = {\n",
    "    \"word_overlap\":word_overlap,\n",
    "    \"regex\":regex,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_case = parse_test_case(example_case_path)\n",
    "example_trace_log  = parse_trace_log(example_trace_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozendict.frozendict({'3': 2, 'node_a1': 1, 'node_z': 4, '1': 5})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_match,score,debug_info =await event_stream_warp(example_trace_log,example_test_case,comparisons=example_comparisons,default_comparison=word_overlap)\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from stringdale.db import openai_embed\n",
    "from stringdale.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:\n",
    "    \"\"\"Compute cosine distance between two strings using OpenAI embeddings.\n",
    "    \n",
    "    Args:\n",
    "        out: First string to compare\n",
    "        expected: Second string to compare\n",
    "        model: OpenAI embedding model to use (default: 'text-embedding-3-small')\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two strings (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Get embeddings for both strings\n",
    "    out_embedding = await openai_embed(out, model=model)\n",
    "    expected_embedding = await openai_embed(expected, model=model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(out_embedding, expected_embedding)\n",
    "    norm_out = np.linalg.norm(out_embedding)\n",
    "    norm_expected = np.linalg.norm(expected_embedding)\n",
    "    \n",
    "    # Return cosine similarity\n",
    "    return 1-dot_product / (norm_out * norm_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_dist = await cosine_dist(\"hello\",\"hello\")\n",
    "basic_dist\n",
    "assert basic_dist < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3944818489490096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await cosine_dist(\"hello\",\"hello stranger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from stringdale.core import jinja_undeclared_vars\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatEvalScore(BaseModel):\n",
    "    score:float\n",
    "\n",
    "\n",
    "async def chat_eval(out:Any,expected:Any,model:str=\"gpt-4o-mini\",system_prompt:str=None)->float:\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"\n",
    "            You are a helpful assistant that evaluates the similarity of two strings.\n",
    "            You will be given two strings, and you will need to evaluate the similarity of the two strings.\n",
    "            You will need to return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "\n",
    "            string1: {{out}}\n",
    "            string2: {{expected}}\n",
    "\n",
    "            return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "            \"\"\"\n",
    "\n",
    "    if not jinja_undeclared_vars(system_prompt) == {'out','expected'}:\n",
    "        raise ValueError(\"System prompt must contain {{out}} and {{expected}} jinja variables\")\n",
    "\n",
    "    chat = Chat(model=model,messages=\n",
    "        [{\"role\":\"system\",\"content\":system_prompt}],\n",
    "        output_schema=ChatEvalScore,\n",
    "        out = out,\n",
    "        expected = expected,\n",
    "        )\n",
    "    response = await chat()\n",
    "    return response['content'].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{out}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n",
    "assert result == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{outs}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "with pytest.raises(ValueError,match=\"System prompt must contain {{out}} and {{expected}} jinja variables\"):\n",
    "    result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eq(a,b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def any(a,b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 3500\n",
    "expr = \"({0} < 4000) & ({0} > 3000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(3500 < 4000) & (3500 > 3000)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_expr = expr.format(out)\n",
    "f_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.tools import run_python_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def safe_eval(out,expression):\n",
    "    try:\n",
    "        formatted_expressions = expression.format(out)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error formatting expression: {expression} with value {out}, error: {e}\")\n",
    "        return np.inf\n",
    "    value = run_python_code(formatted_expressions)\n",
    "    if isinstance(value,str) and value.startswith(\"Error\"):\n",
    "        logger.warning(\n",
    "            f\"Error evaluating expression: {formatted_expressions} = {value}\\n\"\n",
    "            f\"out: {out}\\n\"\n",
    "            f\"expression: {expression}\\n\"\n",
    "            f\"error: {e}\"\n",
    "        )\n",
    "        return np.inf\n",
    "    logger.debug(f\"safe_eval: {formatted_expressions} = {value}\")\n",
    "    if isinstance(value,bool):\n",
    "        return 0 if value else np.inf\n",
    "    elif isinstance(value,float):\n",
    "        return value\n",
    "    else:\n",
    "        logger.debug(\n",
    "            f\"When evaluating {expression} with value {out}\\n\"\n",
    "            f\"Expected float or bool, got {type(value)} with value {repr(value)}\"\n",
    "            )\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - safe_eval: \n",
      "x=4000\n",
      "(3500 < x) & (3500 > 3000)\n",
      " = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_string =\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\"\n",
    "\n",
    "with checkLogs():\n",
    "    y =safe_eval(3500,eval_string)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_eval(3500,\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List,Dict,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPoint(BaseModel):\n",
    "    traces:TraceLog\n",
    "    expected:TestCase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):\n",
    "    d=Agent()\n",
    "    with jsonlines.open(trace_log_path,'w') as writer:\n",
    "        for input in test_case.inputs:\n",
    "            async for trace in d.arun(input):\n",
    "                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))\n",
    "            if d.finished:\n",
    "                break\n",
    "\n",
    "async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):\n",
    "    if trace_log_path is None:\n",
    "        trace_log_path = test_case_path.parent/test_case_path.name.replace(\".yaml\", \".jsonl\").replace(\"expected\", \"actual\")\n",
    "\n",
    "    if not trace_log_path.parent.exists():\n",
    "        os.makedirs(trace_log_path.parent,exist_ok=True)\n",
    "    try:\n",
    "        test_case = parse_test_case(test_case_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing test case {test_case_path}: {e}\") from e\n",
    "        \n",
    "\n",
    "    if force_run or not trace_log_path.exists():\n",
    "        if not trace_log_path.exists():\n",
    "            logger.info(f\"Trace file {trace_log_path.name} does not exist, running agent\")\n",
    "        else:\n",
    "            logger.info(f\"Force running {trace_log_path.name}\")\n",
    "        await _run_agent(Agent,test_case,trace_log_path)\n",
    "    else:\n",
    "        logger.info(f\"Trace file {trace_log_path.name} already exists, skipping agent run\")\n",
    "\n",
    "    parsed_trace = parse_trace_log(trace_log_path)\n",
    "    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)\n",
    "    \n",
    "    return aligned_trace,score,debug_info,trace_log_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.react import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent\n",
    "expected_yaml = sample_data_dir/\"react_expected.yaml\"\n",
    "bad_expected_yaml = sample_data_dir/\"react_bad_expected.yaml\"\n",
    "comparisons = {\n",
    "    \"eq\":eq,\n",
    "    \"eval\":safe_eval,\n",
    "}\n",
    "default_comparison = cosine_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_bad_actual.jsonl already exists, skipping agent run\n",
      "No viable trace row nums for expected trace 1\n",
      "No possible mappings found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " inf,\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_bad_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "with checkLogs():\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)\n",
    "\n",
    "assert alignment is None\n",
    "alignment,score,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_actual.jsonl already exists, skipping agent run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'0': 2, '1': 8}),\n",
       " np.float64(0.3244313858854829),\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "\n",
    "assert dict(alignment) == {'0': 2, '1': 8}\n",
    "alignment,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):\n",
    "    \"\"\"\n",
    "    Reorder the columns of a pandas dataframe to put the first_columns first.\n",
    "    \"\"\"\n",
    "    return df[first_columns + [c for c in df.columns if c not in first_columns]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  expected  distance comparison\n",
       "0       1         1         1         eq\n",
       "1       2         2         2         eq\n",
       "2       3         3         3         eq"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame([\n",
    "    {'distance':1,'comparison':'eq','actual':1,'expected':1},\n",
    "    {'distance':2,'comparison':'eq','actual':2,'expected':2},\n",
    "    {'distance':3,'comparison':'eq','actual':3,'expected':3},\n",
    "])\n",
    "\n",
    "_pd_order_columns_first(x,['actual','expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarize_datapoint(name,alignment,debug_info):\n",
    "    \"\"\"\n",
    "    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage\n",
    "    by using the alignment and the debug info\n",
    "    \"\"\"\n",
    "    deep_dive_fit = []\n",
    "\n",
    "    comp_counter = count()\n",
    "    for expected_node_id,trace_idx in alignment.items():\n",
    "        match_data = debug_info[expected_node_id][trace_idx]\n",
    "        for comp in match_data['comparisons']:\n",
    "            summary = deepcopy(match_data) | deepcopy(comp) \n",
    "            summary['comp_id'] = next(comp_counter)\n",
    "            summary.pop('comparisons')\n",
    "            deep_dive_fit.append(summary)\n",
    "\n",
    "    df = pd.DataFrame(deep_dive_fit)\n",
    "    df['datapoint'] = str(name)\n",
    "    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content.input.q</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Obama</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>content.output</td>\n",
       "      <td>3844</td>\n",
       "      <td>({0} &lt; 4000) &amp; ({0} &gt; 3000)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison              key  \\\n",
       "0     react          0          2         eq     content.name   \n",
       "1     react          0          2       None  content.input.q   \n",
       "2     react          1          8         eq     content.name   \n",
       "3     react          1          8       eval   content.output   \n",
       "\n",
       "             actual                     expected  distance  node_idx  \\\n",
       "0  wikipedia_search             wikipedia_search  0.000000         0   \n",
       "1      Barack Obama                        Obama  0.324431         0   \n",
       "2   run_python_code              run_python_code  0.000000         1   \n",
       "3              3844  ({0} < 4000) & ({0} > 3000)  0.000000         1   \n",
       "\n",
       "  trace_name node_name kwargs  comp_id  \n",
       "0   use_tool  use_tool     {}        0  \n",
       "1   use_tool  use_tool     {}        1  \n",
       "2   use_tool  use_tool     {}        2  \n",
       "3   use_tool  use_tool     {}        3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = summarize_datapoint('react',alignment,debug_info)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['node_label'].to_list() == ['0','0','1','1']\n",
    "assert df['key'].to_list() == ['content.name','content.input.q','content.name','content.output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale import DiagramSchema\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):\n",
    "    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(\".jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TestSetRun(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    test_dir: Path\n",
    "    log_dir: Path\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "    debug: dict\n",
    "\n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"TestSetRun(\\n\"\n",
    "            f\"  test_dir={self.test_dir}, \\n\"\n",
    "            f\"  log_dir={self.log_dir}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\"  debug=dict)\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    # TODO serialize to dir\n",
    "\n",
    "    # deserialize from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _find_yamls(test_dir:Path):\n",
    "    expected_yamls = list(test_dir.glob(\"**/*.yaml\")) + list(test_dir.glob(\"**/*.yml\"))\n",
    "    return expected_yamls\n",
    "\n",
    "\n",
    "def trace_log_len(trace_log_path:Path):\n",
    "    return len(trace_log_path.read_text().splitlines())\n",
    "\n",
    "async def eval_dataset(Agent:DiagramSchema,test_dir,log_dir,force_run=False,comparisons=None,default_comparison=None):\n",
    "\n",
    "    test_cases = _find_yamls(test_dir)\n",
    "    relative_test_cases = [test_cases.relative_to(test_dir) for test_cases in test_cases]\n",
    "\n",
    "    trace_files  = [_trace_out_path(test_case,test_dir,log_dir) for test_case in test_cases]\n",
    "\n",
    "    logger.info(f\"Evaluating {len(test_cases)} datapoints, logging to {log_dir}\")\n",
    "    datapoint_tasks = [evaluate_datapoint(\n",
    "            Agent=Agent,\n",
    "            comparisons=comparisons,\n",
    "            default_comparison=default_comparison,\n",
    "            test_case_path=test_case,\n",
    "            trace_log_path=trace_file,\n",
    "            force_run=force_run,\n",
    "        ) for test_case,trace_file in zip(test_cases,trace_files) if trace_file in trace_files]\n",
    "    \n",
    "    datapoint_results = await asyncio.gather(*datapoint_tasks)\n",
    "\n",
    "    summary_data = list()\n",
    "    deep_dives = list()\n",
    "    debug_infos = dict()\n",
    "\n",
    "    for alignment,score,debug_info,trace_out in datapoint_results:\n",
    "        datapoint_name = trace_out.relative_to(log_dir).with_suffix(\"\")\n",
    "        coverage = len(alignment) / trace_log_len(trace_out)\n",
    "        summary = {'datapoint':str(datapoint_name),'distance':score,'coverage':coverage,'alignment':alignment}\n",
    "        summary_data.append(summary)\n",
    "        deep_dives.append(summarize_datapoint(datapoint_name,alignment,debug_info))\n",
    "        debug_infos[datapoint_name] = debug_info\n",
    "    \n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    if len(deep_dives) > 0:\n",
    "        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)\n",
    "    else:\n",
    "        deep_dives_df = pd.DataFrame()\n",
    "\n",
    "    return TestSetRun(\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir,\n",
    "        summary=summary_df,\n",
    "        details=deep_dives_df,\n",
    "        debug=debug_infos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.rag import Rag\n",
    "from stringdale.db import ChromaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(conf_dir: Path):\n",
    "    agent_yaml_path = conf_dir / 'agent.yml'\n",
    "    vec_db_yaml_path = conf_dir / 'vec_db.yml'\n",
    "\n",
    "    agent_conf = yaml.safe_load(agent_yaml_path.read_text())\n",
    "    vec_db_conf = yaml.safe_load(vec_db_yaml_path.read_text())\n",
    "\n",
    "    db = ChromaClient()\n",
    "    for collection_name, docs in vec_db_conf.items():\n",
    "        db.add_collection(collection_name, exists_ok=True)\n",
    "        db.upsert(collection_name, docs)\n",
    "\n",
    "    agent_conf['db'] = db\n",
    "    \n",
    "    Agent = Rag(**agent_conf)\n",
    "\n",
    "    return Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"test_cases\"\n",
    "log_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"trace_logs\"\n",
    "conf_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"agent_configs\"\n",
    "\n",
    "\n",
    "comparisons = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'chat_eval':chat_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "    # TODO make a chat_eval where you can put the system prompt as a kwarg\n",
    "}\n",
    "\n",
    "default_comparison = cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v002\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v003\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n"
     ]
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    run1 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v001'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v001',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison,\n",
    "        )\n",
    "\n",
    "    run2 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v002'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v002',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n",
    "    run3 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v003'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v003',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSetRun(\n",
       "  test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "  log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001, \n",
       "  summary=Dataframe((2, 4)), \n",
       "  details=Dataframe((3, 13)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>distance</th>\n",
       "      <th>coverage</th>\n",
       "      <th>alignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{'0': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0.755937</td>\n",
       "      <td>0.50</td>\n",
       "      <td>{'0': 1, '1': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  distance  coverage         alignment\n",
       "0  pikachus  0.943595      0.25          {'0': 2}\n",
       "1   goldens  0.755937      0.50  {'0': 1, '1': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.655937</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2           They are:\\n  * friendly\\n  * intelligent  0.655937         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.610838</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are not dogs, they are fictional crea...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  Golden Retrievers are friendly, intelligent do...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.818815         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2           They are:\\n  * friendly\\n  * intelligent  0.610838         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.443801</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.651285</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are dangerous creatures that smoke to...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.443801         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2           They are:\\n  * friendly\\n  * intelligent  0.651285         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run3.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=0, datapoint='pikachus', node_label='0', trace_idx=2, comparison=None, key='content', actual='Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.', expected='They are:\\n  * dangerous\\n  * smoke cigarettes', distance=0.4438011800616313, node_idx=0, trace_name='chat', node_name='chat', kwargs={}, comp_id=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list(run3.details.itertuples())\n",
    "r= rows[0]\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here\n",
    "\n",
    "# a function that takes 2 datasetRuns and returns a comparison per datapoint on the difference between the two runs\n",
    "# then have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m math.isclose(a, b, *, rel_tol=\u001b[32m1e-09\u001b[39m, abs_tol=\u001b[32m0.0\u001b[39m)\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Determine whether two floating-point numbers are close in value.\n",
      "\n",
      "  rel_tol\n",
      "    maximum difference for being considered \"close\", relative to the\n",
      "    magnitude of the input values\n",
      "  abs_tol\n",
      "    maximum difference for being considered \"close\", regardless of the\n",
      "    magnitude of the input values\n",
      "\n",
      "Return True if a is close in value to b, and False otherwise.\n",
      "\n",
      "For the values to be considered close, the difference between them\n",
      "must be smaller than at least one of the tolerances.\n",
      "\n",
      "-inf, inf and NaN behave similarly to the IEEE 754 Standard.  That\n",
      "is, NaN is not close to anything, even itself.  inf and -inf are\n",
      "only close to themselves.\n",
      "\u001b[31mType:\u001b[39m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "math.isclose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sort_conditions(df):\n",
    "    return df.sort_values(by=['node_idx','comparison','expected'])\n",
    "\n",
    "def limit_to_datapoint(df,datapoint):\n",
    "    return df.loc[df['datapoint'] == datapoint]\n",
    "\n",
    "def get_datapoint(ds,datapoint):\n",
    "    return sort_conditions(limit_to_datapoint(ds.details,datapoint))\n",
    "\n",
    "\n",
    "def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Describe the changes between two datapoints\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the detailed version of the datasets and limit to only rows of the given datapoint\n",
    "    datapoint_df1 = get_datapoint_df(ds1,datapoint)\n",
    "    datapoint_df2 = get_datapoint_df(ds2,datapoint)\n",
    "\n",
    "    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)\n",
    "    # lets assert this in the code\n",
    "    assert datapoint_df1.shape == datapoint_df2.shape, f\"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}\"\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    # get the first comparison whose node aligned to a different trace\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if row1.trace_idx != row2.trace_idx:\n",
    "            changes.append({\n",
    "                'datapoint':datapoint,\n",
    "                'change_type':'alignment_change',\n",
    "                'before':row1.trace_idx,\n",
    "                'after':row2.trace_idx,\n",
    "                'comparison_id':row1.comp_id,\n",
    "            })\n",
    "            break\n",
    "            \n",
    "\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):\n",
    "            continue\n",
    "        if row2.distance + epsilon > row1.distance:\n",
    "            change_types = 'regression'\n",
    "        elif row2.distance - epsilon < row1.distance:\n",
    "            change_types = 'improvement'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        changes.append({\n",
    "            'datapoint':datapoint,\n",
    "            'change_type':change_types,\n",
    "            'value':row1.distance - row2.distance,\n",
    "            'comparison_id':row1.comp_id,\n",
    "            'node_label':row1.node_label,\n",
    "            'expected':row1.expected,\n",
    "            'before':row1.actual,\n",
    "            'after':row2.actual,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.045099</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.045099              1          1   \n",
       "\n",
       "                                   expected  \\\n",
       "0  They are:\\n  * friendly\\n  * intelligent   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_changes(run1,run2,'goldens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_datasets(ds1,ds2,epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Compare two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_1 = ds1.summary.sort_values(by='datapoint')\n",
    "    summary_2 = ds2.summary.sort_values(by='datapoint')\n",
    "\n",
    "    changed_datapoints = []\n",
    "    change_summary = []\n",
    "    detailed_changes = []\n",
    "\n",
    "    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):\n",
    "        datapoint = row1.datapoint\n",
    "        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)\n",
    "        coverage_change = row1.coverage != row2.coverage\n",
    "\n",
    "        if distance_change or coverage_change:\n",
    "            changed_datapoints.append(datapoint)\n",
    "\n",
    "            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)\n",
    "            detailed_changes.append(detailed_change)\n",
    "\n",
    "            change_types = set(detailed_change['change_type'])\n",
    "            if 'alignment_change' in change_types:\n",
    "                alignment_change = True\n",
    "            else:\n",
    "                alignment_change = False\n",
    "            \n",
    "            if 'improvement' in change_types and not 'regression' in change_types:\n",
    "                score_change = 'improved'\n",
    "            elif 'regression' in change_types and not 'improvement' in change_types:\n",
    "                score_change = 'regressed'\n",
    "            else:\n",
    "                score_change = 'changed'\n",
    "            \n",
    "            total_score_change = row1.distance-row2.distance\n",
    "\n",
    "            change_summary.append({\n",
    "                'datapoint':datapoint,\n",
    "                'alignment_change':alignment_change,\n",
    "                'score_change_type':score_change,\n",
    "                'total_score_change':total_score_change,\n",
    "            })\n",
    "\n",
    "    changes_summary = pd.DataFrame(change_summary)\n",
    "    if len(detailed_changes) > 0:\n",
    "        detailed_changes = pd.concat(detailed_changes)\n",
    "    else:\n",
    "        detailed_changes = pd.DataFrame()\n",
    "\n",
    "    return changes_summary,detailed_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.045099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.045099\n",
       "1  pikachus             False          improved            0.124780"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_summary,detailed_changes = compare_datasets(run1,run2)\n",
    "change_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.045099</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.124780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.045099              1          1   \n",
       "0  pikachus  improvement  0.124780              0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0        They are:\\n  * friendly\\n  * intelligent   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  \n",
       "0  Pikachus are not dogs, they are fictional crea...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.499794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.004653\n",
       "1  pikachus             False          improved            0.499794"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_summary,detailed_changes = compare_datasets(run1,run3)\n",
    "change_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.004653</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.499794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.004653              1          1   \n",
       "0  pikachus  improvement  0.499794              0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0        They are:\\n  * friendly\\n  * intelligent   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  The Golden Retriever is a friendly, intelligen...  \n",
       "0  Pikachus are dangerous creatures that smoke to...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO from here, make a comparison class that contains all relevant data for a comparison\n",
    "\n",
    "# then continue to make the pprint of a change class with topk changes of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO when pprinting comparisons, give urls to files, so that its easy in nb to navigate to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval main entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval()\n",
    "   \"\"\"\n",
    "    we take\n",
    "     - a dir with expected traced\n",
    "     - a dir to write traces to\n",
    "     - a dict of agents and their names/codes (ie v001, v002 etc)\n",
    "     - a cache_dir (if not, its a temp dir that we abandon later)\n",
    "     - a force flag (means invalidate the cache and re run the agents)\n",
    "     - a baseline version  (used to compare to), assumed to be the first in the list of agents unless otherwise specified\n",
    "     - summary_file=None if None pprint summary to console. Else save summary to file\n",
    "     - k = None, how many datapoints of each type to print to summary at most by default all\n",
    "     - silent = False, if True, dont pprint comparisons\n",
    "     - force_run = False, if True, delete the cache and re run the agents\n",
    "   \"\"\"\n",
    "\n",
    "    # TODO we make eval dataset able to load from a cachedir and run only those where the expected is not the same\n",
    "      # we check if its the same up to yaml whitespace, by comparing the yaml string after loading and serializing\n",
    "    \n",
    "    # we eval all datasets, expected over all agents concurrently\n",
    "\n",
    "    # then we run comparisons between the baseline dataset and the other datasets\n",
    "\n",
    "    # then, we pprint the summary of each comparison seperately\n",
    "\n",
    "    # then we group the per datapoint comps across all dataset comparisons by the datapoint id\n",
    "    # and for each datapoint we pprint a combined datapoint comparison.\n",
    "    # combined datapoints for each datapoint, the total metrics of each version\n",
    "    # and then for each comparison that is different from baseline, say how it is different for every version.\n",
    "\n",
    "    # we return an EvalResult object that tracks the input of the EvalData, but also has the DataSet and DataSetComp objects for each dataset and comparison\n",
    "   \n",
    "   pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add asyncio sempathores to Chat and DB operations etc, so that we dont get rate limited due to tons of async requests\n",
    "\n",
    "\"\"\" # Optional semaphore\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import Optional\n",
    "\n",
    "@asynccontextmanager\n",
    "async def optional_semaphore(semaphore: Optional[asyncio.Semaphore] = None):\n",
    "    if semaphore is not None:\n",
    "        async with semaphore:\n",
    "            yield\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "# Usage example:\n",
    "async def my_function(limit_concurrency: bool = False):\n",
    "    sem = asyncio.Semaphore(2) if limit_concurrency else None\n",
    "    \n",
    "    async with optional_semaphore(sem):\n",
    "        # Your async code here\n",
    "        await asyncio.sleep(1)\n",
    "        print(\"Function executed\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we do training and validation on workflows?\n",
    "\n",
    "# we have 2 expected datasets, train and test\n",
    "\n",
    "# we look at the total distance of the validation set to see that we are improving on it\n",
    "\n",
    "# but we only look at the comparisons and fix our configs or diagrams based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try to make a yaml schema for the expected traces, so that we can validate them and have better error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here see if we can throw out design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "New less confusing naming conventions\n",
    "\n",
    "A trace log is a single run of an agent\n",
    "\n",
    "A test case is a single yaml to describe our expected output of a trace log\n",
    "\n",
    "A test case in build of inputs and expected areas\n",
    "expected has a list of test nodes\n",
    "and each test node has a dict of test conditions\n",
    "\n",
    "\n",
    "A TestSet is a collection of test cases\n",
    "\n",
    "A LogSet is a collection of trace logs\n",
    "\n",
    "Lets avoid $keys and just make a slightly more verbose but more readable format\n",
    "with a list of conditions that have the \"key\" key for the accessor\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# in the end, we want 3 entrypoints:\n",
    "\n",
    "# eval, eval_single, and align_trace\n",
    "\n",
    "# eval will get lists of versions, and which comparisons to do, and log dir to save results to etc..\n",
    "\n",
    "\n",
    "# TODO add to the tutorial a performance section. Explain the lazy eval of distances\n",
    "# TODO add to \"V\" ability to specifiy the funcname for presenting in the drawing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
