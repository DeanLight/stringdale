{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "from stringdale.stream_warping import (\n",
    "    TestCase,\n",
    "    parse_test_case,\n",
    "    TraceLog,\n",
    "    event_stream_warp,\n",
    "    word_overlap,\n",
    "    regex,\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from typing import List, Union\n",
    "import jsonlines\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO multiple inputs\n",
    "# TODO ANY comparison so we can check for existance of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:\n",
    "    \"\"\"\n",
    "    Parse a trace file into a list of Trace objects.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(trace_path) as reader:\n",
    "        traces = [trace for trace in reader]\n",
    "        return TraceLog(steps=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = get_git_root() / \"sample_data\" / \"eval\"\n",
    "\n",
    "example_trace_log_path = sample_data_dir / \"traces0.jsonl\"\n",
    "example_case_path = sample_data_dir / \"expected0.yaml\"\n",
    "\n",
    "\n",
    "example_comparisons = {\n",
    "    \"word_overlap\":word_overlap,\n",
    "    \"regex\":regex,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_case = parse_test_case(example_case_path)\n",
    "example_trace_log  = parse_trace_log(example_trace_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozendict.frozendict({'3': 2, 'node_a1': 1, 'node_z': 4, '1': 5})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_match,score,debug_info =await event_stream_warp(example_trace_log,example_test_case,comparisons=example_comparisons,default_comparison=word_overlap)\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from stringdale.db import openai_embed\n",
    "from stringdale.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:\n",
    "    \"\"\"Compute cosine distance between two strings using OpenAI embeddings.\n",
    "    \n",
    "    Args:\n",
    "        out: First string to compare\n",
    "        expected: Second string to compare\n",
    "        model: OpenAI embedding model to use (default: 'text-embedding-3-small')\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two strings (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Get embeddings for both strings\n",
    "    if not isinstance(out,str):\n",
    "        return np.inf\n",
    "    if not isinstance(expected,str):\n",
    "        raise ValueError(f\"cosine_dist: expected is not a string: {expected}\")\n",
    "    out_embedding = await openai_embed(out, model=model)\n",
    "    expected_embedding = await openai_embed(expected, model=model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(out_embedding, expected_embedding)\n",
    "    norm_out = np.linalg.norm(out_embedding)\n",
    "    norm_expected = np.linalg.norm(expected_embedding)\n",
    "    \n",
    "    # Return cosine similarity\n",
    "    return 1-dot_product / (norm_out * norm_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_dist = await cosine_dist(\"hello\",\"hello\")\n",
    "basic_dist\n",
    "assert basic_dist < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3944818489490096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await cosine_dist(\"hello\",\"hello stranger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from stringdale.core import jinja_undeclared_vars\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatEvalScore(BaseModel):\n",
    "    score:float\n",
    "\n",
    "\n",
    "async def chat_eval(out:Any,expected:Any,model:str=\"gpt-4o-mini\",system_prompt:str=None)->float:\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"\n",
    "            You are a helpful assistant that evaluates the similarity of two strings.\n",
    "            You will be given two strings, and you will need to evaluate the similarity of the two strings.\n",
    "            You will need to return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "\n",
    "            string1: {{out}}\n",
    "            string2: {{expected}}\n",
    "\n",
    "            return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "            \"\"\"\n",
    "\n",
    "    if not jinja_undeclared_vars(system_prompt) == {'out','expected'}:\n",
    "        raise ValueError(\"System prompt must contain {{out}} and {{expected}} jinja variables\")\n",
    "\n",
    "    chat = Chat(model=model,messages=\n",
    "        [{\"role\":\"system\",\"content\":system_prompt}],\n",
    "        output_schema=ChatEvalScore,\n",
    "        out = out,\n",
    "        expected = expected,\n",
    "        )\n",
    "    response = await chat()\n",
    "    return response['content'].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{out}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n",
    "assert result == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{outs}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "with pytest.raises(ValueError,match=\"System prompt must contain {{out}} and {{expected}} jinja variables\"):\n",
    "    result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eq(a,b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def any(a,b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 3500\n",
    "expr = \"({0} < 4000) & ({0} > 3000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(3500 < 4000) & (3500 > 3000)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_expr = expr.format(out)\n",
    "f_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.tools import run_python_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def safe_eval(out,expression):\n",
    "    try:\n",
    "        formatted_expressions = expression.format(out)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error formatting expression: {expression} with value {out}, error: {e}\")\n",
    "        return np.inf\n",
    "    value = run_python_code(formatted_expressions)\n",
    "    if isinstance(value,str) and value.startswith(\"Error\"):\n",
    "        logger.warning(\n",
    "            f\"Error evaluating expression: {formatted_expressions} = {value}\\n\"\n",
    "            f\"out: {out}\\n\"\n",
    "            f\"expression: {expression}\\n\"\n",
    "            f\"error: {e}\"\n",
    "        )\n",
    "        return np.inf\n",
    "    logger.debug(f\"safe_eval: {formatted_expressions} = {value}\")\n",
    "    if isinstance(value,bool):\n",
    "        return 0 if value else np.inf\n",
    "    elif isinstance(value,float):\n",
    "        return value\n",
    "    else:\n",
    "        logger.debug(\n",
    "            f\"When evaluating {expression} with value {out}\\n\"\n",
    "            f\"Expected float or bool, got {type(value)} with value {repr(value)}\"\n",
    "            )\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - safe_eval: \n",
      "x=4000\n",
      "(3500 < x) & (3500 > 3000)\n",
      " = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_string =\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\"\n",
    "\n",
    "with checkLogs():\n",
    "    y =safe_eval(3500,eval_string)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_eval(3500,\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List,Dict,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPoint(BaseModel):\n",
    "    traces:TraceLog\n",
    "    expected:TestCase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):\n",
    "    d=Agent()\n",
    "    with jsonlines.open(trace_log_path,'w') as writer:\n",
    "        for input in test_case.inputs:\n",
    "            async for trace in d.arun(input):\n",
    "                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))\n",
    "            if d.finished:\n",
    "                break\n",
    "\n",
    "async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):\n",
    "    if trace_log_path is None:\n",
    "        trace_log_path = test_case_path.parent/test_case_path.name.replace(\".yaml\", \".jsonl\").replace(\"expected\", \"actual\")\n",
    "\n",
    "    if not trace_log_path.parent.exists():\n",
    "        os.makedirs(trace_log_path.parent,exist_ok=True)\n",
    "    try:\n",
    "        test_case = parse_test_case(test_case_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing test case {test_case_path}: {e}\") from e\n",
    "        \n",
    "\n",
    "    if force_run or not trace_log_path.exists():\n",
    "        if not trace_log_path.exists():\n",
    "            logger.info(f\"Trace file {trace_log_path.name} does not exist, running agent\")\n",
    "        else:\n",
    "            logger.info(f\"Force running {trace_log_path.name}\")\n",
    "        await _run_agent(Agent,test_case,trace_log_path)\n",
    "    else:\n",
    "        logger.info(f\"Trace file {trace_log_path.name} already exists, skipping agent run\")\n",
    "\n",
    "    parsed_trace = parse_trace_log(trace_log_path)\n",
    "    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)\n",
    "    \n",
    "    return aligned_trace,score,debug_info,trace_log_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.react import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent\n",
    "expected_yaml = sample_data_dir/\"react_expected.yaml\"\n",
    "bad_expected_yaml = sample_data_dir/\"react_bad_expected.yaml\"\n",
    "comparisons = {\n",
    "    \"eq\":eq,\n",
    "    \"eval\":safe_eval,\n",
    "}\n",
    "default_comparison = cosine_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_bad_actual.jsonl already exists, skipping agent run\n",
      "No viable trace row nums for expected trace 1\n",
      "No possible mappings found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " inf,\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_bad_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "with checkLogs():\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)\n",
    "\n",
    "assert alignment is None\n",
    "alignment,score,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_actual.jsonl already exists, skipping agent run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'0': 2, '1': 8}),\n",
       " np.float64(0.3244313858854829),\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "\n",
    "assert dict(alignment) == {'0': 2, '1': 8}\n",
    "alignment,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):\n",
    "    \"\"\"\n",
    "    Reorder the columns of a pandas dataframe to put the first_columns first.\n",
    "    \"\"\"\n",
    "    return df[first_columns + [c for c in df.columns if c not in first_columns]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  expected  distance comparison\n",
       "0       1         1         1         eq\n",
       "1       2         2         2         eq\n",
       "2       3         3         3         eq"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame([\n",
    "    {'distance':1,'comparison':'eq','actual':1,'expected':1},\n",
    "    {'distance':2,'comparison':'eq','actual':2,'expected':2},\n",
    "    {'distance':3,'comparison':'eq','actual':3,'expected':3},\n",
    "])\n",
    "\n",
    "_pd_order_columns_first(x,['actual','expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarize_datapoint(name,alignment,debug_info):\n",
    "    \"\"\"\n",
    "    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage\n",
    "    by using the alignment and the debug info\n",
    "    \"\"\"\n",
    "    deep_dive_fit = []\n",
    "\n",
    "    comp_counter = count()\n",
    "    for expected_node_id,trace_idx in alignment.items():\n",
    "        match_data = debug_info[expected_node_id][trace_idx]\n",
    "        for comp in match_data['comparisons']:\n",
    "            summary = deepcopy(match_data) | deepcopy(comp) \n",
    "            summary['comp_id'] = next(comp_counter)\n",
    "            summary.pop('comparisons')\n",
    "            deep_dive_fit.append(summary)\n",
    "\n",
    "    df = pd.DataFrame(deep_dive_fit)\n",
    "    df['datapoint'] = str(name)\n",
    "    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content.input.q</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Obama</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>content.output</td>\n",
       "      <td>3844</td>\n",
       "      <td>({0} &lt; 4000) &amp; ({0} &gt; 3000)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison              key  \\\n",
       "0     react          0          2         eq     content.name   \n",
       "1     react          0          2       None  content.input.q   \n",
       "2     react          1          8         eq     content.name   \n",
       "3     react          1          8       eval   content.output   \n",
       "\n",
       "             actual                     expected  distance  node_idx  \\\n",
       "0  wikipedia_search             wikipedia_search  0.000000         0   \n",
       "1      Barack Obama                        Obama  0.324431         0   \n",
       "2   run_python_code              run_python_code  0.000000         1   \n",
       "3              3844  ({0} < 4000) & ({0} > 3000)  0.000000         1   \n",
       "\n",
       "  trace_name node_name kwargs  comp_id  \n",
       "0   use_tool  use_tool     {}        0  \n",
       "1   use_tool  use_tool     {}        1  \n",
       "2   use_tool  use_tool     {}        2  \n",
       "3   use_tool  use_tool     {}        3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = summarize_datapoint('react',alignment,debug_info)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['node_label'].to_list() == ['0','0','1','1']\n",
    "assert df['key'].to_list() == ['content.name','content.input.q','content.name','content.output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale import DiagramSchema\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):\n",
    "    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(\".jsonl\")\n",
    "\n",
    "def _find_yamls(test_dir:Path):\n",
    "    expected_yamls = list(test_dir.glob(\"**/*.yaml\")) + list(test_dir.glob(\"**/*.yml\"))\n",
    "    return expected_yamls\n",
    "\n",
    "\n",
    "def trace_log_len(trace_log_path:Path):\n",
    "    \"\"\"Gets number of traces in a trace log file\n",
    "    This is equal to the number of nodes executed in the workflow\n",
    "    \"\"\"\n",
    "    return len(trace_log_path.read_text().splitlines())\n",
    "\n",
    "def serialize_test_case(test_case:Path):\n",
    "    yml_version = yaml.safe_load(test_case.read_text())\n",
    "    json_version = json.dumps(yml_version,indent=2)\n",
    "    return json_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TestSetRun(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    test_dir: Path\n",
    "    dir: Path\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "    debug: dict\n",
    "\n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"TestSetRun(\\n\"\n",
    "            f\"  test_dir={self.test_dir}, \\n\"\n",
    "            f\"  dir={self.log_dir}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\"  debug=dict)\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def save(self,dir:Path):\n",
    "        self.summary.to_csv(dir/\"summary.csv\")\n",
    "        self.details.to_csv(dir/\"details.csv\")\n",
    "        (dir/'test_cases_loc.txt').write_text(str(self.test_dir.relative_to(self.dir)))\n",
    "        with open(dir/\"debug.json\",\"w\") as f:\n",
    "            json.dump(self.debug,f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, dir: Path):\n",
    "        # Initialize empty DataFrames and dict for missing files\n",
    "        summary = pd.DataFrame(columns=['datapoint','distance','avg_distance','coverage','alignment','serialized_test_case'])\n",
    "        details = pd.DataFrame()\n",
    "        debug = {}\n",
    "        test_cases_loc = dir  # Default to the input directory\n",
    "        \n",
    "        # Try to load files if they exist\n",
    "        try:\n",
    "            if (dir/\"summary.csv\").exists():\n",
    "                summary = pd.read_csv(dir/\"summary.csv\")\n",
    "            if (dir/\"details.csv\").exists():\n",
    "                details = pd.read_csv(dir/\"details.csv\")\n",
    "            if (dir/\"debug.json\").exists():\n",
    "                with open(dir/\"debug.json\") as f:\n",
    "                    debug = json.load(f)\n",
    "            if (dir/'test_cases_loc.txt').exists():\n",
    "                test_cases_loc = (dir/'test_cases_loc.txt').read_text().strip()\n",
    "                test_cases_loc = dir/test_cases_loc\n",
    "        except Exception as e:\n",
    "            # Log the error but continue with empty/default values\n",
    "            print(f\"Warning: Error loading some files: {str(e)}\")\n",
    "        \n",
    "        return cls(\n",
    "            test_dir=test_cases_loc,\n",
    "            dir=dir,\n",
    "            summary=summary,\n",
    "            details=details,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "\n",
    "    def trace_log_path(self,datapoint:str):\n",
    "        return self.dir/'logs'/datapoint/'.jsonl'\n",
    "# TODO from here, factor all TestSetRun file logic to this class\n",
    "    def datapoint_len(self,datapoint:str):\n",
    "        datapoint_yaml = (self.test_dir/datapoint).with_suffix(\".yaml\")\n",
    "        return trace_log_len(self.dir/datapoint_yaml)\n",
    "\n",
    "    def serialize_test_case(self,datapoint_path):\n",
    "        pass\n",
    "\n",
    "    def is_datapoint_stale(self,datapoint_path):\n",
    "        pass\n",
    "        # if the datapoint is not in the summary, it is stale\n",
    "        # load_serialized_test_case\n",
    "        # if it is in summary but the serialized_test_case is different, it is stale\n",
    "        # else, not stale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def eval_dataset(Agent:DiagramSchema,test_dir,out_dir,comparisons,default_comparison,force_run=False):\n",
    "\n",
    "    # TODO from here\n",
    "    # check if log_dir has a summary (only if not force_run)\n",
    "    # if so, load it, and compare the test_case_json to the existing_yaml\n",
    "    # only if it is different, and the datapoint to the datapoints to be run\n",
    "    # since we computed ourselves which datapoints to run, we call evaluate_datapoint with force_run=True\n",
    "\n",
    "    run = TestSetRun.load(out_dir)\n",
    "\n",
    "    # TODO\n",
    "    # get_all_yamls\n",
    "    # filter to stale ones\n",
    "    # only run the stale ones\n",
    "\n",
    "\n",
    "    test_cases = _find_yamls(test_dir)\n",
    "    relative_test_cases = [test_cases.relative_to(test_dir) for test_cases in test_cases]\n",
    "\n",
    "    trace_files  = [_trace_out_path(test_case,test_dir,out_dir/'logs') for test_case in test_cases]\n",
    "\n",
    "    logger.info(f\"Evaluating {len(test_cases)} datapoints, logging to {out_dir/'logs'}\")\n",
    "    datapoint_tasks = [evaluate_datapoint(\n",
    "            Agent=Agent,\n",
    "            comparisons=comparisons,\n",
    "            default_comparison=default_comparison,\n",
    "            test_case_path=test_case,\n",
    "            trace_log_path=trace_file,\n",
    "            force_run=force_run,\n",
    "        ) for test_case,trace_file in zip(test_cases,trace_files) if trace_file in trace_files]\n",
    "    \n",
    "    datapoint_results = await asyncio.gather(*datapoint_tasks,return_exceptions=True)\n",
    "\n",
    "    for result,relative_test_case in zip(datapoint_results,relative_test_cases):\n",
    "      if isinstance(result,Exception):\n",
    "        result.args = (f\"When evaluating datapoint {relative_test_case}:\\n{result.args[0]}\", )+ result.args[1:]\n",
    "        raise result\n",
    "\n",
    "    summary_data = list()\n",
    "    deep_dives = list()\n",
    "    debug_infos = dict()\n",
    "\n",
    "    for (test_case,alignment,score,debug_info,trace_out),test_case_path in zip(datapoint_results,test_cases):\n",
    "        datapoint_name = trace_out.relative_to(log_dir).with_suffix(\"\")\n",
    "        serialized_test_case = serialize_test_case(test_case_path)\n",
    "        deep_dive = summarize_datapoint(datapoint_name,alignment,debug_info)\n",
    "        deep_dives.append(deep_dive)\n",
    "        avg_distance = deep_dive.distance.mean()\n",
    "        coverage = len(alignment) / trace_log_len(trace_out)\n",
    "        summary = {'datapoint':str(datapoint_name),'distance':score,'avg_distance':avg_distance,'coverage':coverage,'alignment':alignment,'serialized_test_case':serialized_test_case}\n",
    "        summary_data.append(summary)\n",
    "        debug_infos[datapoint_name] = debug_info\n",
    "    \n",
    "\n",
    "    # update the existing summary and details dataframes\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    if len(deep_dives) > 0:\n",
    "        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)\n",
    "    else:\n",
    "        deep_dives_df = pd.DataFrame()\n",
    "\n",
    "    return TestSetRun(\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir,\n",
    "        summary=summary_df,\n",
    "        details=deep_dives_df,\n",
    "        debug=debug_infos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.rag import Rag\n",
    "from stringdale.db import ChromaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(conf_dir: Path):\n",
    "    agent_yaml_path = conf_dir / 'agent.yml'\n",
    "    vec_db_yaml_path = conf_dir / 'vec_db.yml'\n",
    "\n",
    "    agent_conf = yaml.safe_load(agent_yaml_path.read_text())\n",
    "    vec_db_conf = yaml.safe_load(vec_db_yaml_path.read_text())\n",
    "\n",
    "    db = ChromaClient()\n",
    "    for collection_name, docs in vec_db_conf.items():\n",
    "        db.add_collection(collection_name, exists_ok=True)\n",
    "        db.upsert(collection_name, docs)\n",
    "\n",
    "    agent_conf['db'] = db\n",
    "    \n",
    "    Agent = Rag(**agent_conf)\n",
    "\n",
    "    return Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"test_cases\"\n",
    "log_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"trace_logs\"\n",
    "conf_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"agent_configs\"\n",
    "\n",
    "\n",
    "comparisons = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'chat_eval':chat_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "    # TODO make a chat_eval where you can put the system prompt as a kwarg\n",
    "}\n",
    "\n",
    "default_comparison = cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v002\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v003\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file goldens.jsonl already exists, skipping agent run\n"
     ]
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    run1 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v001'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v001',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison,\n",
    "        )\n",
    "\n",
    "    run2 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v002'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v002',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n",
    "    run3 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v003'),\n",
    "        test_dir=test_dir,\n",
    "        log_dir=log_dir/'v003',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSetRun(\n",
       "  test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "  log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001, \n",
       "  summary=Dataframe((2, 5)), \n",
       "  details=Dataframe((3, 13)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>coverage</th>\n",
       "      <th>alignment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.25</td>\n",
       "      <td>{'0': 2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0.751374</td>\n",
       "      <td>0.375687</td>\n",
       "      <td>0.50</td>\n",
       "      <td>{'0': 1, '1': 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  distance  avg_distance  coverage         alignment\n",
       "0  pikachus  0.943595      0.943595      0.25          {'0': 2}\n",
       "1   goldens  0.751374      0.375687      0.50  {'0': 1, '1': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.651374</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.651374         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.602882</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are not dogs, they are fictional crea...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  Golden Retrievers are friendly, intelligent do...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.818815         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.602882         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.443801</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.647368</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx comparison      key  \\\n",
       "0  pikachus          0          2       None  content   \n",
       "1   goldens          0          1  chat_eval        .   \n",
       "2   goldens          1          2       None  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are dangerous creatures that smoke to...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.443801         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.647368         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "   comp_id  \n",
       "0        0  \n",
       "1        0  \n",
       "2        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run3.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pandas(Index=0, datapoint='pikachus', node_label='0', trace_idx=2, comparison=None, key='content', actual='Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.', expected='They are:\\n  * dangerous\\n  * smoke cigarettes', distance=0.4438011800616313, node_idx=0, trace_name='chat', node_name='chat', kwargs={}, comp_id=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = list(run3.details.itertuples())\n",
    "r= rows[0]\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pprint\n",
    "# have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from typing import Optional\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Comparison(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    from_run: TestSetRun\n",
    "    to_run: TestSetRun\n",
    "    log_dir: Optional[Path] = None\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "\n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"Comparison(\\n\"\n",
    "            f\"  from_run={textwrap.indent(self.from_run.__repr__(), '  ').strip()}, \\n\"\n",
    "            f\"  to_run={textwrap.indent(self.to_run.__repr__(), '  ').strip()}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\")\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sort_conditions(df):\n",
    "    return df.sort_values(by=['node_idx','comparison','expected'])\n",
    "\n",
    "def limit_to_datapoint(df,datapoint):\n",
    "    return df.loc[df['datapoint'] == datapoint]\n",
    "\n",
    "def get_datapoint(ds,datapoint):\n",
    "    return sort_conditions(limit_to_datapoint(ds.details,datapoint))\n",
    "\n",
    "\n",
    "def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Describe the changes between two datapoints\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the detailed version of the datasets and limit to only rows of the given datapoint\n",
    "    datapoint_df1 = get_datapoint(ds1,datapoint)\n",
    "    datapoint_df2 = get_datapoint(ds2,datapoint)\n",
    "\n",
    "    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)\n",
    "    # lets assert this in the code\n",
    "    assert datapoint_df1.shape == datapoint_df2.shape, f\"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}\"\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    # get the first comparison whose node aligned to a different trace\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if row1.trace_idx != row2.trace_idx:\n",
    "            changes.append({\n",
    "                'datapoint':datapoint,\n",
    "                'change_type':'alignment_change',\n",
    "                'before':row1.trace_idx,\n",
    "                'after':row2.trace_idx,\n",
    "                'comparison_id':row1.comp_id,\n",
    "            })\n",
    "            break\n",
    "            \n",
    "\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):\n",
    "            continue\n",
    "        if row2.distance + epsilon > row1.distance:\n",
    "            change_types = 'regression'\n",
    "        elif row2.distance - epsilon < row1.distance:\n",
    "            change_types = 'improvement'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        changes.append({\n",
    "            'datapoint':datapoint,\n",
    "            'change_type':change_types,\n",
    "            'value':row1.distance - row2.distance,\n",
    "            'comparison_id':row1.comp_id,\n",
    "            'node_label':row1.node_label,\n",
    "            'expected':row1.expected,\n",
    "            'before':row1.actual,\n",
    "            'after':row2.actual,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.048492              1          1   \n",
       "\n",
       "                                     expected  \\\n",
       "0  They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_changes(run1,run2,'goldens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_datasets(ds1,ds2,epsilon=1e-3,log_dir=None):\n",
    "    \"\"\"\n",
    "    Compare two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_1 = ds1.summary.sort_values(by='datapoint')\n",
    "    summary_2 = ds2.summary.sort_values(by='datapoint')\n",
    "\n",
    "    changed_datapoints = []\n",
    "    change_summary = []\n",
    "    detailed_changes = []\n",
    "\n",
    "    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):\n",
    "        datapoint = row1.datapoint\n",
    "        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)\n",
    "        coverage_change = row1.coverage != row2.coverage\n",
    "\n",
    "        if distance_change or coverage_change:\n",
    "            changed_datapoints.append(datapoint)\n",
    "\n",
    "            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)\n",
    "            detailed_changes.append(detailed_change)\n",
    "\n",
    "            change_types = set(detailed_change['change_type'])\n",
    "            if 'alignment_change' in change_types:\n",
    "                alignment_change = True\n",
    "            else:\n",
    "                alignment_change = False\n",
    "            \n",
    "            if 'improvement' in change_types and not 'regression' in change_types:\n",
    "                score_change = 'improved'\n",
    "            elif 'regression' in change_types and not 'improvement' in change_types:\n",
    "                score_change = 'regressed'\n",
    "            else:\n",
    "                score_change = 'changed'\n",
    "            \n",
    "            total_score_change = row1.distance-row2.distance\n",
    "\n",
    "            change_summary.append({\n",
    "                'datapoint':datapoint,\n",
    "                'alignment_change':alignment_change,\n",
    "                'score_change_type':score_change,\n",
    "                'total_score_change':total_score_change,\n",
    "            })\n",
    "\n",
    "    changes_summary = pd.DataFrame(change_summary)\n",
    "    if len(detailed_changes) > 0:\n",
    "        detailed_changes = pd.concat(detailed_changes)\n",
    "    else:\n",
    "        detailed_changes = pd.DataFrame()\n",
    "\n",
    "    return Comparison(\n",
    "        from_run=ds1,\n",
    "        to_run=ds2,\n",
    "        summary=changes_summary,\n",
    "        details=detailed_changes,\n",
    "        log_dir=log_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comparison(\n",
       "  from_run=TestSetRun(\n",
       "    test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "    log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001, \n",
       "    summary=Dataframe((2, 5)), \n",
       "    details=Dataframe((3, 13)), \n",
       "    debug=dict), \n",
       "  to_run=TestSetRun(\n",
       "    test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "    log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v002, \n",
       "    summary=Dataframe((2, 5)), \n",
       "    details=Dataframe((3, 13)), \n",
       "    debug=dict), \n",
       "  summary=Dataframe((2, 4)), \n",
       "  details=Dataframe((2, 8)), \n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2 = compare_datasets(run1,run2)\n",
    "comparison_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.048492\n",
       "1  pikachus             False          improved            0.124780"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.124780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.048492              1          1   \n",
       "0  pikachus  improvement  0.124780              0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0      They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  \n",
       "0  Pikachus are not dogs, they are fictional crea...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.499794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.004006\n",
       "1  pikachus             False          improved            0.499794"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_3 = compare_datasets(run1,run3)\n",
    "comparison_3.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comparison_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improvement</td>\n",
       "      <td>0.499794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  change_type     value  comparison_id node_label  \\\n",
       "0   goldens  improvement  0.004006              1          1   \n",
       "0  pikachus  improvement  0.499794              0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0      They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  The Golden Retriever is a friendly, intelligen...  \n",
       "0  Pikachus are dangerous creatures that smoke to...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_3.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSetRun(\n",
       "  test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "  log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/trace_logs/v001, \n",
       "  summary=Dataframe((2, 5)), \n",
       "  details=Dataframe((3, 13)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then continue to make the pprint of a change class with topk changes of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO when pprinting comparisons, give urls to files, so that its easy in nb to navigate to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval main entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make eval_dataset save the expected yaml files as serialized objects, and use them to be indempotent\n",
    "# same with comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable,Dict,List,Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EVAL_COMPARISONS = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'chat_eval':chat_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "}\n",
    "\n",
    "EVAL_DEFAULT_COMPARISON = cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  we make eval dataset load_from output dir\n",
    "    # we check if its the same up to yaml whitespace, by comparing the yaml string after loading and serializing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def eval(\n",
    "  test_dir:Path,\n",
    "  out_dir:Path,\n",
    "  agents:List[Tuple[str,DiagramSchema]],\n",
    "  k:Optional[int]=5,\n",
    "  force_run:bool=False,\n",
    "  silent:bool=False,\n",
    "  comparisons: Optional[Dict[str,Callable]]=None,\n",
    "  default_comparison:Optional[Callable]=None,\n",
    "  ):\n",
    "  \"\"\"\n",
    "  The main eval function.\n",
    "  Evaluates a set of agents on a set of tests.\n",
    "  Compares the results of all agents to the first agent.\n",
    "  pprints a summary of the results to the console.\n",
    "  and saves all files to the out_dir.\n",
    "\n",
    "  Args:\n",
    "    tests_dir: Path to the directory containing the tests.\n",
    "    out_dir: Path to the directory to write the results to.\n",
    "    agents: A list of tuples of agent names and their DiagramSchema.\n",
    "    k: The number of datapoints to print to the summary at most. Defaults to 5.\n",
    "    force_run: If True, deletes out dir content and reruns the agents. \n",
    "      If False, we skip the agents that have already been run.\n",
    "      Defaults to False.\n",
    "    silent: If True, dont pprint comparisons. Defaults to False.\n",
    "    comparisons: A dictionary of comparison names and their functions, to add to the allowed comparisons.\n",
    "      Defaults to None.\n",
    "    default_comparison: The default comparison function to use if no comparison is specified.\n",
    "      This is used to compare the first agent to the rest of the agents.\n",
    "      Defaults to stringdale.eval.cosine_dist\n",
    "  \"\"\"\n",
    "\n",
    "  global EVAL_COMPARISONS\n",
    "  global EVAL_DEFAULT_COMPARISON\n",
    "  if comparisons is not None:\n",
    "    comparisons = EVAL_COMPARISONS | comparisons\n",
    "  else: \n",
    "    comparisons = EVAL_COMPARISONS\n",
    "  if default_comparison is None:\n",
    "    default_comparison = EVAL_DEFAULT_COMPARISON\n",
    "\n",
    "  eval_dataset_tasks = []\n",
    "  for agent_name,agent_schema in agents:\n",
    "    log_dir = f'{out_dir}/logs/{agent_name}'\n",
    "    eval_dataset_tasks.append(eval_dataset(\n",
    "      Agent=agent_schema,\n",
    "      test_dir=test_dir,\n",
    "      log_dir=log_dir,\n",
    "      comparisons=comparisons,\n",
    "      default_comparison=default_comparison))\n",
    "\n",
    "  datasets = await asyncio.gather(*eval_dataset_tasks,return_exceptions=True)\n",
    "\n",
    "  for result,(agent_name,_) in zip(datasets,agents):\n",
    "    if isinstance(result,Exception):\n",
    "      result.args = (f\"When evaluating agent {agent_name}:\\n{result.args[0]}\", )+ result.args[1:]\n",
    "      raise result\n",
    "\n",
    "  first_dataset = datasets[0]\n",
    "  comparisons = []\n",
    "  for dataset in datasets[1:]:\n",
    "    comparisons.append(compare_datasets(first_dataset,dataset))\n",
    "  \n",
    "  return datasets,comparisons\n",
    "\n",
    "  # then, we pprint the summary of each comparison seperately\n",
    "\n",
    "  # then we group the per datapoint comps across all dataset comparisons by the datapoint id\n",
    "  # and for each datapoint we pprint a combined datapoint comparison.\n",
    "  # combined datapoints for each datapoint, the total metrics of each version\n",
    "  # and then for each comparison that is different from baseline, say how it is different for every version.\n",
    "\n",
    "  # we return an EvalResult object that tracks the input of the EvalData, but also has the DataSet and DataSetComp objects for each dataset and comparison\n",
    "  \n",
    "  pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v001', <stringdale.base.DiagramSchema>),\n",
       " ('v002', <stringdale.base.DiagramSchema>),\n",
       " ('v003', <stringdale.base.DiagramSchema>)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = [\n",
    "    (version,load_agent(conf_dir/version))\n",
    "    for version in ['v001','v002','v003']\n",
    "]\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"test_cases\"\n",
    "eval_out = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"eval_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await eval(\n",
    "    test_dir=test_dir,\n",
    "    out_dir=eval_out,\n",
    "    agents=agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,c = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestSetRun(\n",
       "   test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "   log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v001, \n",
       "   summary=Dataframe((2, 5)), \n",
       "   details=Dataframe((3, 13)), \n",
       "   debug=dict),\n",
       " TestSetRun(\n",
       "   test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "   log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v002, \n",
       "   summary=Dataframe((2, 5)), \n",
       "   details=Dataframe((3, 13)), \n",
       "   debug=dict),\n",
       " TestSetRun(\n",
       "   test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "   log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v003, \n",
       "   summary=Dataframe((2, 5)), \n",
       "   details=Dataframe((3, 13)), \n",
       "   debug=dict)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Comparison(\n",
       "   from_run=TestSetRun(\n",
       "     test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "     log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v001, \n",
       "     summary=Dataframe((2, 5)), \n",
       "     details=Dataframe((3, 13)), \n",
       "     debug=dict), \n",
       "   to_run=TestSetRun(\n",
       "     test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "     log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v002, \n",
       "     summary=Dataframe((2, 5)), \n",
       "     details=Dataframe((3, 13)), \n",
       "     debug=dict), \n",
       "   summary=Dataframe((1, 4)), \n",
       "   details=Dataframe((1, 8)), \n",
       " ),\n",
       " Comparison(\n",
       "   from_run=TestSetRun(\n",
       "     test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "     log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v001, \n",
       "     summary=Dataframe((2, 5)), \n",
       "     details=Dataframe((3, 13)), \n",
       "     debug=dict), \n",
       "   to_run=TestSetRun(\n",
       "     test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "     log_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/eval_out/logs/v003, \n",
       "     summary=Dataframe((2, 5)), \n",
       "     details=Dataframe((3, 13)), \n",
       "     debug=dict), \n",
       "   summary=Dataframe((0, 0)), \n",
       "   details=Dataframe((0, 0)), \n",
       " )]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# in the end, we want few entrypoints:\n",
    "\n",
    "# eval, eval_single, and align_trace, validate_tests (just to make sure they are formatted correctly)\n",
    "\n",
    "# eval will get lists of versions, and which comparisons to do, and log dir to save results to etc..\n",
    "\n",
    "\n",
    "# TODO add to the tutorial a performance section. Explain the lazy eval of distances\n",
    "# TODO add to \"V\" ability to specifiy the funcname for presenting in the drawing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain in tutorial\n",
    "\n",
    "# how do we do training and validation on workflows?\n",
    "\n",
    "# we have 2 expected datasets, train and test\n",
    "\n",
    "# we look at the total distance of the validation set to see that we are improving on it\n",
    "\n",
    "# but we only look at the comparisons and fix our configs or diagrams based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add asyncio sempathores to Chat and DB operations etc, so that we dont get rate limited due to tons of async requests\n",
    "\n",
    "\"\"\" # Optional semaphore\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import Optional\n",
    "\n",
    "@asynccontextmanager\n",
    "async def optional_semaphore(semaphore: Optional[asyncio.Semaphore] = None):\n",
    "    if semaphore is not None:\n",
    "        async with semaphore:\n",
    "            yield\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "# Usage example:\n",
    "async def my_function(limit_concurrency: bool = False):\n",
    "    sem = asyncio.Semaphore(2) if limit_concurrency else None\n",
    "    \n",
    "    async with optional_semaphore(sem):\n",
    "        # Your async code here\n",
    "        await asyncio.sleep(1)\n",
    "        print(\"Function executed\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
