{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "from stringdale.stream_warping import (\n",
    "    TestCase,\n",
    "    parse_test_case,\n",
    "    TraceLog,\n",
    "    event_stream_warp,\n",
    "    word_overlap,\n",
    "    regex,\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs,await_all\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from typing import List, Union\n",
    "import jsonlines\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO multiple inputs\n",
    "# TODO ANY comparison so we can check for existance of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse_trace_log(trace_path:Union[str,Path]) -> TraceLog:\n",
    "    \"\"\"\n",
    "    Parse a trace file into a list of Trace objects.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(trace_path) as reader:\n",
    "        traces = [trace for trace in reader]\n",
    "        return TraceLog(steps=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = get_git_root() / \"sample_data\" / \"eval\"\n",
    "\n",
    "example_trace_log_path = sample_data_dir / \"traces0.jsonl\"\n",
    "example_case_path = sample_data_dir / \"expected0.yaml\"\n",
    "\n",
    "\n",
    "example_comparisons = {\n",
    "    \"word_overlap\":word_overlap,\n",
    "    \"regex\":regex,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test_case = parse_test_case(example_case_path)\n",
    "example_trace_log  = parse_trace_log(example_trace_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozendict.frozendict({'3': 2, 'node_a1': 1, 'node_z': 4, '1': 5})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_match,score,debug_info =await event_stream_warp(example_trace_log,example_test_case,comparisons=example_comparisons,default_comparison=\"word_overlap\")\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from stringdale.db import openai_embed\n",
    "from stringdale.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:\n",
    "    \"\"\"Compute cosine distance between two strings using OpenAI embeddings.\n",
    "    \n",
    "    Args:\n",
    "        out: First string to compare\n",
    "        expected: Second string to compare\n",
    "        model: OpenAI embedding model to use (default: 'text-embedding-3-small')\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two strings (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Get embeddings for both strings\n",
    "    if not isinstance(out,str):\n",
    "        return np.inf\n",
    "    if not isinstance(expected,str):\n",
    "        raise ValueError(f\"cosine_dist: expected is not a string: {expected}\")\n",
    "    out_embedding = await openai_embed(out, model=model)\n",
    "    expected_embedding = await openai_embed(expected, model=model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(out_embedding, expected_embedding)\n",
    "    norm_out = np.linalg.norm(out_embedding)\n",
    "    norm_expected = np.linalg.norm(expected_embedding)\n",
    "    \n",
    "    # Return cosine similarity\n",
    "    return 1-dot_product / (norm_out * norm_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_dist = await cosine_dist(\"hello\",\"hello\")\n",
    "basic_dist\n",
    "assert basic_dist < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3944818489490096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await cosine_dist(\"hello\",\"hello stranger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from stringdale.core import jinja_undeclared_vars\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatEvalScore(BaseModel):\n",
    "    score:float\n",
    "\n",
    "\n",
    "async def chat_eval(out:Any,expected:Any,model:str=\"gpt-4o-mini\",system_prompt:str=None)->float:\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"\"\"\n",
    "            You are a helpful assistant that evaluates the similarity of two strings.\n",
    "            You will be given two strings, and you will need to evaluate the similarity of the two strings.\n",
    "            You will need to return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "\n",
    "            string1: {{out}}\n",
    "            string2: {{expected}}\n",
    "\n",
    "            return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "            \"\"\"\n",
    "\n",
    "    if not jinja_undeclared_vars(system_prompt) == {'out','expected'}:\n",
    "        raise ValueError(\"System prompt must contain {{out}} and {{expected}} jinja variables\")\n",
    "\n",
    "    chat = Chat(model=model,messages=\n",
    "        [{\"role\":\"system\",\"content\":system_prompt}],\n",
    "        output_schema=ChatEvalScore,\n",
    "        out = out,\n",
    "        expected = expected,\n",
    "        )\n",
    "    response = await chat()\n",
    "    return response['content'].score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{out}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n",
    "assert result == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = \"\"\"\n",
    "if one of the strings contains \"hello\", return 0.5\n",
    "\n",
    "string1: {{outs}}\n",
    "string2: {{expected}}\n",
    "\"\"\"\n",
    "\n",
    "with pytest.raises(ValueError,match=\"System prompt must contain {{out}} and {{expected}} jinja variables\"):\n",
    "    result = await chat_eval(\"hello\",\"world\",system_prompt=custom_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eq(a,b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def any(a,b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 3500\n",
    "expr = \"({0} < 4000) & ({0} > 3000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(3500 < 4000) & (3500 > 3000)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_expr = expr.format(out)\n",
    "f_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.tools import run_python_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def safe_eval(out,expression):\n",
    "    try:\n",
    "        formatted_expressions = expression.format(out)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error formatting expression: {expression} with value {out}, error: {e}\")\n",
    "        return np.inf\n",
    "    value = run_python_code(formatted_expressions)\n",
    "    if isinstance(value,str) and value.startswith(\"Error\"):\n",
    "        logger.warning(\n",
    "            f\"Error evaluating expression: {formatted_expressions} = {value}\\n\"\n",
    "            f\"out: {out}\\n\"\n",
    "            f\"expression: {expression}\\n\"\n",
    "            f\"error: {e}\"\n",
    "        )\n",
    "        return np.inf\n",
    "    logger.debug(f\"safe_eval: {formatted_expressions} = {value}\")\n",
    "    if isinstance(value,bool):\n",
    "        return 0 if value else np.inf\n",
    "    elif isinstance(value,float):\n",
    "        return value\n",
    "    else:\n",
    "        logger.debug(\n",
    "            f\"When evaluating {expression} with value {out}\\n\"\n",
    "            f\"Expected float or bool, got {type(value)} with value {repr(value)}\"\n",
    "            )\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - safe_eval: \n",
      "x=4000\n",
      "(3500 < x) & (3500 > 3000)\n",
      " = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_string =\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\"\n",
    "\n",
    "with checkLogs():\n",
    "    y =safe_eval(3500,eval_string)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_eval(3500,\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List,Dict,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPoint(BaseModel):\n",
    "    traces:TraceLog\n",
    "    expected:TestCase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def _run_agent(Agent,test_case:TestCase,trace_log_path:Path):\n",
    "    d=Agent()\n",
    "    with jsonlines.open(trace_log_path,'w') as writer:\n",
    "        for input in test_case.inputs:\n",
    "            async for trace in d.arun(input):\n",
    "                if trace.node_func is None:\n",
    "                    continue\n",
    "                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))\n",
    "            if d.finished:\n",
    "                break\n",
    "\n",
    "async def evaluate_datapoint(Agent,comparisons,default_comparison,test_case_path,trace_log_path=None,force_run=False):\n",
    "    if trace_log_path is None:\n",
    "        trace_log_path = test_case_path.parent/test_case_path.name.replace(\".yaml\", \".jsonl\").replace(\"expected\", \"actual\")\n",
    "\n",
    "    if not trace_log_path.parent.exists():\n",
    "        os.makedirs(trace_log_path.parent,exist_ok=True)\n",
    "    try:\n",
    "        test_case = parse_test_case(test_case_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing test case {test_case_path}: {e}\") from e\n",
    "        \n",
    "\n",
    "    if force_run or not trace_log_path.exists():\n",
    "        if not trace_log_path.exists():\n",
    "            logger.info(f\"Trace file {trace_log_path.name} does not exist, running agent\")\n",
    "        else:\n",
    "            logger.info(f\"Force running {trace_log_path.name}\")\n",
    "        await _run_agent(Agent,test_case,trace_log_path)\n",
    "    else:\n",
    "        logger.info(f\"Trace file {trace_log_path.name} already exists, skipping agent run\")\n",
    "\n",
    "    parsed_trace = parse_trace_log(trace_log_path)\n",
    "    aligned_trace,score,debug_info = await event_stream_warp(parsed_trace,test_case,comparisons,default_comparison)\n",
    "    \n",
    "    return aligned_trace,score,debug_info,trace_log_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.react import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent\n",
    "expected_yaml = sample_data_dir/\"react_expected.yaml\"\n",
    "bad_expected_yaml = sample_data_dir/\"react_bad_expected.yaml\"\n",
    "comparisons = {\n",
    "    \"eq\":eq,\n",
    "    \"eval\":safe_eval,\n",
    "    \"cosine_dist\":cosine_dist,\n",
    "}\n",
    "default_comparison = 'cosine_dist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_bad_actual.jsonl already exists, skipping agent run\n",
      "No viable trace row nums for expected trace 1\n",
      "No possible mappings found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " inf,\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_bad_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "with checkLogs():\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)\n",
    "\n",
    "assert alignment is None\n",
    "alignment,score,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_actual.jsonl already exists, skipping agent run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'0': 2, '1': 8}),\n",
       " np.float64(0.3244313858854829),\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "\n",
    "assert dict(alignment) == {'0': 2, '1': 8}\n",
    "alignment,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):\n",
    "    \"\"\"\n",
    "    Reorder the columns of a pandas dataframe to put the first_columns first.\n",
    "    \"\"\"\n",
    "    return df[first_columns + [c for c in df.columns if c not in first_columns]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  expected  distance comparison\n",
       "0       1         1         1         eq\n",
       "1       2         2         2         eq\n",
       "2       3         3         3         eq"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame([\n",
    "    {'distance':1,'comparison':'eq','actual':1,'expected':1},\n",
    "    {'distance':2,'comparison':'eq','actual':2,'expected':2},\n",
    "    {'distance':3,'comparison':'eq','actual':3,'expected':3},\n",
    "])\n",
    "\n",
    "_pd_order_columns_first(x,['actual','expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "from itertools import count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarize_datapoint(name,alignment,debug_info):\n",
    "    \"\"\"\n",
    "    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage\n",
    "    by using the alignment and the debug info\n",
    "    \"\"\"\n",
    "    deep_dive_fit = []\n",
    "\n",
    "    comp_counter = count()\n",
    "    for expected_node_id,trace_idx in alignment.items():\n",
    "        match_data = debug_info[expected_node_id][trace_idx]\n",
    "        for comp in match_data['comparisons']:\n",
    "            summary = deepcopy(match_data) | deepcopy(comp) \n",
    "            summary['comp_id'] = next(comp_counter)\n",
    "            summary.pop('comparisons')\n",
    "            summary['aggregation'] = comp['aggregation']\n",
    "            deep_dive_fit.append(summary)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(deep_dive_fit)\n",
    "    df['datapoint'] = str(name)\n",
    "    df = _pd_order_columns_first(df,['datapoint','node_label','trace_idx','comparison','key','actual','expected','distance'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content.input.q</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Obama</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>0</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eq</td>\n",
       "      <td>content.name</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>content.output</td>\n",
       "      <td>3844</td>\n",
       "      <td>({0} &lt; 4000) &amp; ({0} &gt; 3000)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx   comparison              key  \\\n",
       "0     react          0          2           eq     content.name   \n",
       "1     react          0          2  cosine_dist  content.input.q   \n",
       "2     react          1          8           eq     content.name   \n",
       "3     react          1          8         eval   content.output   \n",
       "\n",
       "             actual                     expected  distance  node_idx  \\\n",
       "0  wikipedia_search             wikipedia_search  0.000000         0   \n",
       "1      Barack Obama                        Obama  0.324431         0   \n",
       "2   run_python_code              run_python_code  0.000000         1   \n",
       "3              3844  ({0} < 4000) & ({0} > 3000)  0.000000         1   \n",
       "\n",
       "  trace_name node_name kwargs aggregation  comp_id  \n",
       "0   use_tool  use_tool     {}        None        0  \n",
       "1   use_tool  use_tool     {}        None        1  \n",
       "2   use_tool  use_tool     {}        None        2  \n",
       "3   use_tool  use_tool     {}        None        3  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = summarize_datapoint('react',alignment,debug_info)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['node_label'].to_list() == ['0','0','1','1']\n",
    "assert df['key'].to_list() == ['content.name','content.input.q','content.name','content.output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_and_concat(df1: pd.DataFrame, df2: pd.DataFrame, keys: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter df1 by removing rows with matching key values in df2, then concatenate with df2.\n",
    "    \n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First DataFrame to filter\n",
    "        df2 (pd.DataFrame): Second DataFrame to concatenate\n",
    "        keys (list): List of column names to use as keys for matching\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated DataFrame with filtered df1 and df2\n",
    "    \"\"\"\n",
    "    if df1.empty:\n",
    "        return df2\n",
    "    if df2.empty:\n",
    "        return df1\n",
    "    # Create tuples of key values for comparison\n",
    "    mask = df1[keys].apply(tuple, axis=1).isin(df2[keys].apply(tuple, axis=1))\n",
    "    \n",
    "    # Filter df1 to keep only rows that don't exist in df2 (using inverse mask)\n",
    "    df1_filtered = df1[~mask]\n",
    "    \n",
    "    # Concatenate the filtered df1 with df2\n",
    "    result = pd.concat([df1_filtered, df2], ignore_index=True)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B   C\n",
      "0  1  a  10\n",
      "1  2  b  20\n",
      "2  3  c  50\n",
      "3  4  d  60\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrames\n",
    "df1 = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': ['a', 'b', 'c', 'd'],\n",
    "    'C': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'A': [3, 4],\n",
    "    'B': ['c', 'd'],\n",
    "    'C': [50, 60]\n",
    "})\n",
    "\n",
    "# List of keys to match on\n",
    "keys = ['A', 'B']\n",
    "\n",
    "# Apply the function\n",
    "result = filter_and_concat(df1, df2, keys)\n",
    "\n",
    "assert result.equals(pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4],\n",
    "    'B': ['a', 'b', 'c', 'd'],\n",
    "    'C': [10, 20, 50, 60]\n",
    "}))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale import DiagramSchema\n",
    "from pprint import pprint, pformat\n",
    "from fastcore.basics import patch\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, ConfigDict, PrivateAttr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TestSetRun(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    # Private attributes\n",
    "    _summary_dict: dict = PrivateAttr(default_factory=dict)\n",
    "    _details_dict: dict = PrivateAttr(default_factory=dict)\n",
    "    \n",
    "    # Regular fields\n",
    "    test_dir: Path\n",
    "    dir: Path\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "    debug: dict\n",
    "\n",
    "    def find_cases(self):\n",
    "        yaml_paths =  list(self.test_dir.glob(\"**/*.yaml\"))\n",
    "        return [str(p.relative_to(self.test_dir).with_suffix(\"\")) for p in yaml_paths]\n",
    "\n",
    "    def trace_log_path(self,datapoint:str):\n",
    "        return self.dir/'logs'/f'{datapoint}.jsonl'\n",
    "\n",
    "    def trace_log_len(self,datapoint:str):\n",
    "        log_path = self.trace_log_path(datapoint)\n",
    "        return len(log_path.read_text().splitlines())\n",
    "\n",
    "    def testcase_path(self,datapoint:str):\n",
    "        return self.test_dir/f'{datapoint}.yaml'\n",
    "\n",
    "    def serialize_test_case(self,datapoint:str):\n",
    "        yml_version = yaml.safe_load(self.testcase_path(datapoint).read_text())\n",
    "        json_version = json.dumps(yml_version,indent=2)\n",
    "        return json_version\n",
    "\n",
    "    def datapoint_len(self,datapoint:str):\n",
    "        datapoint_yaml = (self.test_dir/datapoint).with_suffix(\".yaml\")\n",
    "        return trace_log_len(self.dir/datapoint_yaml)\n",
    "        \n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"TestSetRun(\\n\"\n",
    "            f\"  test_dir={self.test_dir}, \\n\"\n",
    "            f\"  dir={self.dir}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\"  debug=dict)\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def save(self,dir:Path):\n",
    "        self.summary.to_csv(dir/\"summary.csv\",index=False)\n",
    "        self.details.to_csv(dir/\"details.csv\",index=False)\n",
    "        test_dir_rel = Path(os.path.relpath(self.test_dir,self.dir))\n",
    "        (dir/'test_cases_loc.txt').write_text(str(test_dir_rel))\n",
    "        with open(dir/\"debug.json\",\"w\") as f:\n",
    "            json.dump(self.debug,f)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, dir: Path,test_dir:Optional[Path]=None):\n",
    "        # Initialize empty DataFrames and dict for missing files\n",
    "        summary = pd.DataFrame()\n",
    "        details = pd.DataFrame()\n",
    "        debug = {}\n",
    "        if test_dir is None:\n",
    "            test_dir = dir\n",
    "        \n",
    "        # Try to load files if they exist\n",
    "        try:\n",
    "            if (dir/\"summary.csv\").exists():\n",
    "                summary = pd.read_csv(dir/\"summary.csv\",index_col=False)\n",
    "            if (dir/\"details.csv\").exists():\n",
    "                details = pd.read_csv(dir/\"details.csv\",index_col=False)\n",
    "            if (dir/\"debug.json\").exists():\n",
    "                with open(dir/\"debug.json\") as f:\n",
    "                    debug = json.load(f)\n",
    "            if (dir/'test_cases_loc.txt').exists():\n",
    "                test_cases_loc = (dir/'test_cases_loc.txt').read_text().strip()\n",
    "                test_cases_loc = dir/test_cases_loc\n",
    "        except Exception as e:\n",
    "            # Log the error but continue with empty/default values\n",
    "            print(f\"Warning: Error loading some files: {str(e)}\")\n",
    "        \n",
    "        return cls(\n",
    "            test_dir=test_dir,\n",
    "            dir=dir,\n",
    "            summary=summary,\n",
    "            details=details,\n",
    "            debug=debug\n",
    "        )\n",
    "\n",
    "    def is_datapoint_stale(self,datapoint_path):\n",
    "        if self.summary.empty:\n",
    "            return True\n",
    "        datapoints  = self.summary['datapoint'].unique().tolist()\n",
    "        # if the datapoint is not in the summary, it is stale\n",
    "        if datapoint_path not in datapoints:\n",
    "            return True\n",
    "        \n",
    "        summarized_test_case = self.summary.loc[self.summary['datapoint'] == datapoint_path]['serialized_test_case'].iloc[0]\n",
    "        current_test_case = self.serialize_test_case(datapoint_path)\n",
    "        return summarized_test_case != current_test_case\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds_dir = get_git_root() / \"sample_data\" / \"eval_datasets\"\n",
    "test_dir = eval_ds_dir / \"test_cases\"\n",
    "out_dir = eval_ds_dir / \"out_rag\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = TestSetRun.load(out_dir,test_dir=test_dir)\n",
    "cases = run.find_cases()\n",
    "assert cases == ['pikachus','goldens'],cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/dean/dl/stringdale/sample_data/eval_datasets/out_rag/logs/pikachus.jsonl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.trace_log_path('pikachus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def eval_dataset(Agent:DiagramSchema,test_dir,out_dir,comparisons,default_comparison,force_run=False):\n",
    "\n",
    "    run = TestSetRun.load(out_dir)\n",
    "    run.test_dir = test_dir    \n",
    "    datapoints = run.find_cases()\n",
    "\n",
    "    if not force_run:\n",
    "        stale_datapoints = [p for p in datapoints if run.is_datapoint_stale(p)]\n",
    "    else:\n",
    "        stale_datapoints = datapoints\n",
    "\n",
    "    if len(stale_datapoints) > 0:\n",
    "        logger.info(f\"{run.dir.name}: Evaluating {len(stale_datapoints)}/{len(datapoints)} datapoints\")\n",
    "    else:\n",
    "        logger.info(f\"{run.dir.name}: No stale datapoints, skipping evaluation\")\n",
    "        return run\n",
    "    \n",
    "    datapoint_results = await await_all(\n",
    "        [\n",
    "            evaluate_datapoint(\n",
    "                Agent=Agent,\n",
    "                comparisons=comparisons,\n",
    "                default_comparison=default_comparison,\n",
    "                test_case_path=run.testcase_path(datapoint),\n",
    "                trace_log_path=run.trace_log_path(datapoint),\n",
    "                force_run=True, # since we computed which datapoints to run, we can force run them\n",
    "            ) for datapoint in stale_datapoints\n",
    "        ],\n",
    "        error_prefix=[\n",
    "            f\"When evaluating datapoint {datapoint}\"\n",
    "            for datapoint in stale_datapoints\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    summary_data = list()\n",
    "    deep_dives = list()\n",
    "    debug_infos = dict()\n",
    "\n",
    "    for (alignment,score,debug_info,trace_out),datapoint in zip(datapoint_results,stale_datapoints):\n",
    "        debug_infos[datapoint] = debug_info\n",
    "        deep_dive = summarize_datapoint(datapoint,alignment,debug_info)\n",
    "        deep_dive['datapoint'] = datapoint\n",
    "        deep_dives.append(deep_dive)\n",
    "        summary_data.append({\n",
    "            'datapoint':str(datapoint),\n",
    "            'distance':score,\n",
    "            'avg_distance':deep_dive.distance.mean(),\n",
    "            'coverage':len(alignment) / run.trace_log_len(datapoint),\n",
    "            'alignment':alignment,\n",
    "            'serialized_test_case':run.serialize_test_case(datapoint)\n",
    "            })\n",
    "\n",
    "    \n",
    "    new_summary = pd.DataFrame.from_records(summary_data).reset_index(drop=True)\n",
    "    run.summary = filter_and_concat(run.summary,new_summary,['datapoint'])\n",
    "\n",
    "    details_data = pd.concat(deep_dives,ignore_index=True)\n",
    "    run.details = filter_and_concat(run.details,details_data,['datapoint'])\n",
    "    run.debug = {**run.debug,**debug_infos}\n",
    "    run.save(out_dir)\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.rag import Rag\n",
    "from stringdale.db import ChromaClient\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_agent(conf_dir: Path):\n",
    "    agent_yaml_path = conf_dir / 'agent.yml'\n",
    "    vec_db_yaml_path = conf_dir / 'vec_db.yml'\n",
    "\n",
    "    agent_conf = yaml.safe_load(agent_yaml_path.read_text())\n",
    "    vec_db_conf = yaml.safe_load(vec_db_yaml_path.read_text())\n",
    "\n",
    "    db = ChromaClient(persist_path=tempfile.mkdtemp())\n",
    "    for collection_name, docs in vec_db_conf.items():\n",
    "        db.add_collection(collection_name, exists_ok=True)\n",
    "        db.upsert(collection_name, docs)\n",
    "\n",
    "    agent_conf['db'] = db\n",
    "    \n",
    "    Agent = Rag(**agent_conf)\n",
    "\n",
    "    return Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dir = eval_ds_dir / \"agent_configs\"\n",
    "\n",
    "comparisons = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'chat_eval':chat_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "}\n",
    "\n",
    "default_comparison = 'cosine_dist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - v001: Evaluating 2/2 datapoints\n",
      "__main__ - INFO - Force running pikachus.jsonl\n",
      "__main__ - INFO - Force running goldens.jsonl\n",
      "__main__ - INFO - v002: Evaluating 2/2 datapoints\n",
      "__main__ - INFO - Trace file pikachus.jsonl does not exist, running agent\n",
      "__main__ - INFO - Trace file goldens.jsonl does not exist, running agent\n",
      "__main__ - INFO - v003: Evaluating 2/2 datapoints\n",
      "__main__ - INFO - Trace file pikachus.jsonl does not exist, running agent\n",
      "__main__ - INFO - Trace file goldens.jsonl does not exist, running agent\n"
     ]
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    run1 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v001'),\n",
    "        test_dir=test_dir,\n",
    "        out_dir=out_dir/'v001',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison,\n",
    "        )\n",
    "\n",
    "    run2 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v002'),\n",
    "        test_dir=test_dir,\n",
    "        out_dir=out_dir/'v002',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n",
    "    run3 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v003'),\n",
    "        test_dir=test_dir,\n",
    "        out_dir=out_dir/'v003',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>coverage</th>\n",
       "      <th>alignment</th>\n",
       "      <th>serialized_test_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'0': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0.751374</td>\n",
       "      <td>0.375687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'0': 0, '1': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  distance  avg_distance  coverage         alignment  \\\n",
       "0  pikachus  0.943595      0.943595       0.5          {'0': 1}   \n",
       "1   goldens  0.751374      0.375687       1.0  {'0': 0, '1': 1}   \n",
       "\n",
       "                                serialized_test_case  \n",
       "0  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  \n",
       "1  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.651374</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx   comparison      key  \\\n",
       "0  pikachus          0          1  cosine_dist  content   \n",
       "1   goldens          0          0    chat_eval        .   \n",
       "2   goldens          1          1  cosine_dist  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.651374         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "  aggregation  comp_id  \n",
       "0        None        0  \n",
       "1         min        0  \n",
       "2        None        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.602882</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx   comparison      key  \\\n",
       "0  pikachus          0          1  cosine_dist  content   \n",
       "1   goldens          0          0    chat_eval        .   \n",
       "2   goldens          1          1  cosine_dist  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are not dogs, they are fictional crea...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  Golden Retrievers are friendly, intelligent do...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.818815         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.602882         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "  aggregation  comp_id  \n",
       "0        None        0  \n",
       "1         min        0  \n",
       "2        None        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.443801</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.647368</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx   comparison      key  \\\n",
       "0  pikachus          0          1  cosine_dist  content   \n",
       "1   goldens          0          0    chat_eval        .   \n",
       "2   goldens          1          1  cosine_dist  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are dangerous creatures that smoke to...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.443801         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.647368         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "  aggregation  comp_id  \n",
       "0        None        0  \n",
       "1         min        0  \n",
       "2        None        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run3.details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO pprint\n",
    "# have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from typing import Optional\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Comparison(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    from_run: TestSetRun\n",
    "    to_run: TestSetRun\n",
    "    dir: Optional[Path]\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "\n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"Comparison(\\n\"\n",
    "            f\"  from_run={textwrap.indent(self.from_run.__repr__(), '  ').strip()}, \\n\"\n",
    "            f\"  to_run={textwrap.indent(self.to_run.__repr__(), '  ').strip()}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\")\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def save(self):\n",
    "        if self.dir is None:\n",
    "            return\n",
    "        self.dir.mkdir(parents=True,exist_ok=True)\n",
    "        self.summary.to_csv(self.dir/\"summary.csv\",index=False)\n",
    "        self.details.to_csv(self.dir/\"details.csv\",index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sort_conditions(df):\n",
    "    return df.sort_values(by=['node_idx','comparison','expected'])\n",
    "\n",
    "def limit_to_datapoint(df,datapoint):\n",
    "    return df.loc[df['datapoint'] == datapoint]\n",
    "\n",
    "def get_datapoint(ds,datapoint):\n",
    "    return sort_conditions(limit_to_datapoint(ds.details,datapoint))\n",
    "\n",
    "\n",
    "def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Describe the changes between two datapoints\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the detailed version of the datasets and limit to only rows of the given datapoint\n",
    "    datapoint_df1 = get_datapoint(ds1,datapoint)\n",
    "    datapoint_df2 = get_datapoint(ds2,datapoint)\n",
    "\n",
    "    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)\n",
    "    # lets assert this in the code\n",
    "    assert datapoint_df1.shape == datapoint_df2.shape, f\"Datapoint {datapoint} has different number of rows in the two datasets {ds1} and {ds2}\"\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    # get the first comparison whose node aligned to a different trace\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if row1.trace_idx != row2.trace_idx:\n",
    "            changes.append({\n",
    "                'datapoint':datapoint,\n",
    "                'change_type':'alignment_change',\n",
    "                'before':row1.trace_idx,\n",
    "                'after':row2.trace_idx,\n",
    "                'comparison_id':row1.comp_id,\n",
    "            })\n",
    "            break\n",
    "            \n",
    "\n",
    "    for row1,row2 in zip(datapoint_df1.itertuples(),datapoint_df2.itertuples()):\n",
    "        if math.isclose(row1.distance,row2.distance,abs_tol=epsilon):\n",
    "            continue\n",
    "        if row2.distance + epsilon > row1.distance:\n",
    "            change_types = 'regressed'\n",
    "        elif row2.distance - epsilon < row1.distance:\n",
    "            change_types = 'improved'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        changes.append({\n",
    "            'datapoint':datapoint,\n",
    "            'change_type':change_types,\n",
    "            'value':row1.distance - row2.distance,\n",
    "            'comp_id':row1.comp_id,\n",
    "            'node_label':row1.node_label,\n",
    "            'expected':row1.expected,\n",
    "            'before':row1.actual,\n",
    "            'after':row2.actual,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(changes)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comp_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint change_type     value  comp_id node_label  \\\n",
       "0   goldens    improved  0.048492        1          1   \n",
       "\n",
       "                                     expected  \\\n",
       "0  They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_changes(run1,run2,'goldens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compare_datasets(ds1,ds2,epsilon=1e-3,out_dir=None):\n",
    "    \"\"\"\n",
    "    Compare two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_1 = ds1.summary.sort_values(by='datapoint')\n",
    "    summary_2 = ds2.summary.sort_values(by='datapoint')\n",
    "\n",
    "    changed_datapoints = []\n",
    "    change_summary = []\n",
    "    detailed_changes = []\n",
    "\n",
    "    for row1,row2 in zip(summary_1.itertuples(),summary_2.itertuples()):\n",
    "        datapoint = row1.datapoint\n",
    "        distance_change = not math.isclose(row1.distance,row2.distance,abs_tol=epsilon)\n",
    "        coverage_change = row1.coverage != row2.coverage\n",
    "\n",
    "        if distance_change or coverage_change:\n",
    "            changed_datapoints.append(datapoint)\n",
    "\n",
    "            detailed_change = describe_changes(ds1,ds2,datapoint,epsilon)\n",
    "            detailed_changes.append(detailed_change)\n",
    "\n",
    "            change_types = set(detailed_change['change_type'])\n",
    "            if 'alignment_change' in change_types:\n",
    "                alignment_change = True\n",
    "            else:\n",
    "                alignment_change = False\n",
    "            \n",
    "            if 'improved' in change_types and not 'regressed' in change_types:\n",
    "                score_change = 'improved'\n",
    "            elif 'regressed' in change_types and not 'improved' in change_types:\n",
    "                score_change = 'regressed'\n",
    "            else:\n",
    "                score_change = 'changed'\n",
    "            \n",
    "            total_score_change = row1.distance-row2.distance\n",
    "\n",
    "            change_summary.append({\n",
    "                'datapoint':datapoint,\n",
    "                'alignment_change':alignment_change,\n",
    "                'score_change_type':score_change,\n",
    "                'total_score_change':total_score_change,\n",
    "            })\n",
    "\n",
    "    changes_summary = pd.DataFrame(change_summary)\n",
    "    if len(detailed_changes) > 0:\n",
    "        detailed_changes = pd.concat(detailed_changes)\n",
    "    else:\n",
    "        detailed_changes = pd.DataFrame()\n",
    "\n",
    "    comp =  Comparison(\n",
    "        from_run=ds1,\n",
    "        to_run=ds2,\n",
    "        summary=changes_summary,\n",
    "        details=detailed_changes,\n",
    "        dir=out_dir,\n",
    "    )\n",
    "    comp.save()\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Comparison(\n",
       "  from_run=TestSetRun(\n",
       "    test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "    dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/out_rag/v001, \n",
       "    summary=Dataframe((2, 6)), \n",
       "    details=Dataframe((3, 14)), \n",
       "    debug=dict), \n",
       "  to_run=TestSetRun(\n",
       "    test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "    dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/out_rag/v002, \n",
       "    summary=Dataframe((2, 6)), \n",
       "    details=Dataframe((3, 14)), \n",
       "    debug=dict), \n",
       "  summary=Dataframe((2, 4)), \n",
       "  details=Dataframe((2, 8)), \n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2 = compare_datasets(run1,run2)\n",
    "comparison_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.048492\n",
       "1  pikachus             False          improved            0.124780"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comp_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint change_type     value  comp_id node_label  \\\n",
       "0   goldens    improved  0.048492        1          1   \n",
       "0  pikachus    improved  0.124780        0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0      They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  \n",
       "0  Pikachus are not dogs, they are fictional crea...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.499794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.004006\n",
       "1  pikachus             False          improved            0.499794"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_3 = compare_datasets(run1,run3)\n",
    "comparison_3.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comp_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.499794</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint change_type     value  comp_id node_label  \\\n",
       "0   goldens    improved  0.004006        1          1   \n",
       "0  pikachus    improved  0.499794        0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0      They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  The Golden Retriever is a friendly, intelligen...  \n",
       "0  Pikachus are dangerous creatures that smoke to...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_3.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestSetRun(\n",
       "  test_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/test_cases, \n",
       "  dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/out_rag/v001, \n",
       "  summary=Dataframe((2, 6)), \n",
       "  details=Dataframe((3, 14)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then continue to make the pprint of a change class with topk changes of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO when pprinting comparisons, give urls to files, so that its easy in nb to navigate to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval main entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable,Dict,List,Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "EVAL_COMPARISONS = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'chat_eval':chat_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "}\n",
    "\n",
    "EVAL_DEFAULT_COMPARISON = 'cosine_dist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EvalResult(BaseModel):\n",
    "    \"\"\"\n",
    "    A class to track evaluation results, including individual runs and comparisons between runs.\n",
    "    \n",
    "    Attributes:\n",
    "        runs (Dict[str, TestSetRun]): Dictionary mapping agent names to their test run results\n",
    "        comparisons (Dict[Tuple[str, str], Comparison]): Dictionary mapping pairs of agent names \n",
    "            (base_run, other_run) to their comparison results\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    \n",
    "    runs: Dict[str, TestSetRun]\n",
    "    comparisons: Dict[Tuple[str, str], Comparison]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        runs_str = f\"runs: {list(self.runs.keys())}\"\n",
    "        comparisons_str = f\"comparisons: {list(self.comparisons.keys())}\"\n",
    "        return f\"EvalResult(\\n  {runs_str},\\n  {comparisons_str}\\n)\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def eval(\n",
    "  test_dir:Path,\n",
    "  out_dir:Path,\n",
    "  agents:List[Tuple[str,DiagramSchema]],\n",
    "  k:Optional[int]=5,\n",
    "  force_run:bool=False,\n",
    "  silent:bool=False,\n",
    "  comparisons: Optional[Dict[str,Callable]]=None,\n",
    "  default_comparison:Optional[Callable]=None,\n",
    "  ):\n",
    "  \"\"\"\n",
    "  The main eval function.\n",
    "  Evaluates a set of agents on a set of tests.\n",
    "  Compares the results of all agents to the first agent.\n",
    "  pprints a summary of the results to the console.\n",
    "  and saves all files to the out_dir.\n",
    "\n",
    "  Args:\n",
    "    tests_dir: Path to the directory containing the tests.\n",
    "    out_dir: Path to the directory to write the results to.\n",
    "    agents: A list of tuples of agent names and their DiagramSchema.\n",
    "    k: The number of datapoints to print to the summary at most. Defaults to 5.\n",
    "    force_run: If True, deletes out dir content and reruns the agents. \n",
    "      If False, we skip the agents that have already been run.\n",
    "      Defaults to False.\n",
    "    silent: If True, dont pprint comparisons. Defaults to False.\n",
    "    comparisons: A dictionary of comparison names and their functions, to add to the allowed comparisons.\n",
    "      Defaults to None.\n",
    "    default_comparison: The default comparison function to use if no comparison is specified.\n",
    "      This is used to compare the first agent to the rest of the agents.\n",
    "      Defaults to stringdale.eval.cosine_dist\n",
    "  \"\"\"\n",
    "\n",
    "  global EVAL_COMPARISONS\n",
    "  global EVAL_DEFAULT_COMPARISON\n",
    "  if comparisons is not None:\n",
    "    comparisons = EVAL_COMPARISONS | comparisons\n",
    "  else: \n",
    "    comparisons = EVAL_COMPARISONS\n",
    "  if default_comparison is None:\n",
    "    default_comparison = EVAL_DEFAULT_COMPARISON\n",
    "\n",
    "  eval_dataset_tasks = []\n",
    "  for agent_name,agent_schema in agents:\n",
    "    log_dir = out_dir / 'logs'/ agent_name\n",
    "    eval_dataset_tasks.append(eval_dataset(\n",
    "      Agent=agent_schema,\n",
    "      test_dir=test_dir,\n",
    "      out_dir=log_dir,\n",
    "      comparisons=comparisons,\n",
    "      default_comparison=default_comparison))\n",
    "\n",
    "  datasets = await asyncio.gather(*eval_dataset_tasks,return_exceptions=True)\n",
    "\n",
    "  for result,(agent_name,_) in zip(datasets,agents):\n",
    "    if isinstance(result,Exception):\n",
    "      result.args = (f\"When evaluating agent {agent_name}:\\n{result.args[0]}\", )+ result.args[1:]\n",
    "      raise result\n",
    "\n",
    "  first_dataset = datasets[0]\n",
    "  comparisons = dict()\n",
    "  for dataset in datasets[1:]:\n",
    "    comp_dir = out_dir / 'comparisons' / f'{first_dataset.dir.name}_{dataset.dir.name}'\n",
    "    comp = compare_datasets(first_dataset,dataset,out_dir=comp_dir)\n",
    "    comparisons[(first_dataset.dir.name,dataset.dir.name)] = comp\n",
    "  \n",
    "  res = EvalResult(\n",
    "    runs={agent_name:dataset for (agent_name,_),dataset in zip(agents,datasets)},\n",
    "    comparisons=comparisons)\n",
    "\n",
    "\n",
    "  return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('v001', <stringdale.base.DiagramSchema>),\n",
       " ('v002', <stringdale.base.DiagramSchema>),\n",
       " ('v003', <stringdale.base.DiagramSchema>)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = [\n",
    "    (version,load_agent(conf_dir/version))\n",
    "    for version in ['v001','v002','v003']\n",
    "]\n",
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"test_cases\"\n",
    "eval_out = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"eval_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await eval(\n",
    "    test_dir=test_dir,\n",
    "    out_dir=eval_out,\n",
    "    agents=agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalResult(\n",
       "  runs: ['v001', 'v002', 'v003'],\n",
       "  comparisons: [('v001', 'v002'), ('v001', 'v003')]\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>coverage</th>\n",
       "      <th>alignment</th>\n",
       "      <th>serialized_test_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'0': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0.751374</td>\n",
       "      <td>0.375687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'0': 0, '1': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  distance  avg_distance  coverage         alignment  \\\n",
       "0  pikachus  0.943595      0.943595       0.5          {'0': 1}   \n",
       "1   goldens  0.751374      0.375687       1.0  {'0': 0, '1': 1}   \n",
       "\n",
       "                                serialized_test_case  \n",
       "0  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  \n",
       "1  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.runs['v001'].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>node_label</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>comparison</th>\n",
       "      <th>key</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>node_idx</th>\n",
       "      <th>trace_name</th>\n",
       "      <th>node_name</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>aggregation</th>\n",
       "      <th>comp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>chat_eval</td>\n",
       "      <td>.</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>{'system_prompt': 'how close are these 2 docum...</td>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goldens</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>content</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>0.651374</td>\n",
       "      <td>1</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint node_label  trace_idx   comparison      key  \\\n",
       "0  pikachus          0          1  cosine_dist  content   \n",
       "1   goldens          0          0    chat_eval        .   \n",
       "2   goldens          1          1  cosine_dist  content   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                            expected  distance  node_idx  \\\n",
       "0     They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595         0   \n",
       "1  The Golden Retriever is a friendly, intelligen...  0.100000         0   \n",
       "2         They are:\\n  * friendly\\n  * intelligent\\n  0.651374         1   \n",
       "\n",
       "  trace_name node_name                                             kwargs  \\\n",
       "0       chat      chat                                                 {}   \n",
       "1   get_docs  get_docs  {'system_prompt': 'how close are these 2 docum...   \n",
       "2       chat      chat                                                 {}   \n",
       "\n",
       "  aggregation  comp_id  \n",
       "0        None        0  \n",
       "1         min        0  \n",
       "2        None        1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.runs['v001'].details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>alignment_change</th>\n",
       "      <th>score_change_type</th>\n",
       "      <th>total_score_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>False</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  alignment_change score_change_type  total_score_change\n",
       "0   goldens             False          improved            0.048492\n",
       "1  pikachus             False          improved            0.124780"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.comparisons[('v001','v002')].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>change_type</th>\n",
       "      <th>value</th>\n",
       "      <th>comp_id</th>\n",
       "      <th>node_label</th>\n",
       "      <th>expected</th>\n",
       "      <th>before</th>\n",
       "      <th>after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goldens</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.048492</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent\\n</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>improved</td>\n",
       "      <td>0.124780</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint change_type     value  comp_id node_label  \\\n",
       "0   goldens    improved  0.048492        1          1   \n",
       "0  pikachus    improved  0.124780        0          0   \n",
       "\n",
       "                                         expected  \\\n",
       "0      They are:\\n  * friendly\\n  * intelligent\\n   \n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes   \n",
       "\n",
       "                                              before  \\\n",
       "0  The Golden Retriever is a friendly, intelligen...   \n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "\n",
       "                                               after  \n",
       "0  Golden Retrievers are friendly, intelligent do...  \n",
       "0  Pikachus are not dogs, they are fictional crea...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.comparisons[('v001','v002')].details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>distance</th>\n",
       "      <th>avg_distance</th>\n",
       "      <th>coverage</th>\n",
       "      <th>alignment</th>\n",
       "      <th>serialized_test_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'0': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goldens</td>\n",
       "      <td>0.751374</td>\n",
       "      <td>0.375687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'0': 0, '1': 1}</td>\n",
       "      <td>{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint  distance  avg_distance  coverage         alignment  \\\n",
       "0  pikachus  0.943595      0.943595       0.5          {'0': 1}   \n",
       "1   goldens  0.751374      0.375687       1.0  {'0': 0, '1': 1}   \n",
       "\n",
       "                                serialized_test_case  \n",
       "0  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  \n",
       "1  {\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.runs['v001'].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import rich\n",
    "from rich.padding import Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rprint(obj,indent:int=0,sep_by:int=2):\n",
    "    rich.print(Padding(obj,pad=(0,0,0,indent*sep_by)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_run_summary(res:EvalResult,run_name:str,topk:int=5):\n",
    "    pass\n",
    "    # pprint name of the run, average distance and coverage\n",
    "\n",
    "def pprint_comparison_summary(res:EvalResult,comparison_name:Tuple[str,str],topk:int=5):\n",
    "    pass\n",
    "    # number of datapoints whose alignment changed (write names of topk)\n",
    "    # number of datapoints whose alignment improved (write names of topk)\n",
    "    # number of datapoints whose alignment regressed (write names of topk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_datapoint(res:EvalResult,datapoint:str,default_comparison=\"cosine_dist\"):\n",
    "    \n",
    "    base_name = list(res.comparisons.keys())[0][0]\n",
    "    jinja_params = {\n",
    "        'base_name':base_name,\n",
    "        'datapoint':datapoint,\n",
    "        'run_summaries':{},\n",
    "        'comp_summaries':{},\n",
    "        'per_comp':[],\n",
    "\n",
    "        'version_style':'purple',\n",
    "        'param_style':'cyan bold',\n",
    "        'comp_config_style':'green bold',\n",
    "        'output_style':'#CE9178',\n",
    "    }\n",
    "\n",
    "\n",
    "    for run_name,run in res.runs.items():\n",
    "        jinja_params['run_summaries'][run_name] = dict(run.summary.set_index('datapoint').loc[datapoint])\n",
    "    \n",
    "    for (base_run,other_run),comparison in res.comparisons.items():\n",
    "        jinja_params['comp_summaries'][base_run,other_run] = comparison.summary.set_index('datapoint').loc[datapoint].to_dict()\n",
    "    \n",
    "    details = {\n",
    "        run_name:run.details[run.details['datapoint'] == datapoint]\n",
    "        for run_name,run in res.runs.items()\n",
    "    }\n",
    "    comp_details = {\n",
    "        (base_run,other_run):comparison.details[comparison.details['datapoint'] == datapoint]\n",
    "        for (base_run,other_run),comparison in res.comparisons.items()\n",
    "    }\n",
    "    comparison_ids = pd.concat(comparison['comp_id'] for comparison in comp_details.values()).unique()\n",
    "\n",
    "    per_comp = {}\n",
    "\n",
    "    for comparison_id in comparison_ids:\n",
    "        per_comp[comparison_id] = {}\n",
    "        for run_name,run in details.items():\n",
    "            run_det = run[run['comp_id'] == comparison_id].iloc[0].to_dict()\n",
    "            if run_det['kwargs'] == '{}':\n",
    "                run_det['kwargs'] = None\n",
    "            \n",
    "            if not run_name == base_name:\n",
    "                sub_comp_details = comp_details[base_name,run_name]\n",
    "                sub_comp_details = sub_comp_details[sub_comp_details['comp_id'] == comparison_id]\n",
    "                run_det['change_type'] = sub_comp_details['change_type'].values[0]\n",
    "                run_det['change_value'] = sub_comp_details['value'].values[0]\n",
    "            per_comp[comparison_id][run_name] = run_det\n",
    "        \n",
    "    jinja_params['per_comp'] = per_comp\n",
    "    return jinja_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import jinja_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_name': 'v001',\n",
       " 'datapoint': 'pikachus',\n",
       " 'run_summaries': {'v001': {'distance': np.float64(0.9435950697036162),\n",
       "   'avg_distance': np.float64(0.9435950697036162),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': frozendict.frozendict({'0': 1}),\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'},\n",
       "  'v002': {'distance': np.float64(0.818815274469515),\n",
       "   'avg_distance': np.float64(0.818815274469515),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': frozendict.frozendict({'0': 1}),\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'},\n",
       "  'v003': {'distance': np.float64(0.4438011800616313),\n",
       "   'avg_distance': np.float64(0.4438011800616313),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': frozendict.frozendict({'0': 1}),\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'}},\n",
       " 'comp_summaries': {('v001', 'v002'): {'alignment_change': False,\n",
       "   'score_change_type': 'improved',\n",
       "   'total_score_change': 0.12477979523410121},\n",
       "  ('v001', 'v003'): {'alignment_change': False,\n",
       "   'score_change_type': 'improved',\n",
       "   'total_score_change': 0.4997938896419849}},\n",
       " 'per_comp': {np.int64(0): {'v001': {'datapoint': 'pikachus',\n",
       "    'node_label': '0',\n",
       "    'trace_idx': 1,\n",
       "    'comparison': 'cosine_dist',\n",
       "    'key': 'content',\n",
       "    'actual': \"I'm sorry, but I can only provide information about dogs. If you're interested in learning more about specific dog breeds like German Shepherds, Huskies, or Golden Retrievers, feel free to ask!\",\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.9435950697036162,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': {},\n",
       "    'aggregation': None,\n",
       "    'comp_id': 0},\n",
       "   'v002': {'datapoint': 'pikachus',\n",
       "    'node_label': '0',\n",
       "    'trace_idx': 1,\n",
       "    'comparison': 'cosine_dist',\n",
       "    'key': 'content',\n",
       "    'actual': 'Pikachus are not dogs, they are fictional creatures from the Pokémon franchise. They are known for their yellow color, pointy ears, and ability to generate electricity.',\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.818815274469515,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': {},\n",
       "    'aggregation': None,\n",
       "    'comp_id': 0,\n",
       "    'change_type': 'improved',\n",
       "    'change_value': np.float64(0.12477979523410121)},\n",
       "   'v003': {'datapoint': 'pikachus',\n",
       "    'node_label': '0',\n",
       "    'trace_idx': 1,\n",
       "    'comparison': 'cosine_dist',\n",
       "    'key': 'content',\n",
       "    'actual': 'Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.',\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.4438011800616313,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': {},\n",
       "    'aggregation': None,\n",
       "    'comp_id': 0,\n",
       "    'change_type': 'improved',\n",
       "    'change_value': np.float64(0.4997938896419849)}}},\n",
       " 'version_style': 'purple',\n",
       " 'param_style': 'cyan bold',\n",
       " 'comp_config_style': 'green bold',\n",
       " 'output_style': '#CE9178'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jinja_params = pprint_datapoint(res,'pikachus')\n",
    "jinja_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "datapoint_template=\"\"\"\n",
    "{{datapoint}}\n",
    "  summary:\n",
    "    {%- for run_name,run in run_summaries.items() %}\n",
    "      [{{version_style}}]{{run_name}}[/{{version_style}}] - Dist: {{\"$%.2f\"|format(run.distance)}} AvgDist: {{\"$%.2f\"|format(run.avg_distance)}} Coverage: {{ \"$%.2f\"|format(run.coverage)}}\n",
    "    {%- endfor %}\n",
    "    {%- for (base_run,other_run),comparison in comp_summaries.items() %}\n",
    "      [{{version_style}}]{{base_run}}[/{{version_style}}] vs [{{version_style}}]{{other_run}}[/{{version_style}}]: \\\n",
    "Alignment change: [{{param_style}}]{{comparison.alignment_change}}[/{{param_style}}] \\\n",
    "Score change: [{{param_style}}]{{comparison.score_change_type}}[/{{param_style}}] \\\n",
    "Score by: {{ \"%.2f\"|format(comparison.total_score_change)}}\n",
    "    {%- endfor%}\n",
    "  details:\n",
    "    {%- for comp_id,comp in per_comp.items() %}\n",
    "    Comparison #[{{comp_config_style}}]{{comp_id}}[/{{comp_config_style}}], node_pattern: [{{comp_config_style}}]{{comp[base_name].node_name}}[/{{comp_config_style}}], key: [{{comp_config_style}}]{{comp[base_name].key}}[/{{comp_config_style}}], func: [{{comp_config_style}}]{{comp[base_name].comparison}}[/{{comp_config_style}}]\n",
    "      {% if comp[base_name].kwargs -%}\n",
    "        kwargs: [{{output_style}}]{{comp[base_name].kwargs}}[/{{output_style}}]\n",
    "      {%- endif -%}\n",
    "      expected: \n",
    "[{{output_style}}]{{comp[base_name].expected | wordwrap(width=100) | indent(10,true) }}[/{{output_style}}]  \n",
    "      {% for run_name,run in comp.items() %}\n",
    "      {{run_name}} - matched [green]{{run.trace_name}}[/green](#{{run.trace_idx}})\n",
    "      {%- if run.alignment_change %}\n",
    "          , Alignment change: {{run.alignment_change}}\n",
    "      {%- endif -%}\n",
    "      {%- if run.change_type -%}\n",
    "          , {{run.change_type}}: {{ \"%.2f\"|format(run.change_value) }}\n",
    "      {%- endif -%}:\n",
    "[{{output_style}}]{{run.actual | wordwrap(width=100) | indent(10,true)}}[/{{output_style}}]\n",
    "      {% endfor -%}\n",
    "    {% endfor %}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n",
       "pikachus                                                                                                           \n",
       "  summary:                                                                                                         \n",
       "      <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v001</span> - Dist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.94</span> AvgDist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.94</span> Coverage: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>                                                            \n",
       "      <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v002</span> - Dist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.82</span> AvgDist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.82</span> Coverage: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>                                                            \n",
       "      <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v003</span> - Dist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.44</span> AvgDist: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.44</span> Coverage: $<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>                                                            \n",
       "      <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v001</span> vs <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v002</span>: Alignment change: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">False</span> Score change: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">improved</span> Score by: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span>                                  \n",
       "      <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v001</span> vs <span style=\"color: #af00ff; text-decoration-color: #af00ff\">v003</span>: Alignment change: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold; font-style: italic\">False</span> Score change: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">improved</span> Score by: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>                                  \n",
       "  details:                                                                                                         \n",
       "    Comparison #<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span>, node_pattern: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">chat</span>, key: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">content</span>, func: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">cosine_dist</span>                                             \n",
       "      expected:                                                                                                    \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          They are:</span>                                                                                                \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">            * dangerous</span>                                                                                            \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">            * smoke cigarettes</span>                                                                                     \n",
       "                                                                                                                   \n",
       "      v001 - matched <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">chat</span><span style=\"font-weight: bold\">(</span>#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>:                                                                                     \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          I'm sorry, but I can only provide information about dogs. If you're interested in learning more</span>          \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          about specific dog breeds like German Shepherds, Huskies, or Golden Retrievers, feel free to ask!</span>        \n",
       "                                                                                                                   \n",
       "      v002 - matched <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">chat</span><span style=\"font-weight: bold\">(</span>#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>, improved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span>:                                                                     \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          Pikachus are not dogs, they are fictional creatures from the Pokémon franchise. They are known for</span>       \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          their yellow color, pointy ears, and ability to generate electricity.</span>                                    \n",
       "                                                                                                                   \n",
       "      v003 - matched <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">chat</span><span style=\"font-weight: bold\">(</span>#<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>, improved: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.50</span>:                                                                     \n",
       "<span style=\"color: #ce9178; text-decoration-color: #ce9178\">          Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.</span>                       \n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                                   \n",
       "pikachus                                                                                                           \n",
       "  summary:                                                                                                         \n",
       "      \u001b[38;5;129mv001\u001b[0m - Dist: $\u001b[1;36m0.94\u001b[0m AvgDist: $\u001b[1;36m0.94\u001b[0m Coverage: $\u001b[1;36m0.50\u001b[0m                                                            \n",
       "      \u001b[38;5;129mv002\u001b[0m - Dist: $\u001b[1;36m0.82\u001b[0m AvgDist: $\u001b[1;36m0.82\u001b[0m Coverage: $\u001b[1;36m0.50\u001b[0m                                                            \n",
       "      \u001b[38;5;129mv003\u001b[0m - Dist: $\u001b[1;36m0.44\u001b[0m AvgDist: $\u001b[1;36m0.44\u001b[0m Coverage: $\u001b[1;36m0.50\u001b[0m                                                            \n",
       "      \u001b[38;5;129mv001\u001b[0m vs \u001b[38;5;129mv002\u001b[0m: Alignment change: \u001b[1;3;36mFalse\u001b[0m Score change: \u001b[1;36mimproved\u001b[0m Score by: \u001b[1;36m0.12\u001b[0m                                  \n",
       "      \u001b[38;5;129mv001\u001b[0m vs \u001b[38;5;129mv003\u001b[0m: Alignment change: \u001b[1;3;36mFalse\u001b[0m Score change: \u001b[1;36mimproved\u001b[0m Score by: \u001b[1;36m0.50\u001b[0m                                  \n",
       "  details:                                                                                                         \n",
       "    Comparison #\u001b[1;32m0\u001b[0m, node_pattern: \u001b[1;32mchat\u001b[0m, key: \u001b[1;32mcontent\u001b[0m, func: \u001b[1;32mcosine_dist\u001b[0m                                             \n",
       "      expected:                                                                                                    \n",
       "\u001b[38;2;206;145;120m          They are:\u001b[0m                                                                                                \n",
       "\u001b[38;2;206;145;120m            * dangerous\u001b[0m                                                                                            \n",
       "\u001b[38;2;206;145;120m            * smoke cigarettes\u001b[0m                                                                                     \n",
       "                                                                                                                   \n",
       "      v001 - matched \u001b[1;32mchat\u001b[0m\u001b[1m(\u001b[0m#\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m:                                                                                     \n",
       "\u001b[38;2;206;145;120m          I'm sorry, but I can only provide information about dogs. If you're interested in learning more\u001b[0m          \n",
       "\u001b[38;2;206;145;120m          about specific dog breeds like German Shepherds, Huskies, or Golden Retrievers, feel free to ask!\u001b[0m        \n",
       "                                                                                                                   \n",
       "      v002 - matched \u001b[1;32mchat\u001b[0m\u001b[1m(\u001b[0m#\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, improved: \u001b[1;36m0.12\u001b[0m:                                                                     \n",
       "\u001b[38;2;206;145;120m          Pikachus are not dogs, they are fictional creatures from the Pokémon franchise. They are known for\u001b[0m       \n",
       "\u001b[38;2;206;145;120m          their yellow color, pointy ears, and ability to generate electricity.\u001b[0m                                    \n",
       "                                                                                                                   \n",
       "      v003 - matched \u001b[1;32mchat\u001b[0m\u001b[1m(\u001b[0m#\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m, improved: \u001b[1;36m0.50\u001b[0m:                                                                     \n",
       "\u001b[38;2;206;145;120m          Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.\u001b[0m                       \n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with rich.get_console():\n",
    "    rprint(jinja_render(datapoint_template,jinja_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here, turn that into a single function that generates the string.\n",
    "# then make the templates for the summaries of whole runs and whole comparisons.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_name': 'v001',\n",
       " 'datapoint': 'pikachus',\n",
       " 'run_summaries': {'v001': {'distance': np.float64(0.9435950697036162),\n",
       "   'avg_distance': np.float64(0.9435950697036162),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': \"frozendict.frozendict({'0': 1})\",\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'},\n",
       "  'v002': {'distance': np.float64(0.818815274469515),\n",
       "   'avg_distance': np.float64(0.818815274469515),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': \"frozendict.frozendict({'0': 1})\",\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'},\n",
       "  'v003': {'distance': np.float64(0.4438011800616313),\n",
       "   'avg_distance': np.float64(0.4438011800616313),\n",
       "   'coverage': np.float64(0.5),\n",
       "   'alignment': \"frozendict.frozendict({'0': 1})\",\n",
       "   'serialized_test_case': '{\\n  \"inputs\": [\\n    {\\n      \"query\": \"tell me about pikachus\"\\n    }\\n  ],\\n  \"test_nodes\": [\\n    {\\n      \"name\": \"chat\",\\n      \"conditions\": [\\n        {\\n          \"key\": \"content\",\\n          \"value\": \"They are:\\\\n  * dangerous\\\\n  * smoke cigarettes\"\\n        }\\n      ]\\n    }\\n  ]\\n}'}},\n",
       " 'comp_summaries': {('v001', 'v002'): {'alignment_change': False,\n",
       "   'score_change_type': 'improved',\n",
       "   'total_score_change': 0.12477979523410121},\n",
       "  ('v001', 'v003'): {'alignment_change': False,\n",
       "   'score_change_type': 'improved',\n",
       "   'total_score_change': 0.4997938896419849}},\n",
       " 'per_comp': {np.int64(0): {'v001': {'datapoint': 'pikachus',\n",
       "    'node_label': 0,\n",
       "    'trace_idx': 1,\n",
       "    'comparison': nan,\n",
       "    'key': 'content',\n",
       "    'actual': \"I'm sorry, but I can only provide information about dogs. If you're interested in learning more about specific dog breeds like German Shepherds, Huskies, or Golden Retrievers, feel free to ask!\",\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.9435950697036162,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': None,\n",
       "    'aggregation': nan,\n",
       "    'comp_id': 0},\n",
       "   'v002': {'datapoint': 'pikachus',\n",
       "    'node_label': 0,\n",
       "    'trace_idx': 1,\n",
       "    'comparison': nan,\n",
       "    'key': 'content',\n",
       "    'actual': 'Pikachus are not dogs, they are fictional creatures from the Pokémon franchise. They are known for their yellow color, pointy ears, and ability to generate electricity.',\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.818815274469515,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': None,\n",
       "    'aggregation': nan,\n",
       "    'comp_id': 0,\n",
       "    'change_type': 'improved',\n",
       "    'change_value': np.float64(0.12477979523410121)},\n",
       "   'v003': {'datapoint': 'pikachus',\n",
       "    'node_label': 0,\n",
       "    'trace_idx': 1,\n",
       "    'comparison': nan,\n",
       "    'key': 'content',\n",
       "    'actual': 'Pikachus are dangerous creatures that smoke tons of cigarettes and scare children.',\n",
       "    'expected': 'They are:\\n  * dangerous\\n  * smoke cigarettes',\n",
       "    'distance': 0.4438011800616313,\n",
       "    'node_idx': 0,\n",
       "    'trace_name': 'chat',\n",
       "    'node_name': 'chat',\n",
       "    'kwargs': None,\n",
       "    'aggregation': nan,\n",
       "    'comp_id': 0,\n",
       "    'change_type': 'improved',\n",
       "    'change_value': np.float64(0.4997938896419849)}}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_datapoint(res,'pikachus') # TODO from here, make a ppprint function that does the indentations and stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">This text is not indented.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "This text is not indented.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    This text is indented by <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> spaces on the left.                                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "    This text is indented by \u001b[1;36m4\u001b[0m spaces on the left.                                                                 \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n",
       "    This text has padding on all sides.                                                                            \n",
       "                                                                                                                   \n",
       "                                                                                                                   \n",
       "                                                                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                                   \n",
       "    This text has padding on all sides.                                                                            \n",
       "                                                                                                                   \n",
       "                                                                                                                   \n",
       "                                                                                                                   \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.console import Console\n",
    "from rich.padding import Padding\n",
    "\n",
    "console = Console()\n",
    "\n",
    "console.print(\"This text is not indented.\")\n",
    "console.print(Padding(\"This text is indented by 4 spaces on the left.\", pad=(0, 0, 0, 4)))\n",
    "console.print(Padding(\"This text has padding on all sides.\", pad=(1, 2, 3, 4))) # top, right, bottom, left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_eval(res:EvalResult,topk:int=5):\n",
    "    pass\n",
    "    # print summary of each run seperately?\n",
    "    # then, we pprint the summary of each comparison seperately\n",
    "\n",
    "    # then we group the per datapoint comps across all dataset comparisons by the datapoint id\n",
    "    # and for each datapoint we pprint a combined datapoint comparison.\n",
    "    # combined datapoints for each datapoint, the total metrics of each version\n",
    "    # and then for each comparison that is different from baseline, say how it is different for every version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO patch the EvalResult class to be able to pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# in the end, we want few entrypoints:\n",
    "\n",
    "# eval, eval_single, and align_trace, validate_tests (just to make sure they are formatted correctly)\n",
    "\n",
    "# eval will get lists of versions, and which comparisons to do, and log dir to save results to etc..\n",
    "\n",
    "\n",
    "# TODO add to the tutorial a performance section. Explain the lazy eval of distances\n",
    "# TODO add to \"V\" ability to specifiy the funcname for presenting in the drawing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain in tutorial\n",
    "\n",
    "# how do we do training and validation on workflows?\n",
    "\n",
    "# we have 2 expected datasets, train and test\n",
    "\n",
    "# we look at the total distance of the validation set to see that we are improving on it\n",
    "\n",
    "# but we only look at the comparisons and fix our configs or diagrams based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Optional semaphore\\nfrom contextlib import asynccontextmanager\\nfrom typing import Optional\\n\\n@asynccontextmanager\\nasync def optional_semaphore(semaphore: Optional[asyncio.Semaphore] = None):\\n    if semaphore is not None:\\n        async with semaphore:\\n            yield\\n    else:\\n        yield\\n\\n# Usage example:\\nasync def my_function(limit_concurrency: bool = False):\\n    sem = asyncio.Semaphore(2) if limit_concurrency else None\\n\\n    async with optional_semaphore(sem):\\n        # Your async code here\\n        await asyncio.sleep(1)\\n        print(\"Function executed\")\\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO add asyncio sempathores to Chat and DB operations etc, so that we dont get rate limited due to tons of async requests\n",
    "\n",
    "\"\"\" # Optional semaphore\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import Optional\n",
    "\n",
    "@asynccontextmanager\n",
    "async def optional_semaphore(semaphore: Optional[asyncio.Semaphore] = None):\n",
    "    if semaphore is not None:\n",
    "        async with semaphore:\n",
    "            yield\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "# Usage example:\n",
    "async def my_function(limit_concurrency: bool = False):\n",
    "    sem = asyncio.Semaphore(2) if limit_concurrency else None\n",
    "    \n",
    "    async with optional_semaphore(sem):\n",
    "        # Your async code here\n",
    "        await asyncio.sleep(1)\n",
    "        print(\"Function executed\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
