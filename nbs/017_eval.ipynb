{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData model\\n\\nWe have a dataset\\n* containing tests\\n* each test has the input to the agent\\n* and the expected output\\n* test is any object that can be serialized to json\\n* expected output is a partial trace spec\\n\\n* partial trace spec is a list of steps\\n* each step has a name is a dict with accessors and value are how to check them\\n* names are the node name we expect to see in the trace\\n* the dict defines what we expect the value to look like\\n\\n\\nWhen we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\\nsince the partial trace spec does not \\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data model\n",
    "\n",
    "We have a dataset\n",
    "* containing tests\n",
    "* each test has the input to the agent\n",
    "* and the expected output\n",
    "* test is any object that can be serialized to json\n",
    "* expected output is a partial trace spec\n",
    "\n",
    "* partial trace spec is a list of steps\n",
    "* each step has a name is a dict with accessors and value are how to check them\n",
    "* names are the node name we expect to see in the trace\n",
    "* the dict defines what we expect the value to look like\n",
    "\n",
    "\n",
    "When we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\n",
    "since the partial trace spec does not \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedTrace:\n",
    "    pass\n",
    "\n",
    "class DataPointRun:\n",
    "    # basically a list of traces, agent input and agent output\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def dynamic_partial_time_warping(trace, expected,comparisons):\n",
    "    pass\n",
    "    # this is a parallel dynamic time warping with an abstract distance metric (as opposed to a euclidean embedding)\n",
    "\n",
    "\n",
    "\n",
    "    # once we have the distance matrix,\n",
    "    #   we need to somehow get the time breaks (ie clustering the steps into groups)\n",
    "\n",
    "    # then we need to do alignment to compute the warping path\n",
    "\n",
    "    # given a set of events we need to figure out how to encode different matches of the set to traces.\n",
    "\n",
    "\n",
    "    # https://www.geeksforgeeks.org/dynamic-time-warping-dtw-in-time-series/\n",
    "    \n",
    "\n",
    "    # TODO idea, instead of sets of parrallel nodes, we could get a partial ordering of the nodes\n",
    "    # each time we try to match a node, we can only do so on the set of nodes that do not have an unmatched predecessor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traces_from_file(file_path):\n",
    "    pass\n",
    "\n",
    "def collect_traces_from_logg_aggregator(logger):\n",
    "    pass\n",
    "\n",
    "def run_dataset(agent,dataset,output_dir):\n",
    "    # for each data point in the dataset\n",
    "    # run the agent\n",
    "    # collect the traces into a file\n",
    "    # return the file path\n",
    "    pass\n",
    "\n",
    "def write_comparison_to_file(dataset_run,expected_traces,output_dir):\n",
    "    # run the comparison and write the results to a file\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_summary(runs,dir):\n",
    "    # get the run files and the comparison files\n",
    "    # get the total metrics per expected node and total\n",
    "    # make them into a dataframe\n",
    "    pass\n",
    "\n",
    "def plot_runs(runs,dir):\n",
    "    # call runs_summary\n",
    "    # plot the results\n",
    "    pass\n",
    "\n",
    "def check_regressions(runs,dir):\n",
    "    # get two runs\n",
    "    # for each input, if the second run is worse than the first, then flag it\n",
    "    # make a dataframe of the regressions on a whole run basis\n",
    "    \n",
    "    # also make a dataframe of the regressions on a per node basis for the runs that regressed.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Order Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import TypeVar, Set, List, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "T = TypeVar('T')\n",
    "\n",
    "class PartialOrder:\n",
    "    def __init__(self):\n",
    "        # Store the direct less-than relations\n",
    "        self.direct_less_than: dict[T, set[T]] = defaultdict(set)\n",
    "        # Store the transitive closure of less-than relations\n",
    "        self.less_than: dict[T, set[T]] = defaultdict(set)\n",
    "        \n",
    "    def add_relation(self, smaller: T, larger: T) -> None:\n",
    "        \"\"\"Add a relation where 'smaller' is less than 'larger'\"\"\"\n",
    "        # Add direct relation\n",
    "        self.direct_less_than[smaller].add(larger)\n",
    "        \n",
    "        # Update transitive closure\n",
    "        # 1. Add direct relation\n",
    "        self.less_than[smaller].add(larger)\n",
    "        \n",
    "        # 2. Add all relations where 'smaller' is less than something that's less than 'larger'\n",
    "        for x in self.less_than[larger]:\n",
    "            self.less_than[smaller].add(x)\n",
    "            \n",
    "        # 3. Add all relations where something less than 'smaller' is less than 'larger'\n",
    "        for x in self.less_than:\n",
    "            if smaller in self.less_than[x]:\n",
    "                self.less_than[x].add(larger)\n",
    "                for y in self.less_than[larger]:\n",
    "                    self.less_than[x].add(y)\n",
    "    \n",
    "    def get_predecessors(self, element: T) -> Set[T]:\n",
    "        \"\"\"Get all elements that are less than the given element\"\"\"\n",
    "        predecessors = set()\n",
    "        for potential_pred, successors in self.less_than.items():\n",
    "            if element in successors:\n",
    "                predecessors.add(potential_pred)\n",
    "        return predecessors\n",
    "    \n",
    "    def get_successors(self, element: T) -> Set[T]:\n",
    "        \"\"\"Get all elements that are greater than the given element\"\"\"\n",
    "        return self.less_than[element]\n",
    "    \n",
    "    def get_minimal_elements(self, elements: Set[T]) -> Set[T]:\n",
    "        \"\"\"Get all minimal elements from the given set according to the partial order.\n",
    "        A minimal element has no other elements in the set that are less than it.\"\"\"\n",
    "        minimal = set(elements)\n",
    "        for element in elements:\n",
    "            # Remove any element that has a predecessor in the set\n",
    "            predecessors = self.get_predecessors(element)\n",
    "            if predecessors & elements:  # if there's any overlap with our set\n",
    "                minimal.remove(element)\n",
    "        return minimal\n",
    "    \n",
    "    def get_maximal_elements(self, elements: Set[T]) -> Set[T]:\n",
    "        \"\"\"Get all maximal elements from the given set according to the partial order.\n",
    "        A maximal element has no other elements in the set that are greater than it.\"\"\"\n",
    "        maximal = set(elements)\n",
    "        for element in elements:\n",
    "            # Remove any element that has a successor in the set\n",
    "            successors = self.get_successors(element)\n",
    "            if successors & elements:  # if there's any overlap with our set\n",
    "                maximal.remove(element)\n",
    "        return maximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "po = PartialOrder()\n",
    "\n",
    "# Add some relations\n",
    "po.add_relation(1, 2)\n",
    "po.add_relation(2, 3)\n",
    "po.add_relation(4, 5)\n",
    "po.add_relation(5, \"joe\")\n",
    "\n",
    "# Now 1 < 3 should be true due to transitivity\n",
    "assert po.get_successors(1) == {2, 3}\n",
    "assert po.get_predecessors(3) == {1, 2}\n",
    "\n",
    "# Get minimal elements from a set\n",
    "elements = {1, 2, 3, 4, 5 }\n",
    "assert po.get_minimal_elements(elements) == {1, 4}\n",
    "\n",
    "# Get maximal elements from a set\n",
    "assert po.get_maximal_elements(elements) == {3, 5}\n",
    "\n",
    "\n",
    "some_new_elements = {1,2,3,4,5,\"jack\"}\n",
    "assert po.get_minimal_elements(some_new_elements) == {1,4,\"jack\"}\n",
    "assert po.get_maximal_elements(some_new_elements) == {3,5,\"jack\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = \"\"\"\n",
    "input:\n",
    "  content: \"hello world\"\n",
    "expected:\n",
    "  # we give the name of the trace node\n",
    "  - node_a:\n",
    "      # we describe what output we expect from the node using accessors as keys\n",
    "      # the value is what we expect the accessor to return\n",
    "      b.c: |\n",
    "        jimmy went\n",
    "        to the store\n",
    "      # we can also give a label to the node so we can refer to it later\n",
    "      # using the $label key\n",
    "      $label: node_a1\n",
    "\n",
    "  - node_b:\n",
    "      # we can give multiple comparisons to the same node, using different accessors\n",
    "      d.e:\n",
    "        value: jimmy\n",
    "        comparison: \"regex\"\n",
    "      f.g:\n",
    "        value: \"is a good boy\"\n",
    "        comparison: \"chat\"\n",
    "        kwargs:\n",
    "          case_sensitive: false\n",
    "\n",
    "  # we can also give a regex to match the node name\n",
    "  - node_.*:\n",
    "      .: \"store\"\n",
    "      # using the $parallel key we can specify that this node is expected in parallel with the previous node\n",
    "      # so we do not know which trace will be logged first\n",
    "      $parallel: true\n",
    "      $label: node_z\n",
    "\n",
    "  - node_c:\n",
    "      b.c: \"store\"\n",
    "      # we can specify more complex ordering constraints using before and after using the $label key\n",
    "      # before and after are either a label or a list of labels\n",
    "      # in this case we say that node_c should be after node_a1 and before node_z\n",
    "      $after: node_a1\n",
    "      $before: node_z\n",
    "      \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.mappings import parse_edge_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_trace = [\n",
    "    {\n",
    "        # should be ignored\n",
    "        \"name\": \"Start\",\n",
    "        \"output\": \"hello world\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_a\",\n",
    "        \"output\": {'b':{'c':\"jimmy went\\nto the store\\nto buy some milk\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node c\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": {'b':{'c':\"store is good\"}}\n",
    "    },\n",
    "    {\n",
    "        # shouldnt match\n",
    "        \"name\": \"node_a2\",\n",
    "        \"output\": {'b':{'d':\"store\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node_z\n",
    "        \"name\": \"node_x\",\n",
    "        \"output\": \"store\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_b\",\n",
    "        \"output\": {\n",
    "            'f':{'g':\"is a good boy\"},\n",
    "            'd':{'e':\"jimmy\"}\n",
    "            }\n",
    "    },\n",
    "    {   \n",
    "        # second option to node c, only relevant if node_* matches to node_y\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": {'b':{'c':\"store is good but not good enough\"}}\n",
    "    },\n",
    "    {\n",
    "        # second option to node_z\n",
    "        \"name\": \"node_y\",\n",
    "        \"output\": \"stores\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# expected labels: [noda_a1,1 (b) ,node_z,3 (c)] # c needs to be before z\n",
    "# a:[1], c:[2,6] , b:[5] z:[4,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_mappings = {\n",
    "    frozendict({'node_a1':1,'1':5,'node_z':4,'3':2}), \n",
    "    frozendict({'node_a1':1,'1':5,'node_z':7,'3':2}),\n",
    "    frozendict({'node_a1':1,'1':5,'node_z':7,'3':6}),\n",
    "}\n",
    "\n",
    "best_mapping = frozendict({'node_a1':1,'1':5,'node_z':4,'3':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Expected Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import Dict, Any,Optional, Union, List\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condition(BaseModel):\n",
    "    accessor: tuple[str, ...]\n",
    "    value: Any\n",
    "    comparison: Optional[str] = None\n",
    "    kwargs: Dict[str,Any] = {}\n",
    "\n",
    "class ExpectedTraceStep(BaseModel):\n",
    "    name: str\n",
    "    label: Union[str,int]\n",
    "    conditions: List[Condition]\n",
    "    before: Optional[List[Union[str,int]]] = None\n",
    "    after: Optional[List[Union[str,int]]] = None\n",
    "\n",
    "class ExpectedTrace(BaseModel):\n",
    "    input: Any\n",
    "    expected: List[ExpectedTraceStep]\n",
    "\n",
    "class Trace(BaseModel):\n",
    "    model_config = ConfigDict(extra='allow')\n",
    "    name: str\n",
    "    output: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace_step(yaml_obj: Dict[str,Any],idx:int,labels:List[str]) -> ExpectedTraceStep:\n",
    "    if len(yaml_obj.keys()) != 1:\n",
    "        raise SyntaxError(f\"Expected a single key in trace step {idx}, got {yaml_obj.keys()}\")\n",
    "    \n",
    "    name = list(yaml_obj.keys())[0]\n",
    "    value = yaml_obj[name]\n",
    "    label = value.pop(\"$label\",None)\n",
    "    if label is None:\n",
    "        label = str(idx)\n",
    "\n",
    "    before = value.pop(\"$before\",list())\n",
    "    if isinstance(before,str):\n",
    "        before = [before]\n",
    "    after = value.pop(\"$after\",list())\n",
    "    if isinstance(after,str):\n",
    "        after = [after]\n",
    "    parallel = value.pop(\"$parallel\",False)\n",
    "\n",
    "    if parallel and idx == 0:\n",
    "        raise ValueError(f\"Expected trace step {idx} is has $parallel: true, but is the first step\")\n",
    "\n",
    "    if not parallel and len(after) == 0 and idx > 0:\n",
    "        after.append(labels[-1])\n",
    "    \n",
    "    conditions = []\n",
    "    for accessor,params in value.items():\n",
    "        if isinstance(params,str):\n",
    "            params = {\"value\":params}\n",
    "        try:\n",
    "            accessor = parse_edge_descriptor(accessor,start='accessor')\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Error parsing accessor {accessor} for step {idx}. Make sure it is formatted correctly\") from e\n",
    "        condition_data ={\n",
    "            'accessor':accessor,\n",
    "            **params\n",
    "        }\n",
    "        try:\n",
    "            conditions.append(Condition.model_validate(condition_data))\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"When parsing condition {value} for step {idx}\") from e\n",
    "    \n",
    "    return ExpectedTraceStep(name=name,label=label,conditions=conditions,before=before,after=after)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_c': {'b.c': 'store', '$after': 'node_a1', '$before': 'node_z'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_obj = yaml.safe_load(example_yaml)\n",
    "sub_yaml = yaml_obj['expected'][-1]\n",
    "sub_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed =  parse_expected_trace_step(sub_yaml,0,labels=[])\n",
    "expected = ExpectedTraceStep(name='node_c', label=\"0\", conditions=[Condition(accessor=('b','c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])\n",
    "assert parsed == expected, parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Trace(name='Start', output='hello world'),\n",
       " Trace(name='node_a', output={'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good'}}),\n",
       " Trace(name='node_a2', output={'b': {'d': 'store'}}),\n",
       " Trace(name='node_x', output='store'),\n",
       " Trace(name='node_b', output={'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good but not good enough'}}),\n",
       " Trace(name='node_y', output='stores')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_traces = [Trace.model_validate(trace) for trace in example_trace]\n",
    "parsed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace(yaml_str: str) -> ExpectedTrace:\n",
    "    if isinstance(yaml_str,Path):\n",
    "        yaml_string = yaml_str.read_text()\n",
    "    else:\n",
    "        yaml_string = yaml_str\n",
    "    \n",
    "    try:\n",
    "        yaml_obj = yaml.safe_load(yaml_string)\n",
    "    except Exception as e:\n",
    "        raise SyntaxError(f\"Error parsing yaml:\\n{yaml_string}\\n{e}\")\n",
    "\n",
    "    if list(yaml_obj.keys()) != [\"input\",\"expected\"]:\n",
    "        raise SyntaxError(f\"Expected keys in main scope are 'input' and 'expected', got {yaml_obj.keys()}\")\n",
    "\n",
    "    input = yaml_obj[\"input\"]\n",
    "    expected = yaml_obj[\"expected\"]\n",
    "\n",
    "    parsed_steps = []\n",
    "    labels = []\n",
    "    for i,expected_step in enumerate(expected):\n",
    "        try:\n",
    "            step = parse_expected_trace_step(expected_step,i,labels)\n",
    "            parsed_steps.append(step)\n",
    "            labels.append(step.label)\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Error parsing expected trace step:\\n{expected_step}\") from e\n",
    "    return ExpectedTrace(input=input,expected=parsed_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExpectedTraceStep(name='node_a', label='node_a1', conditions=[Condition(accessor=('b', 'c'), value='jimmy went\\nto the store\\n', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_b', label='1', conditions=[Condition(accessor=('d', 'e'), value='jimmy', comparison='regex', kwargs={}), Condition(accessor=('f', 'g'), value='is a good boy', comparison='chat', kwargs={'case_sensitive': False})], before=[], after=['node_a1']),\n",
       " ExpectedTraceStep(name='node_.*', label='node_z', conditions=[Condition(accessor=('.',), value='store', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_c', label='3', conditions=[Condition(accessor=('b', 'c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected = parse_expected_trace(example_yaml)\n",
    "\n",
    "expected = parsed_expected.expected\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock comparison functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_overlap(result: str, expected: str,**kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the distance between result and expected strings based on word overlap.\n",
    "    Returns a value between 0 and 1, where:\n",
    "    - 0 means perfect match (all words from result are in expected)\n",
    "    - 1 means no overlap (no words from result are in expected)\n",
    "    \n",
    "    Args:\n",
    "        result (str): The string to check words from\n",
    "        expected (str): The string to check words against\n",
    "        \n",
    "    Returns:\n",
    "        float: Distance metric between 0 and 1\n",
    "    \"\"\"\n",
    "    if not isinstance(result,str) or not isinstance(expected,str):\n",
    "        return np.inf\n",
    "    # Convert both strings to lowercase and split into words\n",
    "    result_words = set(result.lower().split())\n",
    "    expected_words = set(expected.lower().split())\n",
    "    \n",
    "    # If result is empty, return 1.0 (maximum distance)\n",
    "    if not result_words:\n",
    "        return 1.0\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = len(result_words.intersection(expected_words))\n",
    "    total = len(result_words)\n",
    "    \n",
    "    # Calculate distance (1 - percentage)\n",
    "    distance = 1.0 - (overlap / total)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "result = \"The quick brown fox\"\n",
    "expected = \"The lazy brown dog\"\n",
    "assert word_overlap(result, expected) == 0.5  # Output: 0.5 (2 out of 4 words match)\n",
    "\n",
    "# Example 2\n",
    "result = \"Hello world\"\n",
    "expected = \"Hello there world\"\n",
    "assert word_overlap(result, expected) == 0.0  # Output: 0.0 (all words match)\n",
    "\n",
    "# Example 3\n",
    "result = \"Python programming\"\n",
    "expected = \"Java development\"\n",
    "assert word_overlap(result, expected) == 1.0  # Output: 1.0 (no words match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def regex(out: str, expected: str,**kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Compare a string against a regex pattern.\n",
    "    Returns 0 if the regex matches, 1 if it doesn't.\n",
    "    \n",
    "    Args:\n",
    "        out (str): The string to check\n",
    "        expected (str): The regex pattern to match against\n",
    "        \n",
    "    Returns:\n",
    "        float: 0 if match, 1 if no match\n",
    "    \"\"\"\n",
    "    if not isinstance(out,str) or not isinstance(expected,str):\n",
    "        return np.inf\n",
    "    try:\n",
    "        if re.search(expected, out,flags=re.IGNORECASE) is not None:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    except Exception:\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic matching\n",
    "assert regex(\"hello world\", \"hello\") == 0.0  # Simple substring match\n",
    "assert regex(\"hello world\", \"^hello\") == 0.0  # Start anchor\n",
    "assert regex(\"hello world\", \"world$\") == 0.0  # End anchor\n",
    "assert regex(\"hello world\", \"hello.*world\") == 0.0  # Pattern with wildcard\n",
    "\n",
    "# Non-matching\n",
    "assert regex(\"hello world\", \"goodbye\") == 1.0  # No match\n",
    "assert regex(\"hello world\", \"^world\") == 1.0  # Wrong position\n",
    "assert regex(\"hello world\", \"hello$\") == 1.0  # Wrong position with anchor\n",
    "\n",
    "# Pattern errors and edge cases\n",
    "assert regex(\"hello world\", \"(unclosed\") == 1.0  # Invalid regex pattern\n",
    "assert regex(\"hello world\", \"\") == 0.0  # Empty pattern matches anything\n",
    "assert regex(\"\", \".*\") == 0.0  # Empty string matches wildcard\n",
    "assert regex(\"\", \"\") == 0.0  # Empty string matches empty pattern\n",
    "\n",
    "# Case sensitivity\n",
    "assert regex(\"Hello World\", \"hello\") == 0.0  # Case-insensitive by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Partial Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Any, Dict, Callabel\n",
    "import itertools as it\n",
    "import re\n",
    "from stringdale.mappings import access_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "from stringdale.core import checkLogs\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_trace_distance(trace,expected,comparisons,default_comparison):\n",
    "\n",
    "    logger.debug(f\"Computing distance for trace {trace} and expected {expected}\")\n",
    "    if not re.search(expected.name, trace.name):\n",
    "        return np.inf\n",
    "    \n",
    "    # check if all accessors are in the trace\n",
    "    for condition in expected.conditions:\n",
    "        try: \n",
    "            sub_object = access_object(trace.output,condition.accessor)\n",
    "        except Exception as e:\n",
    "            return np.inf\n",
    "\n",
    "    distance = 0\n",
    "    for condition in expected.conditions:\n",
    "        condition_func = comparisons.get(condition.comparison, default_comparison)\n",
    "        output_sub_value = access_object(trace.output,condition.accessor)\n",
    "        try:\n",
    "            condition_distance = condition_func(output_sub_value, condition.value, **condition.kwargs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error computing distance for condition {condition} on trace {trace.name}: {e}\") from e\n",
    "        distance += condition_distance\n",
    "    \n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = {\n",
    "    \"regex\": regex,\n",
    "    \"word_overlap\": word_overlap,\n",
    "}\n",
    "default_comparison = word_overlap\n",
    "# TODO from here, debug the distance function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "def compute_distance_matrix(\n",
    "    traces_outputs:List[Any],\n",
    "    expected_trace:ExpectedTrace,\n",
    "    comparisons:Dict[str,Callabel],\n",
    "    default_comparison:Callabel):\n",
    "    \"\"\"\n",
    "    Compute the distance matrix between the traces and the expected traces.\n",
    "\n",
    "    Args:\n",
    "        traces_outputs: List[Any], the outputs of the traces\n",
    "        expected_traces: ExpectedTrace, the expected traces\n",
    "        comparisons: Dict[str,Callabel], the comparisons to use for the distance matrix\n",
    "        default_comparison: Callabel, the default comparison to use for the distance matrix\n",
    "    \"\"\"\n",
    "    expected_steps = expected_trace.expected\n",
    "    distances = np.zeros((len(traces_outputs), len(expected_steps)))\n",
    "    \n",
    "    for (i, trace), (j, expected) in it.product(enumerate(traces_outputs), enumerate(expected_steps)):\n",
    "        distances[i,j] = compute_trace_distance(trace,expected,comparisons,default_comparison)\n",
    "\n",
    "    row_names = [(i ,trace.name) for i,trace in enumerate(traces_outputs)]\n",
    "    col_names = [(step.label,step.name) for i,step in enumerate(expected_steps)]\n",
    "\n",
    "    return pd.DataFrame(distances,index=row_names,columns=col_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Trace(name='Start', output='hello world'),\n",
       " Trace(name='node_a', output={'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good'}}),\n",
       " Trace(name='node_a2', output={'b': {'d': 'store'}}),\n",
       " Trace(name='node_x', output='store'),\n",
       " Trace(name='node_b', output={'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good but not good enough'}}),\n",
       " Trace(name='node_y', output='stores')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(node_a1, node_a)</th>\n",
       "      <th>(1, node_b)</th>\n",
       "      <th>(node_z, node_.*)</th>\n",
       "      <th>(3, node_c)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(0, Start)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(1, node_a)</th>\n",
       "      <td>0.375</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(2, node_c)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(3, node_a2)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(4, node_x)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(5, node_b)</th>\n",
       "      <td>inf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(6, node_c)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(7, node_y)</th>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              (node_a1, node_a)  (1, node_b)  (node_z, node_.*)  (3, node_c)\n",
       "(0, Start)                  inf          inf                inf          inf\n",
       "(1, node_a)               0.375          inf                inf          inf\n",
       "(2, node_c)                 inf          inf                inf     0.666667\n",
       "(3, node_a2)                inf          inf                inf          inf\n",
       "(4, node_x)                 inf          inf                0.0          inf\n",
       "(5, node_b)                 inf          0.0                inf          inf\n",
       "(6, node_c)                 inf          inf                inf     0.833333\n",
       "(7, node_y)                 inf          inf                1.0          inf"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons = {\n",
    "    \"regex\": regex,\n",
    "    \"word_overlap\": word_overlap,\n",
    "}\n",
    "default_comparison = word_overlap\n",
    "# with checkLogs():\n",
    "dist = compute_distance_matrix(parsed_traces,parsed_expected,comparisons,default_comparison)\n",
    "\n",
    "expected_dist = np.array(   \n",
    "        [[       inf,        inf,        inf,        inf],\n",
    "       [0.375     ,        inf,        inf,        inf],\n",
    "       [       inf,        inf,        inf, 0.66666667],\n",
    "       [       inf,        inf,        inf,        inf],\n",
    "       [       inf,        inf, 0.        ,        inf],\n",
    "       [       inf, 0.        ,        inf,        inf],\n",
    "       [       inf,        inf,        inf, 0.83333333],\n",
    "       [       inf,        inf, 1.        ,        inf]]\n",
    ")\n",
    "\n",
    "assert np.allclose(dist.values,expected_dist)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here play with python-constraint2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExpectedTraceStep(name='node_a', label='node_a1', conditions=[Condition(accessor=('b', 'c'), value='jimmy went\\nto the store\\n', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_b', label='1', conditions=[Condition(accessor=('d', 'e'), value='jimmy', comparison='regex', kwargs={}), Condition(accessor=('f', 'g'), value='is a good boy', comparison='chat', kwargs={'case_sensitive': False})], before=[], after=['node_a1']),\n",
       " ExpectedTraceStep(name='node_.*', label='node_z', conditions=[Condition(accessor=('.',), value='store', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_c', label='3', conditions=[Condition(accessor=('b', 'c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected.expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_excel_col(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Number must be non-negative\")\n",
    "    \n",
    "    result = \"\"\n",
    "    n += 1  # Adjust because Excel columns start at 1, not 0\n",
    "    \n",
    "    while n > 0:\n",
    "        n -= 1  # Adjust for 0-based indexing\n",
    "        result = chr(n % 26 + ord('A')).lower() + result\n",
    "        n //= 26\n",
    "        \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert int_to_excel_col(0) == \"a\"\n",
    "assert int_to_excel_col(25) == \"z\"\n",
    "assert int_to_excel_col(26) == \"aa\"\n",
    "assert int_to_excel_col(27) == \"ab\"\n",
    "assert int_to_excel_col(51) == \"az\"\n",
    "assert int_to_excel_col(52) == \"ba\"\n",
    "assert int_to_excel_col(701) == \"zz\"\n",
    "assert int_to_excel_col(702) == \"aaa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constraint import Problem,FunctionConstraint\n",
    "from bidict import bidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelToVar():\n",
    "    def __init__(self):\n",
    "        self.label_to_var = bidict()\n",
    "        self.label_to_index = bidict()\n",
    "\n",
    "    def add_label(self,label:str,idx:int):\n",
    "        self.label_to_var[label] = int_to_excel_col(idx)\n",
    "        self.label_to_index[label] = idx\n",
    "\n",
    "    def get_label(self,col:str) -> str:\n",
    "        return self.label_to_var.inverse[col]\n",
    "\n",
    "    def get_index(self,label:str) -> int:\n",
    "        return self.label_to_index[label]\n",
    "\n",
    "    def get_col(self,label:str) -> int:\n",
    "        return self.label_to_var[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_var = LabelToVar()\n",
    "label_to_var.add_label(\"x\",0)\n",
    "label_to_var.add_label(\"y\",1)\n",
    "label_to_var.add_label(\"z\",2)\n",
    "\n",
    "assert label_to_var.get_col(\"x\") == \"a\"\n",
    "assert label_to_var.get_col(\"y\") == \"b\"\n",
    "assert label_to_var.get_col(\"z\") == \"c\"\n",
    "\n",
    "assert label_to_var.get_label(\"a\") == \"x\"\n",
    "assert label_to_var.get_label(\"b\") == \"y\"\n",
    "assert label_to_var.get_label(\"c\") == \"z\"\n",
    "\n",
    "assert label_to_var.get_index(\"x\") == 0\n",
    "assert label_to_var.get_index(\"y\") == 1\n",
    "assert label_to_var.get_index(\"z\") == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dist\n",
    "expected_traces = parsed_expected.expected\n",
    "traces = parsed_traces\n",
    "\n",
    "\n",
    "# get a mapping between expected labels and fresh var names\n",
    "label_to_var = LabelToVar()\n",
    "for idx,expected_step in enumerate(parsed_expected.expected):\n",
    "    label_to_var.add_label(expected_step.label,idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_mappings(expected_traces:List[ExpectedTrace],label_to_var:LabelToVar):\n",
    "    \"\"\"\n",
    "    Gets possible mappings between expected traces and actual traces.\n",
    "    By building a constraint satisfaction problem and solving it.\n",
    "    \"\"\"\n",
    "    p = Problem()\n",
    "    for col_idx,expected_step in enumerate(parsed_expected.expected):\n",
    "        viable_trace_row_names = dist.iloc[:,col_idx][dist.iloc[:,col_idx] < np.inf].index.tolist()\n",
    "        viable_trace_row_nums = [row_name[0] for row_name in viable_trace_row_names]\n",
    "        var_name = label_to_var.get_col(expected_step.label)\n",
    "        p.addVariable(var_name,viable_trace_row_nums)\n",
    "        logger.debug(f\"Adding variable {var_name} with domain {viable_trace_row_nums}\")\n",
    "\n",
    "        for before_label in expected_step.before:\n",
    "            before_var_name = label_to_var.get_col(before_label)\n",
    "            logger.debug(f\"Adding constraint {before_var_name} < {var_name}\")\n",
    "            p.addConstraint(f\"{var_name} < {before_var_name}\")\n",
    "\n",
    "        for after_label in expected_step.after:\n",
    "            after_var_name = label_to_var.get_col(after_label)\n",
    "            logger.debug(f\"Adding constraint {var_name} < {after_var_name}\")\n",
    "            p.addConstraint(f\"{after_var_name} < {var_name}\")\n",
    "\n",
    "    # these solutions use colnames    \n",
    "    solutions = p.getSolutions()\n",
    "    # invert the colnames back to labels\n",
    "    labeled_solutions = set(frozendict({label_to_var.get_label(k):v for k,v in sol.items()}) for sol in solutions)\n",
    "    return labeled_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdiff import DeepDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - Adding variable a with domain [1]\n",
      "__main__ - DEBUG - Adding variable b with domain [5]\n",
      "__main__ - DEBUG - Adding constraint b < a\n",
      "__main__ - DEBUG - Adding variable c with domain [4, 7]\n",
      "__main__ - DEBUG - Adding variable d with domain [2, 6]\n",
      "__main__ - DEBUG - Adding constraint c < d\n",
      "__main__ - DEBUG - Adding constraint d < a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 4}),\n",
       " frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 7}),\n",
       " frozendict.frozendict({'node_a1': 1, '3': 6, '1': 5, 'node_z': 7})}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs():\n",
    "    labeled_solutions = get_possible_mappings(parsed_expected.expected,label_to_var)\n",
    "\n",
    "assert DeepDiff(labeled_solutions,possible_mappings) == {}\n",
    "labeled_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here, make a function that finds best mapping using the dist matrix, possible mappings and dict to var, and then make an e2e function\n",
    "# that does the matrix construction, label mapping, mapping finding, and then the choosing of the best mapping.\n",
    "\n",
    "# after that refactor to po time warping notebook and eval notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here, make expected of the runnign example\n",
    "# invert the var names back to labels.\n",
    "\n",
    "# assert exected\n",
    "\n",
    "# Factor into po time warping notebook\n",
    "# and have the higher order code in the eval notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_warping_path(distances:np.ndarray,po:PartialOrder):\n",
    "    \"\"\"\n",
    "    Find the warping path between the traces and the expected traces.\n",
    "\n",
    "    Args:\n",
    "        distances: np.ndarray, the distance matrix between the traces and the expected traces\n",
    "        po: PartialOrder, the partial order of the expected traces, assumed po to be over excepted traces indices\n",
    "\n",
    "    \"\"\"\n",
    "    # find the minimal elements of the expected traces\n",
    "    minimal_elements = po.get_minimal_elements(set(range(len(expected_traces))))\n",
    "\n",
    "    # build the matching with poset constraints as a CSP problem using python-constraint2\n",
    "    # add make sure we give each expected trace the domain over elements that is can actually match to\n",
    "    # so no inf distances\n",
    "\n",
    "    # iterate over all solution and use the distance matrix to find the best match\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input:\n",
      "  content: \"hello world\"\n",
      "expected:\n",
      "  - node_a:\n",
      "      b.c: |\n",
      "        jimmy went\n",
      "        to the store\n",
      "      $label: node_a1\n",
      "\n",
      "  - node_b:\n",
      "      d.e:\n",
      "        value: jimmy\n",
      "        comparison: \"regex\"\n",
      "      f.g:\n",
      "        value: \"is a good boy\"\n",
      "        comparison: \"chat\"\n",
      "        kwargs:\n",
      "          case_sensitive: false\n",
      "\n",
      "  - node_.*:\n",
      "      .: \"store\"\n",
      "      $parallel: true\n",
      "\n",
      "  - node_c:\n",
      "      b.c: \"store\"\n",
      "      $before: node_a1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "D(i,j,hist) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here, make an example of trace and expected trace with length, equality and regex matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, get a list of traces, a dict of the exected traces (maybe keys by label if exists or by index), and a partial ordering of the expected traces\n",
    "\n",
    "# compute the distance matrix between the traces and the expected traces\n",
    "\n",
    "# make the warping path, but rather than a numberic matrix, we need a mapping of who was mapped to who."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dynamic warping\n",
    "# we make a plot like https://pypi.org/project/dtaidistance/ maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': {'content': 'hello world'},\n",
       "  'expected': [{'node_a': {'b.c': 'jimmy went\\nto the store\\n'}},\n",
       "   {'node_b': {'d.e': {'value': 'jimmy', 'comparison': 'regex'},\n",
       "     'f.g': {'value': 'is a good boy',\n",
       "      'comparison': 'chat',\n",
       "      'kwargs': {'case_sensitive': False}}}},\n",
       "   {'node_.*': {'.': 'store', '$parallel': True}}]},\n",
       " {'input': {'content': 'Go to the store and buy some milk'},\n",
       "  'expected': [{'node_a': {'b.c': 'store'}}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml.safe_load(example_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the DPTW to match each trace to an expected trace\n",
    "# than we have multiple scores\n",
    "    # total distance, \n",
    "    # total distance per expected trace, \n",
    "    # coverage (percent of nodes expected), \n",
    "    # time coverage (percent of time of nodes expected), used to ignore nodes with no logic\n",
    "\n",
    "\n",
    "# this experiment object can be dumped into a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Regression detection\n",
    "# here we just compare the runs to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
