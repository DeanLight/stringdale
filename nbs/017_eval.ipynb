{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it on the example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datapoint class\n",
    "\n",
    "\n",
    "# take react and eval it.\n",
    "# build cosine and chat ranking function as example\n",
    "# also use the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dataSeries the distance per step and total metrics such as sum of distances and coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lift this to a least of datapoints\n",
    "# make the dataset class\n",
    "# get it from a directory, and then eval it.\n",
    "\n",
    "# make the time series into a dataframe of all datapoints (in case of directory take the filename relative to the directory)\n",
    "# maybe give ability to have nested directories with regexs to filter the files?\n",
    " \n",
    "# then we output a datasetRun class that has the results dataframe but also metadata about the run\n",
    "# for now lets start with an agent name and run id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes 2 datasetRuns and returns a comparison per datapoint on the difference between the two runs\n",
    "# then have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End regression testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show the following example\n",
    "\n",
    "# always have a factory (or a nested factory expression)\n",
    "# take the same factory and change the prompt\n",
    "# take the same agent but give it a different db\n",
    "# or maybe the same db but a different filter\n",
    "# same factory but different fewshot examples yaml files\n",
    "# and show regression testing\n",
    "\n",
    "# our function should take 2 agents, and a EvalDataSet \n",
    "# return the comparison and print the summary as the k datapoints that regressed the most\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO in future, specify 2 datasets (by dirs and regex)\n",
    "# one is the train one is the test\n",
    "# we take an agent (or 2 for comparison)\n",
    "# and we do the same logic for evaluating and comparing, however, we print the statistics only for the test set\n",
    "# but we show the regression for the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData model\\n\\nWe have a dataset\\n* containing tests\\n* each test has the input to the agent\\n* and the expected output\\n* test is any object that can be serialized to json\\n* expected output is a partial trace spec\\n\\n* partial trace spec is a list of steps\\n* each step has a name is a dict with accessors and value are how to check them\\n* names are the node name we expect to see in the trace\\n* the dict defines what we expect the value to look like\\n\\n\\nWhen we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\\nsince the partial trace spec does not \\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data model\n",
    "\n",
    "We have a dataset\n",
    "* containing tests\n",
    "* each test has the input to the agent\n",
    "* and the expected output\n",
    "* test is any object that can be serialized to json\n",
    "* expected output is a partial trace spec\n",
    "\n",
    "* partial trace spec is a list of steps\n",
    "* each step has a name is a dict with accessors and value are how to check them\n",
    "* names are the node name we expect to see in the trace\n",
    "* the dict defines what we expect the value to look like\n",
    "\n",
    "\n",
    "When we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\n",
    "since the partial trace spec does not \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedTrace:\n",
    "    pass\n",
    "\n",
    "class DataPointRun:\n",
    "    # basically a list of traces, agent input and agent output\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traces_from_file(file_path):\n",
    "    pass\n",
    "\n",
    "def collect_traces_from_logg_aggregator(logger):\n",
    "    pass\n",
    "\n",
    "def run_dataset(agent,dataset,output_dir):\n",
    "    # for each data point in the dataset\n",
    "    # run the agent\n",
    "    # collect the traces into a file\n",
    "    # return the file path\n",
    "    pass\n",
    "\n",
    "def write_comparison_to_file(dataset_run,expected_traces,output_dir):\n",
    "    # run the comparison and write the results to a file\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_summary(runs,dir):\n",
    "    # get the run files and the comparison files\n",
    "    # get the total metrics per expected node and total\n",
    "    # make them into a dataframe\n",
    "    pass\n",
    "\n",
    "def plot_runs(runs,dir):\n",
    "    # call runs_summary\n",
    "    # plot the results\n",
    "    pass\n",
    "\n",
    "def check_regressions(runs,dir):\n",
    "    # get two runs\n",
    "    # for each input, if the second run is worse than the first, then flag it\n",
    "    # make a dataframe of the regressions on a whole run basis\n",
    "    \n",
    "    # also make a dataframe of the regressions on a per node basis for the runs that regressed.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the DPTW to match each trace to an expected trace\n",
    "# than we have multiple scores\n",
    "    # total distance, \n",
    "    # total distance per expected trace, \n",
    "    # coverage (percent of nodes expected), \n",
    "    # time coverage (percent of time of nodes expected), used to ignore nodes with no logic\n",
    "\n",
    "\n",
    "# this experiment object can be dumped into a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Regression detection\n",
    "# here we just compare the runs to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
