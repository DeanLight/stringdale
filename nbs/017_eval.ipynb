{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "from stringdale.podtw import (\n",
    "    parse_expected_trace,\n",
    "    align_traces,\n",
    "    word_overlap,\n",
    "    regex,\n",
    "    ExpectedTrace,\n",
    "    Trace\n",
    ")\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from frozendict import frozendict\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "\n",
    "from typing import List, Union\n",
    "import jsonlines\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO multiple inputs\n",
    "# TODO any comparison so we can check for existance of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def parse_trace(trace_path:Union[str,Path]) -> List[Trace]:\n",
    "    \"\"\"\n",
    "    Parse a trace file into a list of Trace objects.\n",
    "    \"\"\"\n",
    "    with jsonlines.open(trace_path) as reader:\n",
    "        return [Trace.model_validate(trace) for trace in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.core import get_git_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_dir = get_git_root() / \"sample_data\" / \"eval\"\n",
    "\n",
    "example_trace_path = sample_data_dir / \"traces0.jsonl\"\n",
    "example_expected_path = sample_data_dir / \"expected0.yaml\"\n",
    "\n",
    "\n",
    "example_comparisons = {\n",
    "    \"word_overlap\":word_overlap,\n",
    "    \"regex\":regex,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_trace = parse_trace(example_trace_path)\n",
    "example_expected = parse_expected_trace(example_expected_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 4})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_match,score,dist =await align_traces(example_trace,example_expected,comparisons=example_comparisons,default_comparison=word_overlap)\n",
    "best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_a1': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy went\\nto the store\\n',\n",
       "     'actual': 'jimmy went\\nto the store\\nto buy some milk',\n",
       "     'distance': 0.375,\n",
       "     'accessor': 'b.c'}],\n",
       "   'distance': 0.375,\n",
       "   'expected_idx': 0,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_a',\n",
       "   'expected_label': 'node_a1'}},\n",
       " 'node_z': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}},\n",
       "     'distance': inf,\n",
       "     'accessor': '.'}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good'}},\n",
       "     'distance': inf,\n",
       "     'accessor': '.'}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  3: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'d': 'store'}},\n",
       "     'distance': inf,\n",
       "     'accessor': '.'}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 3,\n",
       "   'actual_name': 'node_a2',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  4: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store',\n",
       "     'distance': 0.0,\n",
       "     'accessor': '.'}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 4,\n",
       "   'actual_name': 'node_x',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  5: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}},\n",
       "     'distance': inf,\n",
       "     'accessor': '.'}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good but not good enough'}},\n",
       "     'distance': inf,\n",
       "     'accessor': '.'}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  7: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'stores',\n",
       "     'distance': 1.0,\n",
       "     'accessor': '.'}],\n",
       "   'distance': 1.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 7,\n",
       "   'actual_name': 'node_y',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'}},\n",
       " '3': {2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good',\n",
       "     'distance': 0.6666666666666667,\n",
       "     'accessor': 'b.c'}],\n",
       "   'distance': 0.6666666666666667,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good but not good enough',\n",
       "     'distance': 0.8333333333333334,\n",
       "     'accessor': 'b.c'}],\n",
       "   'distance': 0.8333333333333334,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'}},\n",
       " '1': {5: {'comparisons': [{'comparison': 'regex',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy',\n",
       "     'actual': 'jimmy',\n",
       "     'distance': 0.0,\n",
       "     'accessor': 'd.e'},\n",
       "    {'comparison': 'word_overlap',\n",
       "     'kwargs': {'case_sensitive': False},\n",
       "     'expected': 'is a good boy',\n",
       "     'actual': 'is a good boy',\n",
       "     'distance': 0.0,\n",
       "     'accessor': 'f.g'}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 1,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_b',\n",
       "   'expected_label': '1'}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realistic Comparison Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from stringdale.db import openai_embed\n",
    "from stringdale.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def cosine_dist(out: str, expected: str, model: str = 'text-embedding-3-small') -> float:\n",
    "    \"\"\"Compute cosine distance between two strings using OpenAI embeddings.\n",
    "    \n",
    "    Args:\n",
    "        out: First string to compare\n",
    "        expected: Second string to compare\n",
    "        model: OpenAI embedding model to use (default: 'text-embedding-3-small')\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the two strings (between -1 and 1)\n",
    "    \"\"\"\n",
    "    # Get embeddings for both strings\n",
    "    out_embedding = await openai_embed(out, model=model)\n",
    "    expected_embedding = await openai_embed(expected, model=model)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    dot_product = np.dot(out_embedding, expected_embedding)\n",
    "    norm_out = np.linalg.norm(out_embedding)\n",
    "    norm_expected = np.linalg.norm(expected_embedding)\n",
    "    \n",
    "    # Return cosine similarity\n",
    "    return 1-dot_product / (norm_out * norm_expected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_dist = await cosine_dist(\"hello\",\"hello\")\n",
    "basic_dist\n",
    "assert basic_dist < 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3944818489490096)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await cosine_dist(\"hello\",\"hello stranger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatEvalScore(BaseModel):\n",
    "    score:float\n",
    "\n",
    "class ChatEval:\n",
    "    def __init__(self,model:str=\"gpt-4o-mini\",system_prompt:str=None):\n",
    "        self.model = model\n",
    "        base_prompt = \"\"\"\n",
    "            You are a helpful assistant that evaluates the similarity of two strings.\n",
    "            You will be given two strings, and you will need to evaluate the similarity of the two strings.\n",
    "            You will need to return a score between 0 and 1, where 0 is the lowest similarity and 1 is the highest similarity.\n",
    "            \"\"\"    \n",
    "        self.messages = [\n",
    "            {\"role\":\"system\",\"content\":base_prompt},\n",
    "        ]\n",
    "        if system_prompt is not None:\n",
    "            self.messages.append({\"role\":\"system\",\"content\":system_prompt})\n",
    "\n",
    "            \n",
    "\n",
    "    async def __call__(self,out:str,expected:str)->float:\n",
    "        self.messages.append({\"role\":\"user\",\"content\":f\"string1: {out}\\nstring2: {expected}\"})\n",
    "        chat = Chat(model=self.model,messages=self.messages,output_schema=ChatEvalScore)\n",
    "        response = await chat()\n",
    "        return response['content'].score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_eval = ChatEval(system_prompt=\"if one of the strings is world, output 0.5\")\n",
    "result = await chat_eval(\"hello\",\"world\")\n",
    "assert result == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eq(a,b):\n",
    "    if a == b:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.inf\n",
    "\n",
    "def any(a,b):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 3500\n",
    "expr = \"({0} < 4000) & ({0} > 3000)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(3500 < 4000) & (3500 > 3000)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_expr = expr.format(out)\n",
    "f_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.tools import run_python_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def safe_eval(out,expression):\n",
    "    try:\n",
    "        formatted_expressions = expression.format(out)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error formatting expression: {expression} with value {out}, error: {e}\")\n",
    "        return np.inf\n",
    "    value = run_python_code(formatted_expressions)\n",
    "    if isinstance(value,str) and value.startswith(\"Error\"):\n",
    "        logger.warning(\n",
    "            f\"Error evaluating expression: {formatted_expressions} = {value}\\n\"\n",
    "            f\"out: {out}\\n\"\n",
    "            f\"expression: {expression}\\n\"\n",
    "            f\"error: {e}\"\n",
    "        )\n",
    "        return np.inf\n",
    "    logger.debug(f\"safe_eval: {formatted_expressions} = {value}\")\n",
    "    if isinstance(value,bool):\n",
    "        return 0 if value else np.inf\n",
    "    elif isinstance(value,float):\n",
    "        return value\n",
    "    else:\n",
    "        logger.debug(\n",
    "            f\"When evaluating {expression} with value {out}\\n\"\n",
    "            f\"Expected float or bool, got {type(value)} with value {repr(value)}\"\n",
    "            )\n",
    "        return np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - safe_eval: \n",
      "x=4000\n",
      "(3500 < x) & (3500 > 3000)\n",
      " = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_string =\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\"\n",
    "\n",
    "with checkLogs():\n",
    "    y =safe_eval(3500,eval_string)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_eval(3500,\"\"\"\n",
    "x=4000\n",
    "({0} < x) & ({0} > 3000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running and evaluating a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List,Dict,Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPoint(BaseModel):\n",
    "    traces:List[Trace]\n",
    "    expected:ExpectedTrace\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.react import ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def _run_agent(Agent,expected_trace,trace_out):\n",
    "    d=Agent()\n",
    "    with jsonlines.open(trace_out,'w') as writer:\n",
    "        for input in expected_trace.input:\n",
    "            async for trace in d.arun(input):\n",
    "                writer.write(json.loads(trace.model_dump_json(include={'name','output','duration'})))\n",
    "            if d.finished:\n",
    "                break\n",
    "\n",
    "async def evaluate_datapoint(Agent,comparisons,default_comparison,expected_yaml,trace_out=None,force_run=False):\n",
    "    if trace_out is None:\n",
    "        trace_out = expected_yaml.parent/expected_yaml.name.replace(\".yaml\", \".jsonl\").replace(\"expected\", \"actual\")\n",
    "\n",
    "    if not trace_out.parent.exists():\n",
    "        os.makedirs(trace_out.parent,exist_ok=True)\n",
    "    try:\n",
    "        expected_trace = parse_expected_trace(expected_yaml)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error parsing expected trace {expected_yaml}: {e}\") from e\n",
    "        \n",
    "\n",
    "    if force_run or not trace_out.exists():\n",
    "        if not trace_out.exists():\n",
    "            logger.info(f\"Trace file {trace_out.name} does not exist, running agent\")\n",
    "        else:\n",
    "            logger.info(f\"Force running {trace_out.name}\")\n",
    "        await _run_agent(Agent,expected_trace,trace_out)\n",
    "    else:\n",
    "        logger.info(f\"Trace file {trace_out.name} already exists, skipping agent run\")\n",
    "\n",
    "    parsed_trace = parse_trace(trace_out)\n",
    "    aligned_trace,score,debug_info = await align_traces(parsed_trace,expected_trace,comparisons,default_comparison)\n",
    "    \n",
    "    return aligned_trace,score,debug_info,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReactAgent\n",
    "expected_yaml = sample_data_dir/\"react_expected.yaml\"\n",
    "bad_expected_yaml = sample_data_dir/\"react_bad_expected.yaml\"\n",
    "comparisons = {\n",
    "    \"eq\":eq,\n",
    "    \"eval\":safe_eval,\n",
    "}\n",
    "default_comparison = cosine_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_bad_actual.jsonl already exists, skipping agent run\n",
      "No viable trace row nums for expected trace 1\n",
      "No possible mappings found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " inf,\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_bad_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "with checkLogs():\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,bad_expected_yaml)\n",
    "\n",
    "assert alignment is None\n",
    "alignment,score,trace_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Trace file react_actual.jsonl already exists, skipping agent run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'0': 2, '1': 8}),\n",
       " np.float64(0.3244313858854829),\n",
       " PosixPath('/Users/dean/dl/stringdale/sample_data/eval/react_actual.jsonl'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    alignment,score,debug_info,trace_out = await evaluate_datapoint(agent,comparisons,default_comparison,expected_yaml)\n",
    "\n",
    "assert dict(alignment) == {'0': 2, '1': 8}\n",
    "alignment,score,trace_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _pd_order_columns_first(df:pd.DataFrame,first_columns:list[str]):\n",
    "    \"\"\"\n",
    "    Reorder the columns of a pandas dataframe to put the first_columns first.\n",
    "    \"\"\"\n",
    "    return df[first_columns + [c for c in df.columns if c not in first_columns]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  expected  distance comparison\n",
       "0       1         1         1         eq\n",
       "1       2         2         2         eq\n",
       "2       3         3         3         eq"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame([\n",
    "    {'distance':1,'comparison':'eq','actual':1,'expected':1},\n",
    "    {'distance':2,'comparison':'eq','actual':2,'expected':2},\n",
    "    {'distance':3,'comparison':'eq','actual':3,'expected':3},\n",
    "])\n",
    "\n",
    "_pd_order_columns_first(x,['actual','expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def summarize_datapoint(name,alignment,debug_info):\n",
    "    \"\"\"\n",
    "    Summarize the datapoint by getting the distance per step and total metrics such as sum of distances and coverage\n",
    "    by using the alignment and the debug info\n",
    "    \"\"\"\n",
    "    deep_dive_fit = []\n",
    "\n",
    "    for expected_node_id,trace_idx in alignment.items():\n",
    "        match_data = debug_info[expected_node_id][trace_idx]\n",
    "        for comp in match_data['comparisons']:\n",
    "            summary = deepcopy(comp)\n",
    "            summary['node_name'] = match_data['actual_name']\n",
    "            summary['expected_name'] = match_data['expected_name']\n",
    "            summary['expected_node_id'] = expected_node_id\n",
    "            summary['trace_idx'] = trace_idx\n",
    "            # TODO put node name and node pattern\n",
    "            deep_dive_fit.append(summary)\n",
    "\n",
    "    df = pd.DataFrame(deep_dive_fit)\n",
    "    df['datapoint'] = name\n",
    "    df = _pd_order_columns_first(df,['datapoint','expected_node_id','trace_idx','accessor','comparison','actual','expected','distance'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content.name</td>\n",
       "      <td>eq</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>wikipedia_search</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>react</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content.input.q</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Barack Obama</td>\n",
       "      <td>Obama</td>\n",
       "      <td>0.324431</td>\n",
       "      <td>{}</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>content.name</td>\n",
       "      <td>eq</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>run_python_code</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>react</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>content.output</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>3844</td>\n",
       "      <td>({0} &lt; 4000) &amp; ({0} &gt; 3000)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>use_tool</td>\n",
       "      <td>use_tool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx         accessor   comparison  \\\n",
       "0     react                0          2     content.name           eq   \n",
       "1     react                0          2  content.input.q  cosine_dist   \n",
       "2     react                1          8     content.name           eq   \n",
       "3     react                1          8   content.output    safe_eval   \n",
       "\n",
       "             actual                     expected  distance kwargs node_name  \\\n",
       "0  wikipedia_search             wikipedia_search  0.000000     {}  use_tool   \n",
       "1      Barack Obama                        Obama  0.324431     {}  use_tool   \n",
       "2   run_python_code              run_python_code  0.000000     {}  use_tool   \n",
       "3              3844  ({0} < 4000) & ({0} > 3000)  0.000000     {}  use_tool   \n",
       "\n",
       "  expected_name  \n",
       "0      use_tool  \n",
       "1      use_tool  \n",
       "2      use_tool  \n",
       "3      use_tool  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = summarize_datapoint('react',alignment,debug_info)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['expected_node_id'].to_list() == ['0','0','1','1']\n",
    "assert df['accessor'].to_list() == ['content.name','content.input.q','content.name','content.output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale import DiagramSchema\n",
    "from pprint import pprint, pformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trace_out_path(expected_yaml:Path,expected_dir:Path,trace_dir:Path):\n",
    "    return trace_dir / expected_yaml.relative_to(expected_dir).with_suffix(\".jsonl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EvalDataset(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    expected_dir: Path\n",
    "    trace_dir: Path\n",
    "    summary: pd.DataFrame\n",
    "    details: pd.DataFrame\n",
    "    debug: dict\n",
    "\n",
    "    def __repr__(self):     \n",
    "        return (\n",
    "            f\"EvalDataset(expected_dir={self.expected_dir}, \\n\"\n",
    "            f\"  trace_dir={self.trace_dir}, \\n\"\n",
    "            f\"  summary=Dataframe({self.summary.shape}), \\n\"\n",
    "            f\"  details=Dataframe({self.details.shape}), \\n\"\n",
    "            f\"  debug=dict)\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _find_yamls(expected_dir:Path):\n",
    "    expected_yamls = list(expected_dir.glob(\"**/*.yaml\")) + list(expected_dir.glob(\"**/*.yml\"))\n",
    "    return expected_yamls\n",
    "\n",
    "\n",
    "async def eval_dataset(Agent:DiagramSchema,expected_dir,trace_dir,force_run=False,comparisons=None,default_comparison=None):\n",
    "\n",
    "    expected_yamls = _find_yamls(expected_dir)\n",
    "    relative_expected_yamls = [expected_yamls.relative_to(expected_dir) for expected_yamls in expected_yamls]\n",
    "\n",
    "    trace_files  = [_trace_out_path(expected_yaml,expected_dir,trace_dir) for expected_yaml in expected_yamls]\n",
    "\n",
    "    logger.info(f\"Evaluating {len(expected_yamls)} datapoints, logging to {trace_dir}\")\n",
    "    datapoint_tasks = [evaluate_datapoint(\n",
    "            Agent=Agent,\n",
    "            comparisons=comparisons,\n",
    "            default_comparison=default_comparison,\n",
    "            expected_yaml=expected_yaml,\n",
    "            trace_out=trace_file,\n",
    "            force_run=force_run,\n",
    "        ) for expected_yaml,trace_file in zip(expected_yamls,trace_files) if trace_file in trace_files]\n",
    "    \n",
    "    datapoint_results = await asyncio.gather(*datapoint_tasks)\n",
    "\n",
    "    summary_data = list()\n",
    "    deep_dives = list()\n",
    "    debug_infos = dict()\n",
    "\n",
    "    for alignment,score,debug_info,trace_out in datapoint_results:\n",
    "        datapoint_name = trace_out.relative_to(trace_dir).with_suffix(\"\")\n",
    "        summary = {'datapoint_name':datapoint_name,'score':score,'alignment':alignment}\n",
    "        summary_data.append(summary)\n",
    "        deep_dives.append(summarize_datapoint(datapoint_name,alignment,debug_info))\n",
    "        debug_infos[datapoint_name] = debug_info\n",
    "    \n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    if len(deep_dives) > 0:\n",
    "        deep_dives_df = pd.concat(deep_dives).reset_index(drop=True)\n",
    "    else:\n",
    "        deep_dives_df = pd.DataFrame()\n",
    "\n",
    "    return EvalDataset(\n",
    "        expected_dir=expected_dir,\n",
    "        trace_dir=trace_dir,\n",
    "        summary=summary_df,\n",
    "        details=deep_dives_df,\n",
    "        debug=debug_infos\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stringdale.examples.rag import Rag\n",
    "from stringdale.db import ChromaClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agent(conf_dir: Path):\n",
    "    agent_yaml_path = conf_dir / 'agent.yml'\n",
    "    vec_db_yaml_path = conf_dir / 'vec_db.yml'\n",
    "\n",
    "    agent_conf = yaml.safe_load(agent_yaml_path.read_text())\n",
    "    vec_db_conf = yaml.safe_load(vec_db_yaml_path.read_text())\n",
    "\n",
    "    db = ChromaClient()\n",
    "    for collection_name, docs in vec_db_conf.items():\n",
    "        db.add_collection(collection_name, exists_ok=True)\n",
    "        db.upsert(collection_name, docs)\n",
    "\n",
    "    agent_conf['db'] = db\n",
    "    \n",
    "    Agent = Rag(**agent_conf)\n",
    "\n",
    "    return Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"expected_traces\"\n",
    "trace_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"traces\"\n",
    "conf_dir = get_git_root() / \"sample_data\" / \"eval_datasets\" / \"agent_configs\"\n",
    "\n",
    "\n",
    "comparisons = {\n",
    "    'eq':eq,\n",
    "    'eval':safe_eval,\n",
    "    'cosine_dist':cosine_dist,\n",
    "    # TODO make a chat_eval where you can put the system prompt as a kwarg\n",
    "}\n",
    "\n",
    "default_comparison = cosine_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/traces/v001\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file huskies.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/traces/v002\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file huskies.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Evaluating 2 datapoints, logging to /Users/dean/dl/stringdale/sample_data/eval_datasets/traces/v003\n",
      "__main__ - INFO - Trace file pikachus.jsonl already exists, skipping agent run\n",
      "__main__ - INFO - Trace file huskies.jsonl already exists, skipping agent run\n"
     ]
    }
   ],
   "source": [
    "with checkLogs(level='INFO'):\n",
    "    ds1 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v001'),\n",
    "        expected_dir=expected_dir,\n",
    "        trace_dir=trace_dir/'v001',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison,\n",
    "        )\n",
    "\n",
    "    ds2 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v002'),\n",
    "        expected_dir=expected_dir,\n",
    "        trace_dir=trace_dir/'v002',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n",
    "    ds3 = await eval_dataset(\n",
    "        Agent=load_agent(conf_dir/'v003'),\n",
    "        expected_dir=expected_dir,\n",
    "        trace_dir=trace_dir/'v003',\n",
    "        comparisons=comparisons,\n",
    "        default_comparison=default_comparison)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalDataset(expected_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/expected_traces, \n",
       "  trace_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/traces/v001, \n",
       "  summary=Dataframe((2, 3)), \n",
       "  details=Dataframe((3, 11)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huskies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>'dog3' in [doc['id'] for doc in {}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huskies</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.655937</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx accessor   comparison  \\\n",
       "0  pikachus                0          2  content  cosine_dist   \n",
       "1   huskies                0          1        .    safe_eval   \n",
       "2   huskies                1          2  content  cosine_dist   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                         expected  distance kwargs node_name  \\\n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595     {}      chat   \n",
       "1             'dog3' in [doc['id'] for doc in {}]  0.000000     {}  get_docs   \n",
       "2        They are:\\n  * friendly\\n  * intelligent  0.655937     {}      chat   \n",
       "\n",
       "  expected_name  \n",
       "0          chat  \n",
       "1      get_docs  \n",
       "2          chat  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huskies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>'dog3' in [doc['id'] for doc in {}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huskies</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.610838</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx accessor   comparison  \\\n",
       "0  pikachus                0          2  content  cosine_dist   \n",
       "1   huskies                0          1        .    safe_eval   \n",
       "2   huskies                1          2  content  cosine_dist   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are not dogs, they are fictional crea...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  Golden Retrievers are friendly, intelligent do...   \n",
       "\n",
       "                                         expected  distance kwargs node_name  \\\n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes  0.818815     {}      chat   \n",
       "1             'dog3' in [doc['id'] for doc in {}]  0.000000     {}  get_docs   \n",
       "2        They are:\\n  * friendly\\n  * intelligent  0.610838     {}      chat   \n",
       "\n",
       "  expected_name  \n",
       "0          chat  \n",
       "1      get_docs  \n",
       "2          chat  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Pikachus are dangerous creatures that smoke to...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.443801</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huskies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>'dog3' in [doc['id'] for doc in {}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huskies</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.651285</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx accessor   comparison  \\\n",
       "0  pikachus                0          2  content  cosine_dist   \n",
       "1   huskies                0          1        .    safe_eval   \n",
       "2   huskies                1          2  content  cosine_dist   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are dangerous creatures that smoke to...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                         expected  distance kwargs node_name  \\\n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes  0.443801     {}      chat   \n",
       "1             'dog3' in [doc['id'] for doc in {}]  0.000000     {}  get_docs   \n",
       "2        They are:\\n  * friendly\\n  * intelligent  0.651285     {}      chat   \n",
       "\n",
       "  expected_name  \n",
       "0          chat  \n",
       "1      get_docs  \n",
       "2          chat  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds3.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = [{'id': 'dog1', 'text': 'The Golden Retriever is a friendly, intelligent breed known for its golden coat. They make excellent family pets and are great with children.', 'metadata': {'breed': 'Golden Retriever'}, 'distance': 0.6440681219100952}, {'id': 'dog3', 'text': 'The Golden Retriever is a friendly, intelligent breed with a beautiful golden coat. They are wonderful family pets that get along well with kids.', 'metadata': {'breed': 'Golden Retriever'}, 'distance': 0.6522300839424133}, {'id': 'dog2', 'text': 'German Shepherds are loyal, protective dogs often used in police work. They are highly trainable and good at various tasks.', 'metadata': {'breed': 'German Shepherd'}, 'distance': 1.287759780883789}, {'id': 'dog4', 'text': 'Huskies are energetic working dogs bred for cold climates. They have thick fur and often blue eyes.', 'metadata': {'breed': 'Husky'}, 'distance': 1.344233751296997}, {'id': 'dog5', 'text': 'Siberian Huskies are active working dogs that thrive in cold weather. They are known for their thick coats and striking blue eyes.', 'metadata': {'breed': 'Husky'}, 'distance': 1.465799331665039}]\n",
    "expected = \"\"\"'dog3' in [doc['id'] for doc in {}]  \"\"\"\n",
    "\n",
    "safe_eval(out,expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mv001\u001b[m\u001b[m \u001b[1m\u001b[36mv002\u001b[m\u001b[m \u001b[1m\u001b[36mv003\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls {conf_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvalDataset(expected_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/expected, \n",
       "  trace_dir=/Users/dean/dl/stringdale/sample_data/eval_datasets/traces/v001, \n",
       "  summary=Dataframe((0, 0)), \n",
       "  details=Dataframe((0, 0)), \n",
       "  debug=dict)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = eval_dataset(\n",
    "    Agent=Rag01,\n",
    "    expected_dir=expected_dir,\n",
    "    trace_dir=trace_dir/'v001',\n",
    "    comparisons=comparisons,\n",
    "    default_comparison=default_comparison)\n",
    "\n",
    "ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO later, add more metrics over traces and expected outs beyond total distance. Such as, total coverage\n",
    "# TODO see how we can make an \"in\" modifier, so we can make sure that we retreived \"close\" documents from a document list.\n",
    "# TODO add to \"V\" ability to specifiy the funcname for presenting in the drawing\n",
    "\n",
    "# TODO add a serialize and deserialize function for the DatasetEval class #rename\n",
    "# that allows you to write and load it to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that takes 2 datasetRuns and returns a comparison per datapoint on the difference between the two runs\n",
    "# then have a utility function that prints the summary\n",
    "# and have a utility function that returns the k datapoints that regressed the most\n",
    "# have a pprint version of it that actually plots the traces and the difference between them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_changes(ds1,ds2,datapoint,epsilon=1e-3):\n",
    "    \"\"\"\n",
    "    Describe the changes between two datapoints\n",
    "    \"\"\"\n",
    "\n",
    "    # take the detailed version of the datasets and limit to only rows of the given datapoint\n",
    "\n",
    "    # since these datapoints or not extended or reduced, we expect the same set of expected nodes and the same set of tuples of the type (content,comparison)\n",
    "    # lets assert this in the code\n",
    "    \n",
    "    # we make a list of changes with three types of changes:\n",
    "        # expected_nodes that changed trace\n",
    "        # comparisons that saw improvement\n",
    "        # comparisons that saw regression\n",
    "\n",
    "    # we then pass a dataframe of these changes # with a column that is the datapoint\n",
    "\n",
    "    # we then pass a dataframe of these changes # with a column that is the datapoint\n",
    "    pass\n",
    "\n",
    "\n",
    "def compare_datasets(ds1,ds2,epsilon=1e-3,metrics=None):\n",
    "    \"\"\"\n",
    "    Compare two datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO epsilons are either a fixed value, or a value per metric\n",
    "    # if metrics are none, default is \n",
    "\n",
    "    # we assume all datasets have the same set of datapoints \n",
    "    # assert it\n",
    "    # and that datapoints with the same name have identical expected outputs\n",
    "    # this we dont assert \n",
    "\n",
    "    # improved cases - non of the above and at least one metric improved by more than epsilon and the no metric worsened\n",
    "    # regressed cases - non of the above and at least one metric worsened by more than epsilon and the no metric improved\n",
    "\n",
    "    # changed cases - non of the above, some metrics improved and some worsened by more than epsilon\n",
    "    # unchanged cases - non of the above, no metrics improved or worsened by more than epsilon\n",
    "\n",
    "\n",
    "    # for each case in improved, regressed, and changed, we want to get a detailed summary of the changes\n",
    "    # we compute a detailed summary of the changes for each datapoint\n",
    "    # and then we return one big dataframe of comparison level conflicts\n",
    "    # by concatenating the datapoint level summaries and marking them a column of the correct tag (imporved,regressed,changed)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add num expected and numtraces to summary\n",
    "# TODO and from that derive coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>I'm sorry, but I can only provide information ...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.943595</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huskies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>'dog3' in [doc['id'] for doc in {}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huskies</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>The Golden Retriever is a friendly, intelligen...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.655937</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx accessor   comparison  \\\n",
       "0  pikachus                0          2  content  cosine_dist   \n",
       "1   huskies                0          1        .    safe_eval   \n",
       "2   huskies                1          2  content  cosine_dist   \n",
       "\n",
       "                                              actual  \\\n",
       "0  I'm sorry, but I can only provide information ...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  The Golden Retriever is a friendly, intelligen...   \n",
       "\n",
       "                                         expected  distance kwargs node_name  \\\n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes  0.943595     {}      chat   \n",
       "1             'dog3' in [doc['id'] for doc in {}]  0.000000     {}  get_docs   \n",
       "2        They are:\\n  * friendly\\n  * intelligent  0.655937     {}      chat   \n",
       "\n",
       "  expected_name  \n",
       "0          chat  \n",
       "1      get_docs  \n",
       "2          chat  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datapoint</th>\n",
       "      <th>expected_node_id</th>\n",
       "      <th>trace_idx</th>\n",
       "      <th>accessor</th>\n",
       "      <th>comparison</th>\n",
       "      <th>actual</th>\n",
       "      <th>expected</th>\n",
       "      <th>distance</th>\n",
       "      <th>kwargs</th>\n",
       "      <th>node_name</th>\n",
       "      <th>expected_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pikachus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Pikachus are not dogs, they are fictional crea...</td>\n",
       "      <td>They are:\\n  * dangerous\\n  * smoke cigarettes</td>\n",
       "      <td>0.818815</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>huskies</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>safe_eval</td>\n",
       "      <td>[{'id': 'dog1', 'text': 'The Golden Retriever ...</td>\n",
       "      <td>'dog3' in [doc['id'] for doc in {}]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>{}</td>\n",
       "      <td>get_docs</td>\n",
       "      <td>get_docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>huskies</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>content</td>\n",
       "      <td>cosine_dist</td>\n",
       "      <td>Golden Retrievers are friendly, intelligent do...</td>\n",
       "      <td>They are:\\n  * friendly\\n  * intelligent</td>\n",
       "      <td>0.610838</td>\n",
       "      <td>{}</td>\n",
       "      <td>chat</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datapoint expected_node_id  trace_idx accessor   comparison  \\\n",
       "0  pikachus                0          2  content  cosine_dist   \n",
       "1   huskies                0          1        .    safe_eval   \n",
       "2   huskies                1          2  content  cosine_dist   \n",
       "\n",
       "                                              actual  \\\n",
       "0  Pikachus are not dogs, they are fictional crea...   \n",
       "1  [{'id': 'dog1', 'text': 'The Golden Retriever ...   \n",
       "2  Golden Retrievers are friendly, intelligent do...   \n",
       "\n",
       "                                         expected  distance kwargs node_name  \\\n",
       "0  They are:\\n  * dangerous\\n  * smoke cigarettes  0.818815     {}      chat   \n",
       "1             'dog3' in [doc['id'] for doc in {}]  0.000000     {}  get_docs   \n",
       "2        They are:\\n  * friendly\\n  * intelligent  0.610838     {}      chat   \n",
       "\n",
       "  expected_name  \n",
       "0          chat  \n",
       "1      get_docs  \n",
       "2          chat  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2.details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO when pprinting comparisons, give urls to files, so that its easy in nb to navigate to them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval main entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval()\n",
    "   \"\"\"\n",
    "    we take\n",
    "     - a dir with expected traced\n",
    "     - a dir to write traces to\n",
    "     - a dict of agents and their names/codes (ie v001, v002 etc)\n",
    "     - a cache_dir (if not, its a temp dir that we abandon later)\n",
    "     - a force flag (means invalidate the cache and re run the agents)\n",
    "     - a baseline version  (used to compare to), assumed to be the first in the list of agents unless otherwise specified\n",
    "     - summary_file=None if None pprint summary to console. Else save summary to file\n",
    "     - k = None, how many datapoints of each type to print to summary at most by default all\n",
    "     - silent = False, if True, dont pprint comparisons\n",
    "     - force_run = False, if True, delete the cache and re run the agents\n",
    "   \"\"\"\n",
    "\n",
    "    # TODO we make eval dataset able to load from a cachedir and run only those where the expected is not the same\n",
    "      # we check if its the same up to yaml whitespace, by comparing the yaml string after loading and serializing\n",
    "    \n",
    "    # we eval all datasets, expected over all agents concurrently\n",
    "\n",
    "    # then we run comparisons between the baseline dataset and the other datasets\n",
    "\n",
    "    # then, we pprint the summary of each comparison seperately\n",
    "\n",
    "    # then we group the per datapoint comps across all dataset comparisons by the datapoint id\n",
    "    # and for each datapoint we pprint a combined datapoint comparison.\n",
    "    # combined datapoints for each datapoint, the total metrics of each version\n",
    "    # and then for each comparison that is different from baseline, say how it is different for every version.\n",
    "\n",
    "    # we return an EvalResult object that tracks the input of the EvalData, but also has the DataSet and DataSetComp objects for each dataset and comparison\n",
    "   \n",
    "   pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add asyncio sempathores to Chat and DB operations etc, so that we dont get rate limited due to tons of async requests\n",
    "\n",
    "\"\"\" # Optional semaphore\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import Optional\n",
    "\n",
    "@asynccontextmanager\n",
    "async def optional_semaphore(semaphore: Optional[asyncio.Semaphore] = None):\n",
    "    if semaphore is not None:\n",
    "        async with semaphore:\n",
    "            yield\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "# Usage example:\n",
    "async def my_function(limit_concurrency: bool = False):\n",
    "    sem = asyncio.Semaphore(2) if limit_concurrency else None\n",
    "    \n",
    "    async with optional_semaphore(sem):\n",
    "        # Your async code here\n",
    "        await asyncio.sleep(1)\n",
    "        print(\"Function executed\")\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we do training and validation on workflows?\n",
    "\n",
    "# we have 2 expected datasets, train and test\n",
    "\n",
    "# we look at the total distance of the validation set to see that we are improving on it\n",
    "\n",
    "# but we only look at the comparisons and fix our configs or diagrams based on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO in future, specify 2 datasets (by dirs and regex)\n",
    "# one is the train one is the test\n",
    "# we take an agent (or 2 for comparison)\n",
    "# and we do the same logic for evaluating and comparing, however, we print the statistics only for the test set\n",
    "# but we show the regression for the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData model\\n\\nWe have a dataset\\n* containing tests\\n* each test has the input to the agent\\n* and the expected output\\n* test is any object that can be serialized to json\\n* expected output is a partial trace spec\\n\\n* partial trace spec is a list of steps\\n* each step has a name is a dict with accessors and value are how to check them\\n* names are the node name we expect to see in the trace\\n* the dict defines what we expect the value to look like\\n\\n\\nWhen we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\\nsince the partial trace spec does not \\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data model\n",
    "\n",
    "We have a dataset\n",
    "* containing tests\n",
    "* each test has the input to the agent\n",
    "* and the expected output\n",
    "* test is any object that can be serialized to json\n",
    "* expected output is a partial trace spec\n",
    "\n",
    "* partial trace spec is a list of steps\n",
    "* each step has a name is a dict with accessors and value are how to check them\n",
    "* names are the node name we expect to see in the trace\n",
    "* the dict defines what we expect the value to look like\n",
    "\n",
    "\n",
    "When we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\n",
    "since the partial trace spec does not \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedTrace:\n",
    "    pass\n",
    "\n",
    "class DataPointRun:\n",
    "    # basically a list of traces, agent input and agent output\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traces_from_file(file_path):\n",
    "    pass\n",
    "\n",
    "def collect_traces_from_logg_aggregator(logger):\n",
    "    pass\n",
    "\n",
    "def run_dataset(agent,dataset,output_dir):\n",
    "    # for each data point in the dataset\n",
    "    # run the agent\n",
    "    # collect the traces into a file\n",
    "    # return the file path\n",
    "    pass\n",
    "\n",
    "def write_comparison_to_file(dataset_run,expected_traces,output_dir):\n",
    "    # run the comparison and write the results to a file\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_summary(runs,dir):\n",
    "    # get the run files and the comparison files\n",
    "    # get the total metrics per expected node and total\n",
    "    # make them into a dataframe\n",
    "    pass\n",
    "\n",
    "def plot_runs(runs,dir):\n",
    "    # call runs_summary\n",
    "    # plot the results\n",
    "    pass\n",
    "\n",
    "def check_regressions(runs,dir):\n",
    "    # get two runs\n",
    "    # for each input, if the second run is worse than the first, then flag it\n",
    "    # make a dataframe of the regressions on a whole run basis\n",
    "    \n",
    "    # also make a dataframe of the regressions on a per node basis for the runs that regressed.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the DPTW to match each trace to an expected trace\n",
    "# than we have multiple scores\n",
    "    # total distance, \n",
    "    # total distance per expected trace, \n",
    "    # coverage (percent of nodes expected), \n",
    "    # time coverage (percent of time of nodes expected), used to ignore nodes with no logic\n",
    "\n",
    "\n",
    "# this experiment object can be dumped into a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Regression detection\n",
    "# here we just compare the runs to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# in the end, we want 3 entrypoints:\n",
    "\n",
    "# eval, eval_single, and align_trace\n",
    "\n",
    "# eval will get lists of versions, and which comparisons to do, and log dir to save results to etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
