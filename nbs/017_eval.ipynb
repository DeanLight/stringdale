{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stringdale import (\n",
    "    Define,\n",
    "    Scope,\n",
    "    V,\n",
    "    E,\n",
    "    Condition,\n",
    "    draw_nx\n",
    ")\n",
    "\n",
    "\n",
    "from stringdale.core import  checkLogs\n",
    "import pytest\n",
    "import asyncio\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nData model\\n\\nWe have a dataset\\n* containing tests\\n* each test has the input to the agent\\n* and the expected output\\n* test is any object that can be serialized to json\\n* expected output is a partial trace spec\\n\\n* partial trace spec is a list of steps\\n* each step has a name is a dict with accessors and value are how to check them\\n* names are the node name we expect to see in the trace\\n* the dict defines what we expect the value to look like\\n\\n\\nWhen we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\\nsince the partial trace spec does not \\n\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data model\n",
    "\n",
    "We have a dataset\n",
    "* containing tests\n",
    "* each test has the input to the agent\n",
    "* and the expected output\n",
    "* test is any object that can be serialized to json\n",
    "* expected output is a partial trace spec\n",
    "\n",
    "* partial trace spec is a list of steps\n",
    "* each step has a name is a dict with accessors and value are how to check them\n",
    "* names are the node name we expect to see in the trace\n",
    "* the dict defines what we expect the value to look like\n",
    "\n",
    "\n",
    "When we run a dataset, we take the input, run the agent, and check the output against the partial trace spec\n",
    "since the partial trace spec does not \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedTrace:\n",
    "    pass\n",
    "\n",
    "class DataPointRun:\n",
    "    # basically a list of traces, agent input and agent output\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def dynamic_partial_time_warping(trace, expected,comparisons):\n",
    "    pass\n",
    "    # this is a parallel dynamic time warping with an abstract distance metric (as opposed to a euclidean embedding)\n",
    "\n",
    "\n",
    "\n",
    "    # once we have the distance matrix,\n",
    "    #   we need to somehow get the time breaks (ie clustering the steps into groups)\n",
    "\n",
    "    # then we need to do alignment to compute the warping path\n",
    "\n",
    "    # given a set of events we need to figure out how to encode different matches of the set to traces.\n",
    "\n",
    "\n",
    "    # https://www.geeksforgeeks.org/dynamic-time-warping-dtw-in-time-series/\n",
    "    \n",
    "\n",
    "    # TODO idea, instead of sets of parrallel nodes, we could get a partial ordering of the nodes\n",
    "    # each time we try to match a node, we can only do so on the set of nodes that do not have an unmatched predecessor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_traces_from_file(file_path):\n",
    "    pass\n",
    "\n",
    "def collect_traces_from_logg_aggregator(logger):\n",
    "    pass\n",
    "\n",
    "def run_dataset(agent,dataset,output_dir):\n",
    "    # for each data point in the dataset\n",
    "    # run the agent\n",
    "    # collect the traces into a file\n",
    "    # return the file path\n",
    "    pass\n",
    "\n",
    "def write_comparison_to_file(dataset_run,expected_traces,output_dir):\n",
    "    # run the comparison and write the results to a file\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runs_summary(runs,dir):\n",
    "    # get the run files and the comparison files\n",
    "    # get the total metrics per expected node and total\n",
    "    # make them into a dataframe\n",
    "    pass\n",
    "\n",
    "def plot_runs(runs,dir):\n",
    "    # call runs_summary\n",
    "    # plot the results\n",
    "    pass\n",
    "\n",
    "def check_regressions(runs,dir):\n",
    "    # get two runs\n",
    "    # for each input, if the second run is worse than the first, then flag it\n",
    "    # make a dataframe of the regressions on a whole run basis\n",
    "    \n",
    "    # also make a dataframe of the regressions on a per node basis for the runs that regressed.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Order Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import TypeVar, Set, List, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "T = TypeVar('T')\n",
    "\n",
    "class PartialOrder:\n",
    "    def __init__(self):\n",
    "        # Store the direct less-than relations\n",
    "        self.direct_less_than: dict[T, set[T]] = defaultdict(set)\n",
    "        # Store the transitive closure of less-than relations\n",
    "        self.less_than: dict[T, set[T]] = defaultdict(set)\n",
    "        \n",
    "    def add_relation(self, smaller: T, larger: T) -> None:\n",
    "        \"\"\"Add a relation where 'smaller' is less than 'larger'\"\"\"\n",
    "        # Add direct relation\n",
    "        self.direct_less_than[smaller].add(larger)\n",
    "        \n",
    "        # Update transitive closure\n",
    "        # 1. Add direct relation\n",
    "        self.less_than[smaller].add(larger)\n",
    "        \n",
    "        # 2. Add all relations where 'smaller' is less than something that's less than 'larger'\n",
    "        for x in self.less_than[larger]:\n",
    "            self.less_than[smaller].add(x)\n",
    "            \n",
    "        # 3. Add all relations where something less than 'smaller' is less than 'larger'\n",
    "        for x in self.less_than:\n",
    "            if smaller in self.less_than[x]:\n",
    "                self.less_than[x].add(larger)\n",
    "                for y in self.less_than[larger]:\n",
    "                    self.less_than[x].add(y)\n",
    "    \n",
    "    def get_predecessors(self, element: T) -> Set[T]:\n",
    "        \"\"\"Get all elements that are less than the given element\"\"\"\n",
    "        predecessors = set()\n",
    "        for potential_pred, successors in self.less_than.items():\n",
    "            if element in successors:\n",
    "                predecessors.add(potential_pred)\n",
    "        return predecessors\n",
    "    \n",
    "    def get_successors(self, element: T) -> Set[T]:\n",
    "        \"\"\"Get all elements that are greater than the given element\"\"\"\n",
    "        return self.less_than[element]\n",
    "    \n",
    "    def get_minimal_elements(self, elements: Set[T]) -> Set[T]:\n",
    "        \"\"\"Get all minimal elements from the given set according to the partial order.\n",
    "        A minimal element has no other elements in the set that are less than it.\"\"\"\n",
    "        minimal = set(elements)\n",
    "        for element in elements:\n",
    "            # Remove any element that has a predecessor in the set\n",
    "            predecessors = self.get_predecessors(element)\n",
    "            if predecessors & elements:  # if there's any overlap with our set\n",
    "                minimal.remove(element)\n",
    "        return minimal\n",
    "    \n",
    "    def get_maximal_elements(self, elements: Set[T]) -> Set[T]:\n",
    "        \"\"\"Get all maximal elements from the given set according to the partial order.\n",
    "        A maximal element has no other elements in the set that are greater than it.\"\"\"\n",
    "        maximal = set(elements)\n",
    "        for element in elements:\n",
    "            # Remove any element that has a successor in the set\n",
    "            successors = self.get_successors(element)\n",
    "            if successors & elements:  # if there's any overlap with our set\n",
    "                maximal.remove(element)\n",
    "        return maximal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "po = PartialOrder()\n",
    "\n",
    "# Add some relations\n",
    "po.add_relation(1, 2)\n",
    "po.add_relation(2, 3)\n",
    "po.add_relation(4, 5)\n",
    "po.add_relation(5, \"joe\")\n",
    "\n",
    "# Now 1 < 3 should be true due to transitivity\n",
    "assert po.get_successors(1) == {2, 3}\n",
    "assert po.get_predecessors(3) == {1, 2}\n",
    "\n",
    "# Get minimal elements from a set\n",
    "elements = {1, 2, 3, 4, 5 }\n",
    "assert po.get_minimal_elements(elements) == {1, 4}\n",
    "\n",
    "# Get maximal elements from a set\n",
    "assert po.get_maximal_elements(elements) == {3, 5}\n",
    "\n",
    "\n",
    "some_new_elements = {1,2,3,4,5,\"jack\"}\n",
    "assert po.get_minimal_elements(some_new_elements) == {1,4,\"jack\"}\n",
    "assert po.get_maximal_elements(some_new_elements) == {3,5,\"jack\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = \"\"\"\n",
    "input:\n",
    "  content: \"hello world\"\n",
    "expected:\n",
    "  # we give the name of the trace node\n",
    "  - node_a:\n",
    "      # we describe what output we expect from the node using accessors as keys\n",
    "      # the value is what we expect the accessor to return\n",
    "      b.c: |\n",
    "        jimmy went\n",
    "        to the store\n",
    "      # we can also give a label to the node so we can refer to it later\n",
    "      # using the $label key\n",
    "      $label: node_a1\n",
    "\n",
    "  - node_b:\n",
    "      # we can give multiple comparisons to the same node, using different accessors\n",
    "      d.e:\n",
    "        value: jimmy\n",
    "        comparison: \"regex\"\n",
    "      f.g:\n",
    "        value: \"is a good boy\"\n",
    "        comparison: \"chat\"\n",
    "        kwargs:\n",
    "          case_sensitive: false\n",
    "\n",
    "  # we can also give a regex to match the node name\n",
    "  - node_.*:\n",
    "      .: \"store\"\n",
    "      # using the $parallel key we can specify that this node is expected in parallel with the previous node\n",
    "      # so we do not know which trace will be logged first\n",
    "      $parallel: true\n",
    "      $label: node_z\n",
    "\n",
    "  - node_c:\n",
    "      b.c: \"store\"\n",
    "      # we can specify more complex ordering constraints using before and after using the $label key\n",
    "      # before and after are either a label or a list of labels\n",
    "      # in this case we say that node_c should be after node_a1 and before node_z\n",
    "      $after: node_a1\n",
    "      $before: node_z\n",
    "      \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_trace = [\n",
    "    {\n",
    "        # should be ignored\n",
    "        \"name\": \"Start\",\n",
    "        \"output\": \"hello world\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_a\",\n",
    "        \"output\": {'b':{'c':\"jimmy went\\nto the store\\nto buy some milk\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node c\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": \"store is good\"\n",
    "    },\n",
    "    {\n",
    "        # shouldnt match\n",
    "        \"name\": \"node_a2\",\n",
    "        \"output\": {'b':{'d':\"store\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node_z\n",
    "        \"name\": \"node_x\",\n",
    "        \"output\": \"store\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_b\",\n",
    "        \"output\": {\n",
    "            'f':{'g':\"is a good boy\"},\n",
    "            'd':{'e':\"jimmy\"}\n",
    "            }\n",
    "    },\n",
    "    {   \n",
    "        # second option to node c, only relevant if node_* matches to node_y\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": \"store is good but not good enough\"\n",
    "    },\n",
    "    {\n",
    "        # second option to node_z\n",
    "        \"name\": \"node_y\",\n",
    "        \"output\": \"stores\"\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Expected Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from typing import Dict, Any,Optional, Union, List\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Condition(BaseModel):\n",
    "    accessor: str\n",
    "    value: Any\n",
    "    comparison: Optional[str] = None\n",
    "    kwargs: Dict[str,Any] = {}\n",
    "\n",
    "class ExpectedTraceStep(BaseModel):\n",
    "    name: str\n",
    "    label: Union[str,int]\n",
    "    conditions: List[Condition]\n",
    "    before: Optional[List[Union[str,int]]] = None\n",
    "    after: Optional[List[Union[str,int]]] = None\n",
    "\n",
    "class ExpectedTrace(BaseModel):\n",
    "    input: Any\n",
    "    expected: List[ExpectedTraceStep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace(yaml_str: str) -> ExpectedTrace:\n",
    "    if isinstance(yaml_str,Path):\n",
    "        yaml_string = yaml_str.read_text()\n",
    "    else:\n",
    "        yaml_string = yaml_str\n",
    "    \n",
    "    try:\n",
    "        yaml_obj = yaml.safe_load(yaml_string)\n",
    "    except Exception as e:\n",
    "        raise SyntaxError(f\"Error parsing yaml:\\n{yaml_string}\\n{e}\")\n",
    "\n",
    "    if list(yaml_obj.keys()) != [\"input\",\"expected\"]:\n",
    "        raise SyntaxError(f\"Expected keys in main scope are 'input' and 'expected', got {yaml_obj.keys()}\")\n",
    "\n",
    "    input = yaml_obj[\"input\"]\n",
    "    expected = yaml_obj[\"expected\"]\n",
    "\n",
    "    parsed_steps = []\n",
    "    for i,expected_step in enumerate(expected):\n",
    "        try:\n",
    "            parsed_steps.append(parse_expected_trace_step(expected_step,i))\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Error parsing expected trace step:\\n{expected_step}\") from e\n",
    "    return ExpectedTrace(input=input,expected=parsed_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace_step(yaml_obj: Dict[str,Any],idx:int) -> ExpectedTraceStep:\n",
    "    if len(yaml_obj.keys()) != 1:\n",
    "        raise SyntaxError(f\"Expected a single key in trace step {idx}, got {yaml_obj.keys()}\")\n",
    "    \n",
    "    name = list(yaml_obj.keys())[0]\n",
    "    value = yaml_obj[name]\n",
    "    label = value.pop(\"$label\",None)\n",
    "    if label is None:\n",
    "        label = idx\n",
    "\n",
    "    before = value.pop(\"$before\",list())\n",
    "    if isinstance(before,str):\n",
    "        before = [before]\n",
    "    after = value.pop(\"$after\",list())\n",
    "    if isinstance(after,str):\n",
    "        after = [after]\n",
    "    parallel = value.pop(\"$parallel\",False)\n",
    "\n",
    "    if parallel and idx == 0:\n",
    "        raise ValueError(f\"Expected trace step {idx} is has $parallel: true, but is the first step\")\n",
    "\n",
    "    if not parallel:\n",
    "        after.append(idx-1)\n",
    "    \n",
    "    conditions = []\n",
    "    for accessor,params in value.items():\n",
    "        if isinstance(params,str):\n",
    "            params = {\"value\":params}\n",
    "        condition_data ={\n",
    "            'accessor':accessor,\n",
    "            **params\n",
    "        }\n",
    "        try:\n",
    "            conditions.append(Condition.model_validate(condition_data))\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"When parsing condition {value} for step {idx}\") from e\n",
    "    \n",
    "    return ExpectedTraceStep(name=name,label=label,conditions=conditions,before=before,after=after)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input:\n",
      "  content: \"hello world\"\n",
      "expected:\n",
      "  # we give the name of the trace node\n",
      "  - node_a:\n",
      "      # we describe what output we expect from the node using accessors as keys\n",
      "      # the value is what we expect the accessor to return\n",
      "      b.c: |\n",
      "        jimmy went\n",
      "        to the store\n",
      "      # we can also give a label to the node so we can refer to it later\n",
      "      # using the $label key\n",
      "      $label: node_a1\n",
      "\n",
      "  - node_b:\n",
      "      # we can give multiple comparisons to the same node, using different accessors\n",
      "      d.e:\n",
      "        value: jimmy\n",
      "        comparison: \"regex\"\n",
      "      f.g:\n",
      "        value: \"is a good boy\"\n",
      "        comparison: \"chat\"\n",
      "        kwargs:\n",
      "          case_sensitive: false\n",
      "\n",
      "  # we can also give a regex to match the node name\n",
      "  - node_.*:\n",
      "      .: \"store\"\n",
      "      # using the $parallel key we can specify that this node is expected in parallel with the previous node\n",
      "      # so we do not know which trace will be logged first\n",
      "      $parallel: true\n",
      "      $label: node_z\n",
      "\n",
      "  - node_c:\n",
      "      b.c: \"store\"\n",
      "      # we can specify more complex ordering constraints using before and after using the $label key\n",
      "      # before and after are either a label or a list of labels\n",
      "      # in this case we say that node_c should be after node_a1 and before node_z\n",
      "      $after: node_a1\n",
      "      $before: node_z\n",
      "      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'content': 'hello world'},\n",
       " 'expected': [{'name': 'node_a',\n",
       "   'label': 'node_a1',\n",
       "   'conditions': [{'accessor': 'b.c',\n",
       "     'value': 'jimmy went\\nto the store\\n',\n",
       "     'comparison': None,\n",
       "     'kwargs': {}}],\n",
       "   'before': [],\n",
       "   'after': [-1]},\n",
       "  {'name': 'node_b',\n",
       "   'label': 1,\n",
       "   'conditions': [{'accessor': 'd.e',\n",
       "     'value': 'jimmy',\n",
       "     'comparison': 'regex',\n",
       "     'kwargs': {}},\n",
       "    {'accessor': 'f.g',\n",
       "     'value': 'is a good boy',\n",
       "     'comparison': 'chat',\n",
       "     'kwargs': {'case_sensitive': False}}],\n",
       "   'before': [],\n",
       "   'after': [0]},\n",
       "  {'name': 'node_.*',\n",
       "   'label': 'node_z',\n",
       "   'conditions': [{'accessor': '.',\n",
       "     'value': 'store',\n",
       "     'comparison': None,\n",
       "     'kwargs': {}}],\n",
       "   'before': [],\n",
       "   'after': []},\n",
       "  {'name': 'node_c',\n",
       "   'label': 3,\n",
       "   'conditions': [{'accessor': 'b.c',\n",
       "     'value': 'store',\n",
       "     'comparison': None,\n",
       "     'kwargs': {}}],\n",
       "   'before': ['node_z'],\n",
       "   'after': ['node_a1', 2]}]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = parse_expected_trace(example_yaml)\n",
    "\n",
    "x.model_dump()\n",
    "\n",
    "# TODO fix loading issue in node_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Partial Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Any, Dict, Callable\n",
    "import itertools as it\n",
    "import re\n",
    "from stringdale.mappings import access_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_trace_distance(trace,expected,comparisons,default_comparison):\n",
    "    if not re.match(expected.name, trace.name):\n",
    "        return np.inf\n",
    "    \n",
    "    # check if all accessors are in the trace\n",
    "    for condition in expected.conditions:\n",
    "        if access_object(trace.output,condition.accessor) is None:\n",
    "            return np.inf\n",
    "\n",
    "    distance = 0\n",
    "    for condition in expected.conditions:\n",
    "        condition_func = comparisons.get(condition.comparison, default_comparison)\n",
    "        output_sub_value = access_object(trace.output,condition.accessor)\n",
    "        try:\n",
    "            condition_distance = condition_func(output_sub_value, condition.value, **condition.kwargs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\n",
    "                f\"Error computing distance for condition {condition}\\n\"\n",
    "                f\"trace: {trace.name}\\n\"\n",
    "                f\"expected: {expected.name}({expected.label})\\n\"\n",
    "                f\"actual: {output_sub_value}\"\n",
    "                ) from e\n",
    "        distance += condition_distance\n",
    "    \n",
    "    return distance\n",
    "\n",
    "def compute_distance_matrix(\n",
    "    traces_outputs:List[Any],\n",
    "    expected_traces:ExpectedTrace,\n",
    "    comparisons:Dict[str,Callable],\n",
    "    default_comparison:Callable):\n",
    "    \"\"\"\n",
    "    Compute the distance matrix between the traces and the expected traces.\n",
    "\n",
    "    Args:\n",
    "        traces_outputs: List[Any], the outputs of the traces\n",
    "        expected_traces: ExpectedTrace, the expected traces\n",
    "        comparisons: Dict[str,Callable], the comparisons to use for the distance matrix\n",
    "        default_comparison: Callable, the default comparison to use for the distance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = np.zeros((len(traces_outputs), len(expected_traces)))\n",
    "    \n",
    "    for (i, trace), (j, expected) in it.product(enumerate(traces_outputs), enumerate(expected_traces)):\n",
    "        distances[i,j] = compute_trace_distance(trace,expected,comparisons,default_comparison)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_warping_path(distances:np.ndarray,po:PartialOrder):\n",
    "    \"\"\"\n",
    "    Find the warping path between the traces and the expected traces.\n",
    "\n",
    "    Args:\n",
    "        distances: np.ndarray, the distance matrix between the traces and the expected traces\n",
    "        po: PartialOrder, the partial order of the expected traces, assumed po to be over excepted traces indices\n",
    "\n",
    "    \"\"\"\n",
    "    # find the minimal elements of the expected traces\n",
    "    minimal_elements = po.get_minimal_elements(set(range(len(expected_traces))))\n",
    "\n",
    "    # build the matching with poset constraints as a CSP problem using python-constraint2\n",
    "    # add make sure we give each expected trace the domain over elements that is can actually match to\n",
    "    # so no inf distances\n",
    "\n",
    "    # iterate over all solution and use the distance matrix to find the best match\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input:\n",
      "  content: \"hello world\"\n",
      "expected:\n",
      "  - node_a:\n",
      "      b.c: |\n",
      "        jimmy went\n",
      "        to the store\n",
      "      $label: node_a1\n",
      "\n",
      "  - node_b:\n",
      "      d.e:\n",
      "        value: jimmy\n",
      "        comparison: \"regex\"\n",
      "      f.g:\n",
      "        value: \"is a good boy\"\n",
      "        comparison: \"chat\"\n",
      "        kwargs:\n",
      "          case_sensitive: false\n",
      "\n",
      "  - node_.*:\n",
      "      .: \"store\"\n",
      "      $parallel: true\n",
      "\n",
      "  - node_c:\n",
      "      b.c: \"store\"\n",
      "      $before: node_a1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(example_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "\n",
    "D(i,j,hist) = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO from here, make an example of trace and expected trace with length, equality and regex matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, get a list of traces, a dict of the exected traces (maybe keys by label if exists or by index), and a partial ordering of the expected traces\n",
    "\n",
    "# compute the distance matrix between the traces and the expected traces\n",
    "\n",
    "# make the warping path, but rather than a numberic matrix, we need a mapping of who was mapped to who."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dynamic warping\n",
    "# we make a plot like https://pypi.org/project/dtaidistance/ maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input': {'content': 'hello world'},\n",
       "  'expected': [{'node_a': {'b.c': 'jimmy went\\nto the store\\n'}},\n",
       "   {'node_b': {'d.e': {'value': 'jimmy', 'comparison': 'regex'},\n",
       "     'f.g': {'value': 'is a good boy',\n",
       "      'comparison': 'chat',\n",
       "      'kwargs': {'case_sensitive': False}}}},\n",
       "   {'node_.*': {'.': 'store', '$parallel': True}}]},\n",
       " {'input': {'content': 'Go to the store and buy some milk'},\n",
       "  'expected': [{'node_a': {'b.c': 'store'}}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml.safe_load(example_yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO start with directories of files with traces.\n",
    "# here we just run the agent on the input and collect the traces to files\n",
    "# Later, add a way to customize the runs from a logger or something \n",
    "# I think the best way would be to be able to turn the logs into a dataset file and work on it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the DPTW to match each trace to an expected trace\n",
    "# than we have multiple scores\n",
    "    # total distance, \n",
    "    # total distance per expected trace, \n",
    "    # coverage (percent of nodes expected), \n",
    "    # time coverage (percent of time of nodes expected), used to ignore nodes with no logic\n",
    "\n",
    "\n",
    "# this experiment object can be dumped into a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Regression detection\n",
    "# here we just compare the runs to each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
