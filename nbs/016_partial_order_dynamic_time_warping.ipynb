{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Order Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp podtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "from frozendict import frozendict\n",
    "from collections import defaultdict\n",
    "\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import List, Any, Dict, Callable,Set, Optional\n",
    "\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import re\n",
    "import asyncio\n",
    "\n",
    "from constraint import Problem,FunctionConstraint\n",
    "from bidict import bidict\n",
    "\n",
    "import logging \n",
    "from stringdale.core import checkLogs\n",
    "from stringdale.mappings import access_object, parse_edge_descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping labels to fresh variables for CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def int_to_excel_col(n):\n",
    "    if n < 0:\n",
    "        raise ValueError(\"Number must be non-negative\")\n",
    "    \n",
    "    result = \"\"\n",
    "    n += 1  # Adjust because Excel columns start at 1, not 0\n",
    "    \n",
    "    while n > 0:\n",
    "        n -= 1  # Adjust for 0-based indexing\n",
    "        result = chr(n % 26 + ord('A')).lower() + result\n",
    "        n //= 26\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert int_to_excel_col(0) == \"a\"\n",
    "assert int_to_excel_col(25) == \"z\"\n",
    "assert int_to_excel_col(26) == \"aa\"\n",
    "assert int_to_excel_col(27) == \"ab\"\n",
    "assert int_to_excel_col(51) == \"az\"\n",
    "assert int_to_excel_col(52) == \"ba\"\n",
    "assert int_to_excel_col(701) == \"zz\"\n",
    "assert int_to_excel_col(702) == \"aaa\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LabelToVar():\n",
    "    def __init__(self):\n",
    "        self.label_to_var = bidict()\n",
    "        self.label_to_index = bidict()\n",
    "\n",
    "    def add_label(self,label:str,idx:int):\n",
    "        self.label_to_var[label] = int_to_excel_col(idx)\n",
    "        self.label_to_index[label] = idx\n",
    "\n",
    "    def get_label(self,col:str) -> str:\n",
    "        return self.label_to_var.inverse[col]\n",
    "\n",
    "    def get_index(self,label:str) -> int:\n",
    "        return self.label_to_index[label]\n",
    "\n",
    "    def get_col(self,label:str) -> int:\n",
    "        return self.label_to_var[label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_var = LabelToVar()\n",
    "label_to_var.add_label(\"x\",0)\n",
    "label_to_var.add_label(\"y\",1)\n",
    "label_to_var.add_label(\"z\",2)\n",
    "\n",
    "assert label_to_var.get_col(\"x\") == \"a\"\n",
    "assert label_to_var.get_col(\"y\") == \"b\"\n",
    "assert label_to_var.get_col(\"z\") == \"c\"\n",
    "\n",
    "assert label_to_var.get_label(\"a\") == \"x\"\n",
    "assert label_to_var.get_label(\"b\") == \"y\"\n",
    "assert label_to_var.get_label(\"c\") == \"z\"\n",
    "\n",
    "assert label_to_var.get_index(\"x\") == 0\n",
    "assert label_to_var.get_index(\"y\") == 1\n",
    "assert label_to_var.get_index(\"z\") == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock comparison functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "async def word_overlap(result: str, expected: str,**kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the distance between result and expected strings based on word overlap.\n",
    "    Returns a value between 0 and 1, where:\n",
    "    - 0 means perfect match (all words from result are in expected)\n",
    "    - 1 means no overlap (no words from result are in expected)\n",
    "    \n",
    "    Args:\n",
    "        result (str): The string to check words from\n",
    "        expected (str): The string to check words against\n",
    "        \n",
    "    Returns:\n",
    "        float: Distance metric between 0 and 1\n",
    "    \"\"\"\n",
    "    if not isinstance(result,str) or not isinstance(expected,str):\n",
    "        return np.inf\n",
    "    # Convert both strings to lowercase and split into words\n",
    "    result_words = set(result.lower().split())\n",
    "    expected_words = set(expected.lower().split())\n",
    "    \n",
    "    # If result is empty, return 1.0 (maximum distance)\n",
    "    if not result_words:\n",
    "        return 1.0\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = len(result_words.intersection(expected_words))\n",
    "    total = len(result_words)\n",
    "    \n",
    "    # Calculate distance (1 - percentage)\n",
    "    distance = 1.0 - (overlap / total)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "result = \"The quick brown fox\"\n",
    "expected = \"The lazy brown dog\"\n",
    "assert await word_overlap(result, expected) == 0.5  # Output: 0.5 (2 out of 4 words match)\n",
    "\n",
    "# Example 2\n",
    "result = \"Hello world\"\n",
    "expected = \"Hello there world\"\n",
    "assert await word_overlap(result, expected) == 0.0  # Output: 0.0 (all words match)\n",
    "\n",
    "# Example 3\n",
    "result = \"Python programming\"\n",
    "expected = \"Java development\"\n",
    "assert await word_overlap(result, expected) == 1.0  # Output: 1.0 (no words match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def regex(out: str, expected: str,**kwargs) -> float:\n",
    "    \"\"\"\n",
    "    Compare a string against a regex pattern.\n",
    "    Returns 0 if the regex matches, 1 if it doesn't.\n",
    "    \n",
    "    Args:\n",
    "        out (str): The string to check\n",
    "        expected (str): The regex pattern to match against\n",
    "        \n",
    "    Returns:\n",
    "        float: 0 if match, 1 if no match\n",
    "    \"\"\"\n",
    "    if not isinstance(out,str) or not isinstance(expected,str):\n",
    "        return np.inf\n",
    "    try:\n",
    "        if re.search(expected, out,flags=re.IGNORECASE) is not None:\n",
    "            return 0.0\n",
    "        return 1.0\n",
    "    except Exception:\n",
    "        return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic matching\n",
    "assert regex(\"hello world\", \"hello\") == 0.0  # Simple substring match\n",
    "assert regex(\"hello world\", \"^hello\") == 0.0  # Start anchor\n",
    "assert regex(\"hello world\", \"world$\") == 0.0  # End anchor\n",
    "assert regex(\"hello world\", \"hello.*world\") == 0.0  # Pattern with wildcard\n",
    "\n",
    "# Non-matching\n",
    "assert regex(\"hello world\", \"goodbye\") == 1.0  # No match\n",
    "assert regex(\"hello world\", \"^world\") == 1.0  # Wrong position\n",
    "assert regex(\"hello world\", \"hello$\") == 1.0  # Wrong position with anchor\n",
    "\n",
    "# Pattern errors and edge cases\n",
    "assert regex(\"hello world\", \"(unclosed\") == 1.0  # Invalid regex pattern\n",
    "assert regex(\"hello world\", \"\") == 0.0  # Empty pattern matches anything\n",
    "assert regex(\"\", \".*\") == 0.0  # Empty string matches wildcard\n",
    "assert regex(\"\", \"\") == 0.0  # Empty string matches empty pattern\n",
    "\n",
    "# Case sensitivity\n",
    "assert regex(\"Hello World\", \"hello\") == 0.0  # Case-insensitive by default\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_yaml = \"\"\"\n",
    "input:\n",
    "  content: \"hello world\"\n",
    "expected:\n",
    "  # we give the name of the trace node\n",
    "  - node_a:\n",
    "      # we describe what output we expect from the node using accessors as keys\n",
    "      # the value is what we expect the accessor to return\n",
    "      b.c: |\n",
    "        jimmy went\n",
    "        to the store\n",
    "      # we can also give a label to the node so we can refer to it later\n",
    "      # using the $label key\n",
    "      $label: node_a1\n",
    "\n",
    "  - node_b:\n",
    "      # we can give multiple comparisons to the same node, using different accessors\n",
    "      d.e:\n",
    "        value: jimmy\n",
    "        comparison: \"regex\"\n",
    "      f.g:\n",
    "        value: \"is a good boy\"\n",
    "        comparison: \"chat\"\n",
    "        kwargs:\n",
    "          case_sensitive: false\n",
    "\n",
    "  # we can also give a regex to match the node name\n",
    "  - node_.*:\n",
    "      .: \"store\"\n",
    "      # using the $parallel key we can specify that this node is expected in parallel with the previous node\n",
    "      # so we do not know which trace will be logged first\n",
    "      $parallel: true\n",
    "      $label: node_z\n",
    "\n",
    "  - node_c:\n",
    "      b.c: \"store\"\n",
    "      # we can specify more complex ordering constraints using before and after using the $label key\n",
    "      # before and after are either a label or a list of labels\n",
    "      # in this case we say that node_c should be after node_a1 and before node_z\n",
    "      $after: node_a1\n",
    "      $before: node_z\n",
    "      \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_trace = [\n",
    "    {\n",
    "        # should be ignored\n",
    "        \"name\": \"Start\",\n",
    "        \"output\": \"hello world\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_a\",\n",
    "        \"output\": {'b':{'c':\"jimmy went\\nto the store\\nto buy some milk\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node c\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": {'b':{'c':\"store is good\"}}\n",
    "    },\n",
    "    {\n",
    "        # shouldnt match\n",
    "        \"name\": \"node_a2\",\n",
    "        \"output\": {'b':{'d':\"store\"}}\n",
    "    },\n",
    "    {\n",
    "        # first option to node_z\n",
    "        \"name\": \"node_x\",\n",
    "        \"output\": \"store\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"node_b\",\n",
    "        \"output\": {\n",
    "            'f':{'g':\"is a good boy\"},\n",
    "            'd':{'e':\"jimmy\"}\n",
    "            }\n",
    "    },\n",
    "    {   \n",
    "        # second option to node c, only relevant if node_* matches to node_y\n",
    "        \"name\": \"node_c\",\n",
    "        \"output\": {'b':{'c':\"store is good but not good enough\"}}\n",
    "    },\n",
    "    {\n",
    "        # second option to node_z\n",
    "        \"name\": \"node_y\",\n",
    "        \"output\": \"stores\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# expected labels: [noda_a1,1 (b) ,node_z,3 (c)] # c needs to be before z\n",
    "# a:[1], c:[2,6] , b:[5] z:[4,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_mappings = {\n",
    "    frozendict({'node_a1':1,'1':5,'node_z':4,'3':2}), \n",
    "    frozendict({'node_a1':1,'1':5,'node_z':7,'3':2}),\n",
    "    frozendict({'node_a1':1,'1':5,'node_z':7,'3':6}),\n",
    "}\n",
    "\n",
    "best_mapping = frozendict({'node_a1':1,'1':5,'node_z':4,'3':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Expected Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "from typing import Dict, Any,Optional, Union, List\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Condition(BaseModel):\n",
    "    accessor: tuple[str, ...]\n",
    "    value: Any\n",
    "    comparison: Optional[str] = None\n",
    "    kwargs: Dict[str,Any] = {}\n",
    "\n",
    "class ExpectedTraceStep(BaseModel):\n",
    "    name: str\n",
    "    label: Union[str,int]\n",
    "    conditions: List[Condition]\n",
    "    before: Optional[List[Union[str,int]]] = None\n",
    "    after: Optional[List[Union[str,int]]] = None\n",
    "\n",
    "class ExpectedTrace(BaseModel):\n",
    "    input: List[Any]\n",
    "    expected: List[ExpectedTraceStep]\n",
    "\n",
    "class Trace(BaseModel):\n",
    "    model_config = ConfigDict(extra='allow')\n",
    "    name: str\n",
    "    output: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace_step(yaml_obj: Dict[str,Any],idx:int,labels:List[str]) -> ExpectedTraceStep:\n",
    "    if len(yaml_obj.keys()) != 1:\n",
    "        raise SyntaxError(f\"Expected a single key in trace step {idx}, got {yaml_obj.keys()}\")\n",
    "    \n",
    "    name = list(yaml_obj.keys())[0]\n",
    "    value = yaml_obj[name]\n",
    "    label = value.pop(\"$label\",None)\n",
    "    if label is None:\n",
    "        label = str(idx)\n",
    "\n",
    "    before = value.pop(\"$before\",list())\n",
    "    if isinstance(before,str):\n",
    "        before = [before]\n",
    "    after = value.pop(\"$after\",list())\n",
    "    if isinstance(after,str):\n",
    "        after = [after]\n",
    "    parallel = value.pop(\"$parallel\",False)\n",
    "\n",
    "    if parallel and idx == 0:\n",
    "        raise ValueError(f\"Expected trace step {idx} is has $parallel: true, but is the first step\")\n",
    "\n",
    "    if not parallel and len(after) == 0 and idx > 0:\n",
    "        after.append(labels[-1])\n",
    "    \n",
    "    conditions = []\n",
    "    for accessor,params in value.items():\n",
    "        if isinstance(params,str):\n",
    "            params = {\"value\":params}\n",
    "        try:\n",
    "            accessor = parse_edge_descriptor(accessor,start='accessor')\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Error parsing accessor {accessor} for step {idx}. Make sure it is formatted correctly\") from e\n",
    "        condition_data ={\n",
    "            'accessor':accessor,\n",
    "            **params\n",
    "        }\n",
    "        try:\n",
    "            conditions.append(Condition.model_validate(condition_data))\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"When parsing condition {value} for step {idx}\") from e\n",
    "    \n",
    "    return ExpectedTraceStep(name=name,label=label,conditions=conditions,before=before,after=after)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_c': {'b.c': 'store', '$after': 'node_a1', '$before': 'node_z'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_obj = yaml.safe_load(example_yaml)\n",
    "sub_yaml = yaml_obj['expected'][-1]\n",
    "sub_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed =  parse_expected_trace_step(sub_yaml,0,labels=[])\n",
    "expected = ExpectedTraceStep(name='node_c', label=\"0\", conditions=[Condition(accessor=('b','c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])\n",
    "assert parsed == expected, parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Trace(name='Start', output='hello world'),\n",
       " Trace(name='node_a', output={'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good'}}),\n",
       " Trace(name='node_a2', output={'b': {'d': 'store'}}),\n",
       " Trace(name='node_x', output='store'),\n",
       " Trace(name='node_b', output={'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good but not good enough'}}),\n",
       " Trace(name='node_y', output='stores')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_traces = [Trace.model_validate(trace) for trace in example_trace]\n",
    "parsed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def parse_expected_trace(yaml_str: str) -> ExpectedTrace:\n",
    "    if isinstance(yaml_str,Path):\n",
    "        yaml_string = yaml_str.read_text()\n",
    "    else:\n",
    "        yaml_string = yaml_str\n",
    "    \n",
    "    try:\n",
    "        yaml_obj = yaml.safe_load(yaml_string)\n",
    "    except Exception as e:\n",
    "        raise SyntaxError(f\"Error parsing yaml:\\n{yaml_string}\\n{e}\")\n",
    "\n",
    "    if list(yaml_obj.keys()) != [\"input\",\"expected\"]:\n",
    "        raise SyntaxError(f\"Expected keys in main scope are 'input' and 'expected', got {yaml_obj.keys()}\")\n",
    "\n",
    "    input = yaml_obj[\"input\"]\n",
    "    if not isinstance(input,list):\n",
    "        input = [input]\n",
    "    expected = yaml_obj[\"expected\"]\n",
    "\n",
    "    parsed_steps = []\n",
    "    labels = []\n",
    "    for i,expected_step in enumerate(expected):\n",
    "        try:\n",
    "            step = parse_expected_trace_step(expected_step,i,labels)\n",
    "            parsed_steps.append(step)\n",
    "            labels.append(step.label)\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Error parsing expected trace step:\\n{expected_step}\") from e\n",
    "    return ExpectedTrace(input=input,expected=parsed_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExpectedTraceStep(name='node_a', label='node_a1', conditions=[Condition(accessor=('b', 'c'), value='jimmy went\\nto the store\\n', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_b', label='1', conditions=[Condition(accessor=('d', 'e'), value='jimmy', comparison='regex', kwargs={}), Condition(accessor=('f', 'g'), value='is a good boy', comparison='chat', kwargs={'case_sensitive': False})], before=[], after=['node_a1']),\n",
       " ExpectedTraceStep(name='node_.*', label='node_z', conditions=[Condition(accessor=('.',), value='store', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_c', label='3', conditions=[Condition(accessor=('b', 'c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected = parse_expected_trace(example_yaml)\n",
    "\n",
    "expected = parsed_expected.expected\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Partial Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from stringdale.core import maybe_await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def compute_trace_distance(trace,expected,comparisons,default_comparison):\n",
    "\n",
    "    logger.debug(f\"Computing distance for trace {trace} and expected {expected}\")\n",
    "    if not re.search(expected.name, trace.name):\n",
    "        return None,[]\n",
    "    \n",
    "    # check if all accessors are in the trace\n",
    "    for condition in expected.conditions:\n",
    "        try: \n",
    "            sub_object = access_object(trace.output,condition.accessor)\n",
    "        except Exception as e:\n",
    "            return None, []\n",
    "\n",
    "    distance = 0\n",
    "    debug_info = []\n",
    "    for condition in expected.conditions:\n",
    "        condition_func = comparisons.get(condition.comparison, default_comparison)\n",
    "        output_sub_value = access_object(trace.output,condition.accessor)\n",
    "        try:\n",
    "            condition_distance = await maybe_await(condition_func,args=[output_sub_value, condition.value],kwargs=condition.kwargs)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error computing distance for condition {condition} on trace {trace.name}: {e}\") from e\n",
    "        distance += condition_distance\n",
    "        debug_info.append({\n",
    "            \"comparison\": condition_func.__qualname__,\n",
    "            \"kwargs\": condition.kwargs,\n",
    "            \"expected\": condition.value,\n",
    "            \"actual\": output_sub_value,\n",
    "            \"distance\": condition_distance,\n",
    "        })\n",
    "    \n",
    "    return distance,debug_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def compute_distances(\n",
    "    traces_outputs:List[Any],\n",
    "    expected_trace:ExpectedTrace,\n",
    "    comparisons:Dict[str,Callable],\n",
    "    default_comparison:Callable):\n",
    "    \"\"\"\n",
    "    Compute the distance matrix between the traces and the expected traces.\n",
    "\n",
    "    Args:\n",
    "        traces_outputs: List[Any], the outputs of the traces\n",
    "        expected_traces: ExpectedTrace, the expected traces\n",
    "        comparisons: Dict[str,Callable], the comparisons to use for the distance matrix\n",
    "        default_comparison: Callable, the default comparison to use for the distance matrix\n",
    "    \"\"\"\n",
    "    expected_steps = expected_trace.expected\n",
    "    distances = defaultdict(dict)\n",
    "    debug_info = defaultdict(dict)\n",
    "    \n",
    "    a_iter = list(it.product(enumerate(traces_outputs), enumerate(expected_steps)))\n",
    "    tasks = [\n",
    "        compute_trace_distance(trace,expected,comparisons,default_comparison)\n",
    "        for (i, trace), (j, expected) in a_iter\n",
    "    ]\n",
    "    distance_list = await asyncio.gather(*tasks)\n",
    "    \n",
    "    for ((i, trace), (j, expected)), (d,debug) in zip(a_iter, distance_list):\n",
    "        if not d == None:\n",
    "            if not d == np.inf:\n",
    "                distances[expected.label][i] = d\n",
    "            debug_info[expected.label][i]={\n",
    "                'comparisons':debug,\n",
    "                'distance':d,\n",
    "                'expected_idx':j,\n",
    "                'actual_idx':i,\n",
    "                'actual_name':trace.name,\n",
    "                'expected_name':expected.name,\n",
    "                'expected_label':expected.label,\n",
    "            }\n",
    "\n",
    "    return dict(distances),dict(debug_info)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Trace(name='Start', output='hello world'),\n",
       " Trace(name='node_a', output={'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good'}}),\n",
       " Trace(name='node_a2', output={'b': {'d': 'store'}}),\n",
       " Trace(name='node_x', output='store'),\n",
       " Trace(name='node_b', output={'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}}),\n",
       " Trace(name='node_c', output={'b': {'c': 'store is good but not good enough'}}),\n",
       " Trace(name='node_y', output='stores')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_a1': {1: 0.375},\n",
       " '3': {2: 0.6666666666666667, 6: 0.8333333333333334},\n",
       " 'node_z': {4: 0.0, 7: 1.0},\n",
       " '1': {5: 0.0}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons = {\n",
    "    \"regex\": regex,\n",
    "    \"word_overlap\": word_overlap,\n",
    "}\n",
    "default_comparison = word_overlap\n",
    "# with checkLogs():\n",
    "dist,debug_info = await compute_distances(parsed_traces,parsed_expected,comparisons,default_comparison)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_a1': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy went\\nto the store\\n',\n",
       "     'actual': 'jimmy went\\nto the store\\nto buy some milk',\n",
       "     'distance': 0.375}],\n",
       "   'distance': 0.375,\n",
       "   'expected_idx': 0,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_a',\n",
       "   'expected_label': 'node_a1'}},\n",
       " 'node_z': {1: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'jimmy went\\nto the store\\nto buy some milk'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 1,\n",
       "   'actual_name': 'node_a',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  3: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'d': 'store'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 3,\n",
       "   'actual_name': 'node_a2',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  4: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store',\n",
       "     'distance': 0.0}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 4,\n",
       "   'actual_name': 'node_x',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  5: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'f': {'g': 'is a good boy'}, 'd': {'e': 'jimmy'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': {'b': {'c': 'store is good but not good enough'}},\n",
       "     'distance': inf}],\n",
       "   'distance': inf,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'},\n",
       "  7: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'stores',\n",
       "     'distance': 1.0}],\n",
       "   'distance': 1.0,\n",
       "   'expected_idx': 2,\n",
       "   'actual_idx': 7,\n",
       "   'actual_name': 'node_y',\n",
       "   'expected_name': 'node_.*',\n",
       "   'expected_label': 'node_z'}},\n",
       " '3': {2: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good',\n",
       "     'distance': 0.6666666666666667}],\n",
       "   'distance': 0.6666666666666667,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 2,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'},\n",
       "  6: {'comparisons': [{'comparison': 'word_overlap',\n",
       "     'kwargs': {},\n",
       "     'expected': 'store',\n",
       "     'actual': 'store is good but not good enough',\n",
       "     'distance': 0.8333333333333334}],\n",
       "   'distance': 0.8333333333333334,\n",
       "   'expected_idx': 3,\n",
       "   'actual_idx': 6,\n",
       "   'actual_name': 'node_c',\n",
       "   'expected_name': 'node_c',\n",
       "   'expected_label': '3'}},\n",
       " '1': {5: {'comparisons': [{'comparison': 'regex',\n",
       "     'kwargs': {},\n",
       "     'expected': 'jimmy',\n",
       "     'actual': 'jimmy',\n",
       "     'distance': 0.0},\n",
       "    {'comparison': 'word_overlap',\n",
       "     'kwargs': {'case_sensitive': False},\n",
       "     'expected': 'is a good boy',\n",
       "     'actual': 'is a good boy',\n",
       "     'distance': 0.0}],\n",
       "   'distance': 0.0,\n",
       "   'expected_idx': 1,\n",
       "   'actual_idx': 5,\n",
       "   'actual_name': 'node_b',\n",
       "   'expected_name': 'node_b',\n",
       "   'expected_label': '1'}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_dist = {'node_a1': {1: 0.375},\n",
    " '3': {2: 0.6666666666666667, 6: 0.8333333333333334},\n",
    " 'node_z': {4: 0.0, 7: 1.0},\n",
    " '1': {5: 0.0}}\n",
    "\n",
    "assert dist == expected_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExpectedTraceStep(name='node_a', label='node_a1', conditions=[Condition(accessor=('b', 'c'), value='jimmy went\\nto the store\\n', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_b', label='1', conditions=[Condition(accessor=('d', 'e'), value='jimmy', comparison='regex', kwargs={}), Condition(accessor=('f', 'g'), value='is a good boy', comparison='chat', kwargs={'case_sensitive': False})], before=[], after=['node_a1']),\n",
       " ExpectedTraceStep(name='node_.*', label='node_z', conditions=[Condition(accessor=('.',), value='store', comparison=None, kwargs={})], before=[], after=[]),\n",
       " ExpectedTraceStep(name='node_c', label='3', conditions=[Condition(accessor=('b', 'c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected.expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dist\n",
    "expected_traces = parsed_expected.expected\n",
    "traces = parsed_traces\n",
    "\n",
    "\n",
    "# get a mapping between expected labels and fresh var names\n",
    "label_to_var = LabelToVar()\n",
    "for idx,expected_step in enumerate(parsed_expected.expected):\n",
    "    label_to_var.add_label(expected_step.label,idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_possible_mappings(dist,expected_traces:ExpectedTrace,label_to_var:LabelToVar):\n",
    "    \"\"\"\n",
    "    Gets possible mappings between expected traces and actual traces.\n",
    "    By building a constraint satisfaction problem and solving it.\n",
    "    \"\"\"\n",
    "    p = Problem()\n",
    "    for col_idx,expected_step in enumerate(expected_traces.expected):\n",
    "        viable_trace_row_nums = list(dist[expected_step.label].keys())\n",
    "        var_name = label_to_var.get_col(expected_step.label)\n",
    "        p.addVariable(var_name,viable_trace_row_nums)\n",
    "        logger.debug(f\"Adding variable {var_name} with domain {viable_trace_row_nums}\")\n",
    "\n",
    "        for before_label in expected_step.before:\n",
    "            before_var_name = label_to_var.get_col(before_label)\n",
    "            logger.debug(f\"Adding constraint {before_var_name} < {var_name}\")\n",
    "            p.addConstraint(f\"{var_name} < {before_var_name}\")\n",
    "\n",
    "        for after_label in expected_step.after:\n",
    "            after_var_name = label_to_var.get_col(after_label)\n",
    "            logger.debug(f\"Adding constraint {var_name} < {after_var_name}\")\n",
    "            p.addConstraint(f\"{after_var_name} < {var_name}\")\n",
    "\n",
    "    # these solutions use colnames    \n",
    "    solutions = p.getSolutions()\n",
    "    # invert the colnames back to labels\n",
    "    labeled_solutions = set(frozendict({label_to_var.get_label(k):v for k,v in sol.items()}) for sol in solutions)\n",
    "    return labeled_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdiff import DeepDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__ - DEBUG - Adding variable a with domain [1]\n",
      "__main__ - DEBUG - Adding variable b with domain [5]\n",
      "__main__ - DEBUG - Adding constraint b < a\n",
      "__main__ - DEBUG - Adding variable c with domain [4, 7]\n",
      "__main__ - DEBUG - Adding variable d with domain [2, 6]\n",
      "__main__ - DEBUG - Adding constraint c < d\n",
      "__main__ - DEBUG - Adding constraint d < a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 4}),\n",
       " frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 7}),\n",
       " frozendict.frozendict({'node_a1': 1, '3': 6, '1': 5, 'node_z': 7})}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with checkLogs():\n",
    "    labeled_solutions = get_possible_mappings(dist,parsed_expected,label_to_var)\n",
    "\n",
    "assert DeepDiff(labeled_solutions,possible_mappings) == {}\n",
    "labeled_solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_best_mapping(dist_matrix,possible_mappings,label_to_var):\n",
    "    \"\"\"\n",
    "    dist_matrix: np.ndarray\n",
    "    possible_mappings: list of tuples\n",
    "    label_to_var: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    score_per_solution = {}\n",
    "    for sol in possible_mappings:\n",
    "        sum_dist = 0\n",
    "        for expected_label,trace_idx in sol.items():\n",
    "            sum_dist += dist_matrix[expected_label][trace_idx]\n",
    "        score_per_solution[sol] = sum_dist\n",
    "\n",
    "    best_solution =  min(score_per_solution,key=score_per_solution.get)\n",
    "    best_solution_score = score_per_solution[best_solution]\n",
    "    return best_solution,best_solution_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mapping,best_score = get_best_mapping(dist,possible_mappings,label_to_var)\n",
    "assert best_mapping == frozendict({'node_a1': 1,'3': 2,'node_z': 4,'1': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def align_traces(traces_outputs,expected_trace,comparisons,default_comparison):\n",
    "    \"\"\"\n",
    "    Compute the distance matrix between the traces and the expected traces.\n",
    "    \"\"\"\n",
    "    label_to_var = LabelToVar()\n",
    "    for idx,expected_step in enumerate(expected_trace.expected):\n",
    "        label_to_var.add_label(expected_step.label,idx)\n",
    "\n",
    "    dist,debug_info = await compute_distances(traces_outputs,expected_trace,comparisons,default_comparison)\n",
    "    possible_mappings = get_possible_mappings(dist,expected_trace,label_to_var)\n",
    "    best_mapping,best_score = get_best_mapping(dist,possible_mappings,label_to_var)\n",
    "    return best_mapping, best_score, debug_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExpectedTrace(input=[{'content': 'hello world'}], expected=[ExpectedTraceStep(name='node_a', label='node_a1', conditions=[Condition(accessor=('b', 'c'), value='jimmy went\\nto the store\\n', comparison=None, kwargs={})], before=[], after=[]), ExpectedTraceStep(name='node_b', label='1', conditions=[Condition(accessor=('d', 'e'), value='jimmy', comparison='regex', kwargs={}), Condition(accessor=('f', 'g'), value='is a good boy', comparison='chat', kwargs={'case_sensitive': False})], before=[], after=['node_a1']), ExpectedTraceStep(name='node_.*', label='node_z', conditions=[Condition(accessor=('.',), value='store', comparison=None, kwargs={})], before=[], after=[]), ExpectedTraceStep(name='node_c', label='3', conditions=[Condition(accessor=('b', 'c'), value='store', comparison=None, kwargs={})], before=['node_z'], after=['node_a1'])])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(frozendict.frozendict({'node_a1': 1, '3': 2, '1': 5, 'node_z': 4}),\n",
       " 1.0416666666666667)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_expected = parse_expected_trace(example_yaml)\n",
    "example_trace\n",
    "\n",
    "default_comparison = word_overlap\n",
    "comparisons = {\n",
    "    \"regex\": regex,\n",
    "    \"word_overlap\": word_overlap,\n",
    "}\n",
    "\n",
    "\n",
    "aligned_traces,score,debug_info = await align_traces(parsed_traces,parsed_expected,comparisons,default_comparison)\n",
    "assert aligned_traces == best_mapping\n",
    "aligned_traces,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_a1', 'node_z', '3', '1'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_info.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
